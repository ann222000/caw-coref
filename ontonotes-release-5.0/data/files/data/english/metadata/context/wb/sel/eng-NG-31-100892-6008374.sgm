<DOC>
<DOCID> eng-NG-31-100892-6008374 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2007-01-24T07:42:00 </DATETIME>
<BODY>
<HEADLINE>
Subject and Object
</HEADLINE>
<TEXT>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-24T07:42:00 </POSTDATE>
"If we rigorously and systematically discriminate subject and object often
enough, long enough, and on sufficiently various occasions, it is likely to
dawn on us sooner or later that the subject is not an object. The subject is
not any particular object or any combination of objects. The chief barrier
to drawing this conclusion is conceptual. That is, our models of reality may
impose constraints on empirical investigation. We get attached to our
models, particularly if we have worked hard on them, and we may want to
avoid looking in a direction which threatens them.

"Issues at the core of current consciousness debates are variations of the
problem of subjectivity, such as the 'hard problem', the problem of other
minds, and the zombie problem. Progress on these issues would seem
to presuppose some understanding of the subject/object distinction. The
proposition that the subject is not an object has radical implications for
current tendencies in the philosophy of mind or consciousness studies.
It means that 'consciousness' is not to be identified with, or explained in
terms of, any entities or processes to be found as objects of attention in
the world. It means that any attempt to understand 'consciousness' as a
property of some kind-whether a property of systems or a property of
states-is fundamentally misguided."

http://www.imprint.co.uk/online/edey.html
</POST>
<POST>
<POSTER> "$$$" &lt;e...@lycos.com&gt; </POSTER>
<POSTDATE> 2007-01-24T09:04:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
andy-k wrote:
&gt; &quot;If we rigorously and systematically discriminate subject and object often
&gt; enough, long enough, and on sufficiently various occasions, it is likely to
&gt; dawn on us sooner or later that the subject is not an object. The subject is
&gt; not any particular object or any combination of objects. The chief barrier
&gt; to drawing this conclusion is conceptual. That is, our models of reality may
&gt; impose constraints on empirical investigation. We get attached to our
&gt; models, particularly if we have worked hard on them, and we may want to
&gt; avoid looking in a direction which threatens them.

&gt; &quot;Issues at the core of current consciousness debates are variations of the
&gt; problem of subjectivity, such as the 'hard problem', the problem of other
&gt; minds, and the zombie problem. Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction. The
&gt; proposition that the subject is not an object has radical implications for
&gt; current tendencies in the philosophy of mind or consciousness studies.
&gt; It means that 'consciousness' is not to be identified with, or explained in
&gt; terms of, any entities or processes to be found as objects of attention in
&gt; the world. It means that any attempt to understand 'consciousness' as a
&gt; property of some kind-whether a property of systems or a property of
&gt; states-is fundamentally misguided.&quot;

&gt; http://www.imprint.co.uk/online/edey.html
">

Well, it's a property of brains. A rock probably isn't experiencing
images / sounds or linguistic thoughts pertaining to images and sounds.
But any elemental preceding neural structures that the complexity of
human experience emerges from probably [indeed] isn't going to show-up
in consciousness apart from maybe whatever physical correlate it's
found in association with. It's akin to trying to detect gravitons in
space, which is complicated by space itself being composed of
gravitons.

=======
</POST>
<POST>
<POSTER> "Ed" &lt;solon...@earthlink.net&gt; </POSTER>
<POSTDATE> 2007-01-24T09:49:00 </POSTDATE>
On Jan 24, 7:42 am, "andy-k" &lt;spam.free@last&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;If we rigorously and systematically discriminate subject and object often
&gt; enough, long enough, and on sufficiently various occasions, it is likely to
&gt; dawn on us sooner or later that the subject is not an object. The subject is
&gt; not any particular object or any combination of objects. The chief barrier
&gt; to drawing this conclusion is conceptual. That is, our models of reality may
&gt; impose constraints on empirical investigation. We get attached to our
&gt; models, particularly if we have worked hard on them, and we may want to
&gt; avoid looking in a direction which threatens them.

&gt; &quot;Issues at the core of current consciousness debates are variations of the
&gt; problem of subjectivity, such as the 'hard problem', the problem of other
&gt; minds, and the zombie problem. Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction. The
&gt; proposition that the subject is not an object has radical implications for
&gt; current tendencies in the philosophy of mind or consciousness studies.
&gt; It means that 'consciousness' is not to be identified with, or explained in
&gt; terms of, any entities or processes to be found as objects of attention in
&gt; the world. It means that any attempt to understand 'consciousness' as a
&gt; property of some kind-whether a property of systems or a property of
&gt; states-is fundamentally misguided.&quot;

&gt; http://www.imprint.co.uk/online/edey.html
">

Well, it *looks* like a property; my dog displays consciousness, my
shoes do not.
My dog is solid, so are my shoes, but my cigarette smoke is not.
Consciousness seems to fit the way we talk about properties, what is it
about it that makes it not a property?
</POST>
<POST>
<POSTER> "Scott H" &lt;nospam&gt; </POSTER>
<POSTDATE> 2007-01-24T10:50:00 </POSTDATE>
I strongly believe that consciousness must involve not only passivity in the
form of perception, but activity in the form of self-understanding--activity
of both the brain and the body. What we see and what we do are related. And,
what is mysterious, and what every physicist knows, the laws of nature
import mathematical intelligence to us.

<QUOTE PREVIOUSPOST="
&quot;andy-k&quot; &lt;spam.free@last&gt; wrote in message
">

news:LuIth.46108$RL5.12824@newsfe2-gui.ntli.net ...

<QUOTE PREVIOUSPOST="
&gt; &quot;If we rigorously and systematically discriminate subject and object often
&gt; enough, long enough, and on sufficiently various occasions, it is likely
&gt; to
&gt; dawn on us sooner or later that the subject is not an object. The subject
&gt; is
&gt; not any particular object or any combination of objects. The chief barrier
&gt; to drawing this conclusion is conceptual. That is, our models of reality
&gt; may
&gt; impose constraints on empirical investigation. We get attached to our
&gt; models, particularly if we have worked hard on them, and we may want to
&gt; avoid looking in a direction which threatens them.

&gt; &quot;Issues at the core of current consciousness debates are variations of the
&gt; problem of subjectivity, such as the 'hard problem', the problem of other
&gt; minds, and the zombie problem. Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction. The
&gt; proposition that the subject is not an object has radical implications for
&gt; current tendencies in the philosophy of mind or consciousness studies.
&gt; It means that 'consciousness' is not to be identified with, or explained
&gt; in
&gt; terms of, any entities or processes to be found as objects of attention in
&gt; the world. It means that any attempt to understand 'consciousness' as a
&gt; property of some kind-whether a property of systems or a property of
&gt; states-is fundamentally misguided.&quot;
">
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-24T16:49:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:LuIth.46108$RL5.12824@newsfe2-gui.ntli.net:

<QUOTE PREVIOUSPOST="
&gt; &quot;Issues at the core of current consciousness debates are variations of
&gt; the problem of subjectivity, such as the 'hard problem', the problem of
&gt; other minds, and the zombie problem.
">

Dead right.

<QUOTE PREVIOUSPOST="
&gt; Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction.
">

True, but unhelpful.

<QUOTE PREVIOUSPOST="
&gt; The
&gt; proposition that the subject is not an object has radical implications
&gt; for current tendencies in the philosophy of mind or consciousness
&gt; studies. It means that 'consciousness' is not to be identified with, or
&gt; explained in terms of, any entities or processes to be found as objects
&gt; of attention in the world.
">

Dead wrong. Consciousness is not the subject. The subject is conscious, but
so are many objects. The fallacious assumption there is that the primary
evidence for consciousness is first-person evidence.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-24T18:22:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; Mait Edey wrote in article http://www.imprint.co.uk/online/edey.html:
&gt;&gt; &quot;The proposition that the subject is not an object has radical
&gt;&gt; implications for current tendencies in the philosophy of mind or
&gt;&gt; consciousness studies. It means that 'consciousness' is not to be
&gt;&gt; identified with, or explained in terms of, any entities or processes to
&gt;&gt; be found as objects of attention in the world.&quot;

&gt; Dead wrong. Consciousness is not the subject. The subject is conscious,
&gt; but so are many objects. The fallacious assumption there is that the
&gt; primary evidence for consciousness is first-person evidence.
">

Edey writes in an article in JCS 4.5/6:

"When I realize that I am, or am conscious, I'm not realizing anything about
any objects or contents of mind or awareness. I realize consciousness
(or awareness, or subjectivity) itself. My realization that I am conscious
is not tied to awareness of any particular objects. Indeed, it has more to
do with realizing the potential absence of any particular object or objects.
In realizing the potential absence of any particular objects, I realize
myself or my being ('I am') as distinct from all objects."
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-24T18:23:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Ed&quot; wrote:
&gt; Mait Edey wrote in article http://www.imprint.co.uk/online/edey.html:
&gt;&gt; &quot;The proposition that the subject is not an object has radical
&gt;&gt; implications for current tendencies in the philosophy of mind or
&gt;&gt; consciousness studies. It means that 'consciousness' is not to be
&gt;&gt; identified with, or explained in terms of, any entities or processes to
&gt;&gt; be found as objects of attention in the world. It means that any attempt
&gt;&gt; to understand 'consciousness' as a property of some kind-whether a
&gt;&gt; property of systems or a property of states-is fundamentally misguided.&quot;

&gt; Well, it *looks* like a property; my dog displays consciousness, my
&gt; shoes do not.
&gt; My dog is solid, so are my shoes, but my cigarette smoke is not.
&gt; Consciousness seems to fit the way we talk about properties, what is it
&gt; about it that makes it not a property?
">

Edey writes in an article in JCS 4.5/6:

"Obviously, as a practical matter I use behavioural clues as evidence to
answer the question: are you conscious? But those clues do not define the
meaning of my question or of my conclusion. What then is that meaning?
I can wonder if you are conscious, in the first place, only if I have
already realized that I am conscious. And it is only my realization that I
am conscious which defines what I mean by 'conscious' or 'consciousness'.
What I have realized must be the actual referent of the word
'consciousness'.

"I do not wonder whether I am conscious. I already realize I am conscious.
My realization is peculiar in two respects whose significance is crucial.
First, it is peculiarly obvious. And second, I don't attribute consciousness
to myself on any grounds whatever. I haven't reasoned it out. I haven't
looked for clues. I haven't decided what empirical evidence is relevant,
and collected that evidence. No evidence is relevant. My realization that
I'm conscious is neither verifiable nor falsifiable. It is not testable at
all.

"These peculiarities are the case because my realization is not the
attribution of some property to some system. It has nothing to do with
properties or their attribution. I may have a theory that I am this or that
kind of a system, and I may have been led to assume that consciousness
is a property of the system, but that is not what I actually realize.
I don't conclude that I have passed an operational test. I don't actually
check myself out to see if I exhibit any particular behavioural or
functional properties. So such criteria do not define what I mean when I say
that I am conscious. Therefore they cannot define what I mean when I
conclude that you are conscious, unless we are willing to say that the term
'conscious' is used here in two entirely different senses, which of course
gets us nowhere."
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-24T19:35:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:gTRth.81484$Qa6.26105@newsfe6-gui.ntli.net:

<QUOTE PREVIOUSPOST="
&gt; &quot;When I realize that I am, or am conscious, I'm not realizing anything
&gt; about any objects or contents of mind or awareness. I realize
&gt; consciousness (or awareness, or subjectivity) itself.
">

That is circular. "Realize" means the same thing as "conscious of." So he
is conscious of consciousness. Tells us nothing.

---------------

I shall begin by boldly declaring what consciousness is. That is, I shall
give a definition which I trust captures the ordinary intension of that
term, and supplies the criteria upon which we normally rely when deciding
when and to what to apply it --- in other words, its truth conditions. I
shall trust also that everyone understands that such a truth-functional
definition is not an analytical definition; it is not an *explanation* of
consciousness. It does not specify how consciousness comes about in a
physical or any other system, or what else must be true of a system ---
what other properties the system must have --- in order for it to become
conscious.

To make clear the difference between these two kinds of definition, let's
consider an example often encountered in the literature on consciousness
--- that of "water." "Water is H2O" is an analytical definition. It tells
us what water is made of, in the terms of atomic theory, which in turn
helps us explain why it has some of the properties it does, and why those
properties vary as they do. But the ordinary, truth-functional definition
would go something like, "The clear, colorless liquid (which is also
odorless and tasteless when pure) of which of rain, rivers, lakes, and the
oceans chiefly consist, which boils to a vapor when heated to 100°C and
freezes to a solid when cooled to 0°C., and which is essential for most
forms of life."

It should be obvious that when we are trying to decide whether something is
or is not water, it is the latter kind of definition upon which we
routinely rely. We normally have no idea what might be the molecular
composition of a clear fluid in a stream from which we are considering
drinking, and no means to determine it. Two hundred years ago no one could
possibly have determined it. Yet no one had any trouble recognizing water
when they encountered it. We need to begin with an analogous definition of
consciousness.

Here is the definition:

"Consciousness" is a capacity imputed to certain living organisms, and
perhaps (controversially) also to certain non-biological systems, for
having subjective states. That capacity is imputed based on the behavior or
function of the system in question, although the structure of the system,
as revealed by both its gross appearance and its internal architecture, is
often accepted as predictive of the function.

I take it to be beyond question that when we declare a system to be
conscious, we mean to claim one of two closely related things:

1. That the system is capable of being in various subjective states, such
as emotional states, states of desire, and subjective perceptual
("phenomenal") states ("qualia"). I.e., that it has a capacity for being in
those states; that it is the sort of thing to which we would we willing to
attribute one of those states (I'll give a fuller account of "subjective
states" in Sec. 5).

2. We may also mean by "conscious" that a system is *currently* in one or
more of those subjective states. Consciousness is, in other words, a
capacity which may be exercised or quiescent at any given time. Hence a
person who is anaesthetized or sleeping is conscious in the first sense,
but perhaps not in the second. An awake person who is responding normally
to the environment, and even a sleeping person who is dreaming, is also
conscious in the second sense; a dead person or a brick is conscious in
neither sense.

The two senses are related in that the second presumes the first: a system
cannot be in a subjective state unless it has the *capacity* for
consciousness. For terminological simplicity we may denote these two senses
the capacity sense and the occurrent sense.

Another way to visualize this schema is by analogy to television
broadcasting. A television transmitter at a given time may be in any of
several states, e.g., it may be "off the air," (powered down), or it may be
"up and running" but transmitting no images ("dead air"), or it may be
currently broadcasting a program. The capacity for consciousness is
analogous to the "up and running" state of the television transmitter ---
it is capable of transmitting, whether or not any program is presently
being transmitted.

Now, of course, to round out this this analogy, we should note that the
television transmitter has a fourth possible state --- a dysfunctional
state. E.g., the underlying hardware is damaged or not of the proper design
to allow transmission of a television signal. It cannot be powered on, or
would not transmit any images if it were. That is another sense of
"capacity" --- the "hardware capacity" sense. This is a theoretical
capacity --- one we might attribute based on analysis of the circuit
diagram and the physical condition of the (purported) transmitter. The
transmitter may have a capacity to broadcast in this sense even when
powered down. But when we attribute consciousness to a system we refer to
the "ready capacity" described above --- the system is functional, "up and
running," whether or not it is currently transmitting any images, and
regardless of what particular images it may be transmitting.

I also take it to be beyond question that we attribute this "ready
capacity" of consciousness to systems other than ourselves entirely on
behavioral grounds. However, because in our experience only systems with
certain physical features display the behaviors which prompt us to
attribute it, we routinely impute that capacity to anything that *appears*
to be one of those "eligible" systems, even before we have observed any
behavior on its part. But that is always a preliminary judgment which may
be reversed if the requisite behaviors are not subsequently observed. If I
walk into a news shop and find the shopkeeper seated behind the counter,
apparently dozing, I'll tap the counter bell and announce, "Good morning.
I'd like the Times, please." I have assumed, based only on appearances,
that he is conscious in the capacity sense. But if he does not rouse
himself and respond (behavior), I'll look more closely, and perhaps
discover that he is a mannequin, or even dead. At that point I abandon my
initial assumption --- he is not conscious after all.

On the other hand, he may respond normally, perhaps handing me the
newspaper and saying, "Sorry for nodding on the job. Bit of a hangover this
morning, I'm afraid --- friend's bachelor party last night. Will that be
all, sir?" In that case my initial assumption is confirmed, and indeed is
irreversible --- he is conscious. I don't have to examine an MRI scan of
the shopkeeper's brain, or feel his hangover, to confirm that conclusion.

Having decided a system has the capacity for subjective states, we may then
attribute such states to it, also based on behavioral indicators, with
different states imputed according to differences of behavior, in relation
to differences in circumstances.
</POST>
<POST>
<POSTER> "Ed" &lt;solon...@earthlink.net&gt; </POSTER>
<POSTDATE> 2007-01-24T22:13:00 </POSTDATE>
On Jan 24, 6:23 pm, "andy-k" &lt;spam.free@last&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;Ed&quot; wrote:
&gt; &gt; Mait Edey wrote in article http://www.imprint.co.uk/online/edey.html:
&gt; &gt;&gt; &quot;The proposition that the subject is not an object has radical
&gt; &gt;&gt; implications for current tendencies in the philosophy of mind or
&gt; &gt;&gt; consciousness studies. It means that 'consciousness' is not to be
&gt; &gt;&gt; identified with, or explained in terms of, any entities or processes to
&gt; &gt;&gt; be found as objects of attention in the world. It means that any attempt
&gt; &gt;&gt; to understand 'consciousness' as a property of some kind-whether a
&gt; &gt;&gt; property of systems or a property of states-is fundamentally misguided.&quot;

&gt; &gt; Well, it *looks* like a property; my dog displays consciousness, my
&gt; &gt; shoes do not.
&gt; &gt; My dog is solid, so are my shoes, but my cigarette smoke is not.
&gt; &gt; Consciousness seems to fit the way we talk about properties, what is it
&gt; &gt; about it that makes it not a property?Edey writes in an article in JCS 4.5/6:

&gt; &quot;Obviously, as a practical matter I use behavioural clues as evidence to
&gt; answer the question: are you conscious? But those clues do not define the
&gt; meaning of my question or of my conclusion. What then is that meaning?
&gt; I can wonder if you are conscious, in the first place, only if I have
&gt; already realized that I am conscious. And it is only my realization that I
&gt; am conscious which defines what I mean by 'conscious' or 'consciousness'.
&gt; What I have realized must be the actual referent of the word
&gt; 'consciousness'.

&gt; &quot;I do not wonder whether I am conscious. I already realize I am conscious.
&gt; My realization is peculiar in two respects whose significance is crucial.
&gt; First, it is peculiarly obvious. And second, I don't attribute consciousness
&gt; to myself on any grounds whatever. I haven't reasoned it out. I haven't
&gt; looked for clues. I haven't decided what empirical evidence is relevant,
&gt; and collected that evidence. No evidence is relevant. My realization that
&gt; I'm conscious is neither verifiable nor falsifiable. It is not testable at
&gt; all.

&gt; &quot;These peculiarities are the case because my realization is not the
&gt; attribution of some property to some system. It has nothing to do with
&gt; properties or their attribution. I may have a theory that I am this or that
&gt; kind of a system, and I may have been led to assume that consciousness
&gt; is a property of the system, but that is not what I actually realize.
&gt; I don't conclude that I have passed an operational test. I don't actually
&gt; check myself out to see if I exhibit any particular behavioural or
&gt; functional properties. So such criteria do not define what I mean when I say
&gt; that I am conscious. Therefore they cannot define what I mean when I
&gt; conclude that you are conscious, unless we are willing to say that the term
&gt; 'conscious' is used here in two entirely different senses, which of course
&gt; gets us nowhere.&quot;
">

For me, that was a very informative reply, thank you!

It sounds as if being conscious and being "I" are intimately related.
Imagine a person with multiple personalities ala "Sybil"; where one
personality(called "A") would suddenly awaken to find that days had
passed since the last conscious experience, presumably because one of
the other personalities has "taken over" in the interim.  Does it make
sense for "A" to wonder if she was conscious the whole time?  How about
persons suffering from complete amnesia, does it make sense for them to
wonder if they were conscious previously?

In short, is it possible to question one's own consciousness even if
this possibility only show itself under somewhat bizzare circumstances?
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-25T05:28:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; Mait Edey wrote in article http://www.imprint.co.uk/online/edey.html:
&gt;&gt; &quot;When I realize that I am, or am conscious, I'm not realizing
&gt;&gt; anything about any objects or contents of mind or awareness.
&gt;&gt; I realize consciousness (or awareness, or subjectivity) itself.

&gt; That is circular. &quot;Realize&quot; means the same thing as &quot;conscious of.&quot;
&gt; So he is conscious of consciousness. Tells us nothing.
">

It tells us there's been a radical change of state. Edey proposes that this
change of state is a consequence of the emergence of a sense of self.
Strawson, in an article in the same issue of JCS, gives some interesting
quotes such as:

"_What_ was I before I came to self-consciousness? . . .
_I_ did not exist at all, for I was not an I. The I exists only
insofar as it is conscious of itself. . . . _The self posits itself_,
and by virtue of this mere self-assertion it exists. (Fichte 1794-5)"

"The ego continuously constitutes itself as existing. (Husserl 1929)."

"The self which is reflexively referred to is synthesized
in that very act of reflexive self-reference. (Nozick 1981)".

-------------

Many thanks for the essay -- I don't have time to give it the attention it
deserves just yet, but I will read it carefully in due course and get back
to you.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-25T05:29:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Ed&quot; wrote:
&gt; It sounds as if being conscious and being &quot;I&quot; are intimately related.
">

Yes indeed -- Edey writes "I must already have realized
THAT I am  before I can even begin to wonder WHAT I am,
much less decide that I am such-and-such."

<QUOTE PREVIOUSPOST="
&gt; Imagine a person with multiple personalities ala &quot;Sybil&quot;; where one
&gt; personality(called &quot;A&quot;) would suddenly awaken to find that days had
&gt; passed since the last conscious experience, presumably because one of
&gt; the other personalities has &quot;taken over&quot; in the interim.  Does it make
&gt; sense for &quot;A&quot; to wonder if she was conscious the whole time?  How about
&gt; persons suffering from complete amnesia, does it make sense for them to
&gt; wonder if they were conscious previously?
">

These are really interesting questions, and I'm putting myself out on a limb
by suggesting possible answers. Even so, I would suggest that it's possible
there's some "supervisor" function in the brain that switches tasks in and
out, just like the scheduler in your PC. Some cases of psychogenic fugue
provide an example of this, where a person's life has become so stressful
that the brain suddenly enters a state of total retrograde amnesia
(regarding event memory, not procedural memory). Multiple personality
disorder, IIRC, is a similar stress-management response. We can and do
forget, and we can and do invent "false memories". Michael Gazzaniga,
in his wonderful little book "The Mind's Past", makes the claim that most of
our auto-biographical memory consists of such false memories, wherein a
script is continually being written for the 'self' that fits with what
actually took place.

<QUOTE PREVIOUSPOST="
&gt; In short, is it possible to question one's own consciousness even if
&gt; this possibility only show itself under somewhat bizzare circumstances?
">

As I said to you in a different thread, my reservations about the concept of
consciousness falter at a very early hurdle. Edey comes closest to putting
my reservations into words, but he's not quite approaching the subject from
the same angle.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-26T05:05:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; I shall begin by boldly declaring what consciousness is. That is, I shall
&gt; give a definition which I trust captures the ordinary intension of that
&gt; term, and supplies the criteria upon which we normally rely when deciding
&gt; when and to what to apply it --- in other words, its truth conditions. I
&gt; shall trust also that everyone understands that such a truth-functional
&gt; definition is not an analytical definition; it is not an *explanation* of
&gt; consciousness. It does not specify how consciousness comes about in a
&gt; physical or any other system, or what else must be true of a system ---
&gt; what other properties the system must have --- in order for it to become
&gt; conscious.

&gt; To make clear the difference between these two kinds of definition, let's
&gt; consider an example often encountered in the literature on consciousness
&gt; --- that of &quot;water.&quot; &quot;Water is H2O&quot; is an analytical definition. It tells
&gt; us what water is made of, in the terms of atomic theory, which in turn
&gt; helps us explain why it has some of the properties it does, and why those
&gt; properties vary as they do. But the ordinary, truth-functional definition
&gt; would go something like, &quot;The clear, colorless liquid (which is also
&gt; odorless and tasteless when pure) of which of rain, rivers, lakes, and the
&gt; oceans chiefly consist, which boils to a vapor when heated to 100°C and
&gt; freezes to a solid when cooled to 0°C., and which is essential for most
&gt; forms of life.&quot;

&gt; It should be obvious that when we are trying to decide whether something
&gt; is or is not water, it is the latter kind of definition upon which we
&gt; routinely rely. We normally have no idea what might be the molecular
&gt; composition of a clear fluid in a stream from which we are considering
&gt; drinking, and no means to determine it. Two hundred years ago no one could
&gt; possibly have determined it. Yet no one had any trouble recognizing water
&gt; when they encountered it. We need to begin with an analogous definition of
&gt; consciousness.

&gt; Here is the definition:

&gt; &quot;Consciousness&quot; is a capacity imputed to certain living organisms, and
&gt; perhaps (controversially) also to certain non-biological systems, for
&gt; having subjective states. That capacity is imputed based on the behavior
&gt; or function of the system in question, although the structure of the
&gt; system, as revealed by both its gross appearance and its internal
&gt; architecture, is often accepted as predictive of the function.

&gt; I take it to be beyond question that when we declare a system to be
&gt; conscious, we mean to claim one of two closely related things:

&gt; 1. That the system is capable of being in various subjective states, such
&gt; as emotional states, states of desire, and subjective perceptual
&gt; (&quot;phenomenal&quot;) states (&quot;qualia&quot;). I.e., that it has a capacity for being
&gt; in those states; that it is the sort of thing to which we would we willing
&gt; to attribute one of those states (I'll give a fuller account of
&gt; &quot;subjective states&quot; in Sec. 5).

&gt; 2. We may also mean by &quot;conscious&quot; that a system is *currently* in one or
&gt; more of those subjective states. Consciousness is, in other words, a
&gt; capacity which may be exercised or quiescent at any given time. Hence a
&gt; person who is anaesthetized or sleeping is conscious in the first sense,
&gt; but perhaps not in the second. An awake person who is responding normally
&gt; to the environment, and even a sleeping person who is dreaming, is also
&gt; conscious in the second sense; a dead person or a brick is conscious in
&gt; neither sense.

&gt; The two senses are related in that the second presumes the first: a system
&gt; cannot be in a subjective state unless it has the *capacity* for
&gt; consciousness. For terminological simplicity we may denote these two
&gt; senses the capacity sense and the occurrent sense.

&gt; Another way to visualize this schema is by analogy to television
&gt; broadcasting. A television transmitter at a given time may be in any of
&gt; several states, e.g., it may be &quot;off the air,&quot; (powered down), or it may
&gt; be &quot;up and running&quot; but transmitting no images (&quot;dead air&quot;), or it may be
&gt; currently broadcasting a program. The capacity for consciousness is
&gt; analogous to the &quot;up and running&quot; state of the television transmitter ---
&gt; it is capable of transmitting, whether or not any program is presently
&gt; being transmitted.

&gt; Now, of course, to round out this this analogy, we should note that the
&gt; television transmitter has a fourth possible state --- a dysfunctional
&gt; state. E.g., the underlying hardware is damaged or not of the proper
&gt; design to allow transmission of a television signal. It cannot be powered
&gt; on, or would not transmit any images if it were. That is another sense of
&gt; &quot;capacity&quot; --- the &quot;hardware capacity&quot; sense. This is a theoretical
&gt; capacity --- one we might attribute based on analysis of the circuit
&gt; diagram and the physical condition of the (purported) transmitter. The
&gt; transmitter may have a capacity to broadcast in this sense even when
&gt; powered down. But when we attribute consciousness to a system we refer to
&gt; the &quot;ready capacity&quot; described above --- the system is functional, &quot;up and
&gt; running,&quot; whether or not it is currently transmitting any images, and
&gt; regardless of what particular images it may be transmitting.

&gt; I also take it to be beyond question that we attribute this &quot;ready
&gt; capacity&quot; of consciousness to systems other than ourselves entirely on
&gt; behavioral grounds. However, because in our experience only systems with
&gt; certain physical features display the behaviors which prompt us to
&gt; attribute it, we routinely impute that capacity to anything that *appears*
&gt; to be one of those &quot;eligible&quot; systems, even before we have observed any
&gt; behavior on its part. But that is always a preliminary judgment which may
&gt; be reversed if the requisite behaviors are not subsequently observed. If I
&gt; walk into a news shop and find the shopkeeper seated behind the counter,
&gt; apparently dozing, I'll tap the counter bell and announce, &quot;Good morning.
&gt; I'd like the Times, please.&quot; I have assumed, based only on appearances,
&gt; that he is conscious in the capacity sense. But if he does not rouse
&gt; himself and respond (behavior), I'll look more closely, and perhaps
&gt; discover that he is a mannequin, or even dead. At that point I abandon my
&gt; initial assumption --- he is not conscious after all.

&gt; On the other hand, he may respond normally, perhaps handing me the
&gt; newspaper and saying, &quot;Sorry for nodding on the job. Bit of a hangover
&gt; this morning, I'm afraid --- friend's bachelor party last night. Will that
&gt; be all, sir?&quot; In that case my initial assumption is confirmed, and indeed
&gt; is irreversible --- he is conscious. I don't have to examine an MRI scan
&gt; of the shopkeeper's brain, or feel his hangover, to confirm that
&gt; conclusion.

&gt; Having decided a system has the capacity for subjective states, we may
&gt; then attribute such states to it, also based on behavioral indicators,
&gt; with different states imputed according to differences of behavior, in
&gt; relation to differences in circumstances.
">

I have an instinctive conviction that other people have "experiential
states" (perhaps what you call "subjective states" thought I would
have liked to have seen your account of these), and a strong suspicion
that humans are not the only instantiations of experiential states.
The question for me, then, is how to defend any propensity to draw a
line -- i.e. to claim this capacity for some objects and not for others.
I view this as a spectrum problem, with solipsism at one end
and a Whiteheadian-style of panexperientialism at the other.
I may be misinterpreting Whitehead here but I understand him to
have made a distinction between experience and consciousness in
that consciousness is an emergent property of experience wherein
experience has become reflexive -- i.e. in Edey's terms, the realization of
subjectivity has taken place. You say in your account of consciousness
that "That capacity is imputed based on the behavior or function of the
system in question", implying that the line is drawn on behavioral grounds.
Whilst I can see that might be the case for *cognitive* states, I don't know
how it might be applied to *experiential* states -- i.e. on what grounds
would we be justified in concluding whether or not an electron simply
*experiences* its state transitions?
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-26T06:22:00 </POSTDATE>
On 24 Jan, 12:42, "andy-k" &lt;spam.free@last&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;If we rigorously and systematically discriminate subject and object often
&gt; enough, long enough, and on sufficiently various occasions, it is likely to
&gt; dawn on us sooner or later that the subject is not an object. The subject is
&gt; not any particular object or any combination of objects. The chief barrier
&gt; to drawing this conclusion is conceptual. That is, our models of reality may
&gt; impose constraints on empirical investigation. We get attached to our
&gt; models, particularly if we have worked hard on them, and we may want to
&gt; avoid looking in a direction which threatens them.

&gt; &quot;Issues at the core of current consciousness debates are variations of the
&gt; problem of subjectivity, such as the 'hard problem', the problem of other
&gt; minds, and the zombie problem. Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction. The
&gt; proposition that the subject is not an object has radical implications for
&gt; current tendencies in the philosophy of mind or consciousness studies.
&gt; It means that 'consciousness' is not to be identified with, or explained in
&gt; terms of, any entities or processes to be found as objects of attention in
&gt; the world. It means that any attempt to understand 'consciousness' as a
&gt; property of some kind-whether a property of systems or a property of
&gt; states-is fundamentally misguided.&quot;

&gt; http://www.imprint.co.uk/online/edey.html
">

That wouldn't mean to say that any attempt to understand the connection
between conscious experiences and neural state would be a waste of time
though.
</POST>
<POST>
<POSTER> "Ed" &lt;solon...@earthlink.net&gt; </POSTER>
<POSTDATE> 2007-01-26T12:46:00 </POSTDATE>
On Jan 25, 5:29 am, "andy-k" &lt;spam.free@last&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;Ed&quot; wrote:
&gt; &gt; It sounds as if being conscious and being &quot;I&quot; are intimately related.Yes indeed -- Edey writes &quot;I must already have realized
&gt; THAT I am  before I can even begin to wonder WHAT I am,
&gt; much less decide that I am such-and-such.&quot;

&gt; &gt; Imagine a person with multiple personalities ala &quot;Sybil&quot;; where one
&gt; &gt; personality(called &quot;A&quot;) would suddenly awaken to find that days had
&gt; &gt; passed since the last conscious experience, presumably because one of
&gt; &gt; the other personalities has &quot;taken over&quot; in the interim.  Does it make
&gt; &gt; sense for &quot;A&quot; to wonder if she was conscious the whole time?  How about
&gt; &gt; persons suffering from complete amnesia, does it make sense for them to
&gt; &gt; wonder if they were conscious previously?These are really interesting questions, and I'm putting myself out on a limb
&gt; by suggesting possible answers. Even so, I would suggest that it's possible
&gt; there's some &quot;supervisor&quot; function in the brain that switches tasks in and
&gt; out, just like the scheduler in your PC. Some cases of psychogenic fugue
&gt; provide an example of this, where a person's life has become so stressful
&gt; that the brain suddenly enters a state of total retrograde amnesia
&gt; (regarding event memory, not procedural memory). Multiple personality
&gt; disorder, IIRC, is a similar stress-management response. We can and do
&gt; forget, and we can and do invent &quot;false memories&quot;. Michael Gazzaniga,
&gt; in his wonderful little book &quot;The Mind's Past&quot;, makes the claim that most of
&gt; our auto-biographical memory consists of such false memories, wherein a
&gt; script is continually being written for the 'self' that fits with what
&gt; actually took place.

&gt; &gt; In short, is it possible to question one's own consciousness even if
&gt; &gt; this possibility only show itself under somewhat bizzare circumstances?As I said to you in a different thread, my reservations about the concept of
&gt; consciousness falter at a very early hurdle. Edey comes closest to putting
&gt; my reservations into words, but he's not quite approaching the subject from
&gt; the same angle.
">

Consciousness has been made into a very "big deal" by some philosophers
(like Chalmers) and one may question their sense that is a
Universe-altering phenomenon; still, the basket of concepts that
include consciousness, awareness, and self-identity are refering to
something real and something puzzling.  The more we poke at them the
better the chance to find a key of some kind. I mention them together
because I have some doubts about the separateness of these thing that
we have different words for, or if it is resonable to categorize these
aspects separately, about the way in which we've divided them up.

I am bemused by the fact that most philosophers seem to treat these
issues by examining them in lone individuals, doing lone introspection.
I suspect that all of these concepts are somehow related to
socialization and sex in humans and need to be analyzed from that point
of view.  Thus, even though "consciousness" seems a very private
affair, in ordinary, nonphilosophical situations we use the concept to
make judgements on others, not on ourselves.

For me identity is even more puzzling than consciousness (probably
because I don't understand the issues), especially because so much of
what "I" am goes on "in the dark" and yet all that stuff is part of
what "I" am, again, especially to others.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-26T18:43:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:Enkuh.77329$UC.25257@newsfe5-win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt; I have an instinctive conviction that other people have &quot;experiential
&gt; states&quot; (perhaps what you call &quot;subjective states&quot; thought I would
&gt; have liked to have seen your account of these), and a strong suspicion
&gt; that humans are not the only instantiations of experiential states.
">

I suspect that your conviction that other people are conscious and have
subjective states is not "instinctive," but is *a posteriori*. It is based on
your observations of their behavior, together with your understanding of the
meanings (the use) of those terms. It is not unlike the basis you have for
deciding other people or animals (or plants, for that matter) are alive.

<QUOTE PREVIOUSPOST="
&gt; The question for me, then, is how to defend any propensity to draw a
&gt; line -- i.e. to claim this capacity for some objects and not for others.
&gt; I view this as a spectrum problem, with solipsism at one end
&gt; and a Whiteheadian-style of panexperientialism at the other.
">

The thing to recognize about consciousness is that it is an *imputed*
property. It is not, like the redness of a rose, a manifest property. The
difference between an imputed property (what I call a "paraproperty" in the
paper), and a manifest property is that the latter is attributed (or not) to
an object x solely on the basis of observing x, while paraproperties are
imputed based on observations of states of affairs beyond x (though perhaps
including x). A simple example is nationality --- we cannot decide whether
Alfie is Irish merely by observing Alfie; we have to investigate his
parentage or birthplace. If it proves that Alfie was born in Ireland, then we
attribute that papraproperty, being Irish, to him --- we *impute it* to him.

Paraproperties are imputed for various reasons, an important one of which is
to facilitate explanation. We impute properties in order to explain (and thus
predict) observable behavior. E.g., for the same reason we impute spin to an
electron. So the question of where to "draw the line" is not one of which
things are and are not conscious, as though consciousness is a manifest
property which we can observe if we look carefully enough, but the question
of when, to what things, should we impute it? And the answer is, To whatever
things we could better explain by imputing that (para)property.
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-26T23:42:00 </POSTDATE>
On 26 Jan, 23:43, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;andy-k&quot; &lt;spam.free@last&gt; wrote in news:Enkuh.77329$UC.25257@newsfe5-win.ntli.net:

&gt; &gt; I have an instinctive conviction that other people have &quot;experiential
&gt; &gt; states&quot; (perhaps what you call &quot;subjective states&quot; thought I would
&gt; &gt; have liked to have seen your account of these), and a strong suspicion
&gt; &gt; that humans are not the only instantiations of experiential states.I suspect that your conviction that other people are conscious and have
&gt; subjective states is not &quot;instinctive,&quot; but is *a posteriori*. It is based on
&gt; your observations of their behavior, together with your understanding of the
&gt; meanings (the use) of those terms. It is not unlike the basis you have for
&gt; deciding other people or animals (or plants, for that matter) are alive.

&gt; &gt; The question for me, then, is how to defend any propensity to draw a
&gt; &gt; line -- i.e. to claim this capacity for some objects and not for others.
&gt; &gt; I view this as a spectrum problem, with solipsism at one end
&gt; &gt; and a Whiteheadian-style of panexperientialism at the other.The thing to recognize about consciousness is that it is an *imputed*
&gt; property. It is not, like the redness of a rose, a manifest property. The
&gt; difference between an imputed property (what I call a &quot;paraproperty&quot; in the
&gt; paper), and a manifest property is that the latter is attributed (or not) to
&gt; an object x solely on the basis of observing x, while paraproperties are
&gt; imputed based on observations of states of affairs beyond x (though perhaps
&gt; including x). A simple example is nationality --- we cannot decide whether
&gt; Alfie is Irish merely by observing Alfie; we have to investigate his
&gt; parentage or birthplace. If it proves that Alfie was born in Ireland, then we
&gt; attribute that papraproperty, being Irish, to him --- we *impute it* to him.

&gt; Paraproperties are imputed for various reasons, an important one of which is
&gt; to facilitate explanation. We impute properties in order to explain (and thus
&gt; predict) observable behavior. E.g., for the same reason we impute spin to an
&gt; electron. So the question of where to &quot;draw the line&quot; is not one of which
&gt; things are and are not conscious, as though consciousness is a manifest
&gt; property which we can observe if we look carefully enough, but the question
&gt; of when, to what things, should we impute it? And the answer is, To whatever
&gt; things we could better explain by imputing that (para)property.
&gt;From the materialist perspective, something conscious and experiencing
">

'free will', will always behave as though it isn't conscious. So you
suggest that materialists use behavioural tests to determine whether
something is conscious? How would consciousness ever better explain
behaviour given the materialist perspective?
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-26T23:49:00 </POSTDATE>
On 26 Jan, 23:43, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;andy-k&quot; &lt;spam.free@last&gt; wrote in news:Enkuh.77329$UC.25257@newsfe5-win.ntli.net:

&gt; &gt; I have an instinctive conviction that other people have &quot;experiential
&gt; &gt; states&quot; (perhaps what you call &quot;subjective states&quot; thought I would
&gt; &gt; have liked to have seen your account of these), and a strong suspicion
&gt; &gt; that humans are not the only instantiations of experiential states.I suspect that your conviction that other people are conscious and have
&gt; subjective states is not &quot;instinctive,&quot; but is *a posteriori*. It is based on
&gt; your observations of their behavior, together with your understanding of the
&gt; meanings (the use) of those terms. It is not unlike the basis you have for
&gt; deciding other people or animals (or plants, for that matter) are alive.

&gt; &gt; The question for me, then, is how to defend any propensity to draw a
&gt; &gt; line -- i.e. to claim this capacity for some objects and not for others.
&gt; &gt; I view this as a spectrum problem, with solipsism at one end
&gt; &gt; and a Whiteheadian-style of panexperientialism at the other.The thing to recognize about consciousness is that it is an *imputed*
&gt; property. It is not, like the redness of a rose, a manifest property. The
&gt; difference between an imputed property (what I call a &quot;paraproperty&quot; in the
&gt; paper), and a manifest property is that the latter is attributed (or not) to
&gt; an object x solely on the basis of observing x, while paraproperties are
&gt; imputed based on observations of states of affairs beyond x (though perhaps
&gt; including x). A simple example is nationality --- we cannot decide whether
&gt; Alfie is Irish merely by observing Alfie; we have to investigate his
&gt; parentage or birthplace. If it proves that Alfie was born in Ireland, then we
&gt; attribute that papraproperty, being Irish, to him --- we *impute it* to him.

&gt; Paraproperties are imputed for various reasons, an important one of which is
&gt; to facilitate explanation. We impute properties in order to explain (and thus
&gt; predict) observable behavior. E.g., for the same reason we impute spin to an
&gt; electron. So the question of where to &quot;draw the line&quot; is not one of which
&gt; things are and are not conscious, as though consciousness is a manifest
&gt; property which we can observe if we look carefully enough, but the question
&gt; of when, to what things, should we impute it? And the answer is, To whatever
&gt; things we could better explain by imputing that (para)property.
">

.

<QUOTE PREVIOUSPOST="
&gt;From the materialist perspective, something conscious and experiencing
">

'free will', will always behave as though it isn't conscious. So you
suggest that materialists use behavioural tests to determine whether
something is conscious? How would consciousness ever better explain
behaviour given the materialist perspective?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-27T01:15:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1169872931.011614.124510@s48g2000cws.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; From the materialist perspective, something conscious and experiencing
&gt; 'free will', will always behave as though it isn't conscious. So you
&gt; suggest that materialists use behavioural tests to determine whether
&gt; something is conscious? How would consciousness ever better explain
&gt; behaviour given the materialist perspective?
">

Any materialist who would argue that "something conscious and experiencing
'free will' will always behave as though it isn't conscious" would not
understand the concept of consciousness, and the role it plays in our
explanatory apparatus.

He *might* argue that a system could behave in exactly the same way, with or
without having subjective states. That is precisely the "zombie argument."
That argument is fallacious (although the conclusion is true for other
reasons). The rebuttal is complicated, but the gist of it is, that if the
"zombie" behaved in exactly the same way as humans, then we would have the
very same reason for imputing subjective states to it that we have for
imputing them to humans. The basis for imputing subjective states, like
consciousness itself, is objective.
</POST>
<POST>
<POSTER> mikegor...@xtra.co.nz </POSTER>
<POSTDATE> 2007-01-27T02:41:00 </POSTDATE>
On Jan 25, 9:35 am, Publius

<QUOTE PREVIOUSPOST="
&gt; I shall begin by boldly declaring what consciousness is......
">

Followed by a load of nonsensical Kantian idiotic meaningless piffle, a
definition of absolute stupidty.

Consciousness is a state of existence characterized by having a state
of awareness.

Consciousness is the faculty that perceives that which exists.

"Directly or indirectly, every phenomenon of consciousness is derived
from one's awareness of the external world. Some object, i.e., some
content, is involved in every state of awareness. Extrospection is a
process of cognition directed outward -- a process of apprehending some
existent(s) of the external world. Introspection is a process of
cognition directed inward -- a process of apprehending one's own
psychological actions in regard to some existent(s) of the external
world, such actions as thinking, feeling, reminiscing, etc. ... A
content-less state of consciousness is a contradiction in terms."
Ayn Rand

Because to be conscious is to perceive something, consciousness
requires something outside of itself in order to function;
consciousness requires and is dependent upon, existence. Even if it
were possible AND ITS NOT, the state of consciousness would require a
state of existing consciousness to identify it.

Michael Gordge
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-27T12:19:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt; I have an instinctive conviction that other people have &quot;experiential
&gt;&gt; states&quot; (perhaps what you call &quot;subjective states&quot; thought I would
&gt;&gt; have liked to have seen your account of these), and a strong suspicion
&gt;&gt; that humans are not the only instantiations of experiential states.

&gt; I suspect that your conviction that other people are conscious and have
&gt; subjective states is not &quot;instinctive,&quot; but is *a posteriori*. It is based
&gt; on your observations of their behavior, together with your understanding
&gt; of the meanings (the use) of those terms. It is not unlike the basis you
&gt; have for deciding other people or animals (or plants, for that matter) are
&gt; alive.
">

There is an argument that empathy is instinctive in our species,
and I find the argument compelling.

<QUOTE PREVIOUSPOST="
&gt;&gt; The question for me, then, is how to defend any propensity to draw a
&gt;&gt; line -- i.e. to claim this capacity for some objects and not for others.
&gt;&gt; I view this as a spectrum problem, with solipsism at one end
&gt;&gt; and a Whiteheadian-style of panexperientialism at the other.

&gt; The thing to recognize about consciousness is that it is an *imputed*
&gt; property. It is not, like the redness of a rose, a manifest property. The
&gt; difference between an imputed property (what I call a &quot;paraproperty&quot; in
&gt; the paper), and a manifest property is that the latter is attributed (or
&gt; not) to an object x solely on the basis of observing x, while
&gt; paraproperties are imputed based on observations of states of affairs
&gt; beyond x (though perhaps including x). A simple example is nationality ---
&gt; we cannot decide whether Alfie is Irish merely by observing Alfie; we have
&gt; to investigate his parentage or birthplace. If it proves that Alfie was
&gt; born in Ireland, then we attribute that papraproperty, being Irish, to
&gt; him --- we *impute it* to him.

&gt; Paraproperties are imputed for various reasons, an important one of which
&gt; is to facilitate explanation. We impute properties in order to explain
&gt; (and thus predict) observable behavior. E.g., for the same reason we
&gt; impute spin to an electron. So the question of where to &quot;draw the line&quot; is
&gt; not one of which things are and are not conscious, as though consciousness
&gt; is a manifest property which we can observe if we look carefully enough,
&gt; but the question of when, to what things, should we impute it? And the
&gt; answer is, To whatever things we could better explain by imputing that
&gt; (para)property.
">

The idea of 'self' is evident in behavior, and I would concur with your view
if we are using the word 'consciousness' to denote that aspect of behavior.
But if we are using the word 'consciousness' to denote the kind of
experience that is a pre-requisite for the experience of an idea of 'self',
then I'm still at a loss to know how one would discern those objects upon
which it should be conferred and those to which it should be denied.
What would be better explained by imputing (or denying) pre-self-aware
experience to, say, an amoeba?
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-27T12:51:00 </POSTDATE>
On 27 Jan, 06:15, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1169872931.011614.124510@s48g2000cws.googlegroups.com:

&gt; &gt; From the materialist perspective, something conscious and experiencing
&gt; &gt; 'free will', will always behave as though it isn't conscious. So you
&gt; &gt; suggest that materialists use behavioural tests to determine whether
&gt; &gt; something is conscious? How would consciousness ever better explain
&gt; &gt; behaviour given the materialist perspective?Any materialist who would argue that &quot;something conscious and experiencing
&gt; 'free will' will always behave as though it isn't conscious&quot; would not
&gt; understand the concept of consciousness, and the role it plays in our
&gt; explanatory apparatus.

&gt; He *might* argue that a system could behave in exactly the same way, with or
&gt; without having subjective states. That is precisely the &quot;zombie argument.&quot;
&gt; That argument is fallacious (although the conclusion is true for other
&gt; reasons). The rebuttal is complicated, but the gist of it is, that if the
&gt; &quot;zombie&quot; behaved in exactly the same way as humans, then we would have the
&gt; very same reason for imputing subjective states to it that we have for
&gt; imputing them to humans. The basis for imputing subjective states, like
&gt; consciousness itself, is objective.
">

How can the basis for imputing subjective states be objective, if no
expected difference in behaviour is expected between something which
has conscious experiences is assumed to behave the same as something
that doesn't have conscious experiences? How can if there is no
expected difference in behaviour, can behaviour be used as a criteria
by which consciousness is imputed?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-27T21:43:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:bQLuh.61512$v4.30386@newsfe3-win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt;&gt; I suspect that your conviction that other people are conscious and have
&gt;&gt; subjective states is not &quot;instinctive,&quot; but is *a posteriori*. It is
&gt;&gt; based on your observations of their behavior, together with your
&gt;&gt; understanding of the meanings (the use) of those terms. It is not
&gt;&gt; unlike the basis you have for deciding other people or animals (or
&gt;&gt; plants, for that matter) are alive.

&gt; There is an argument that empathy is instinctive in our species,
&gt; and I find the argument compelling.
">

It may be, but empathy is not the cognitive faculty by which you recognize
consciousness. It is an affective response to that recognition.

<QUOTE PREVIOUSPOST="
&gt; The idea of 'self' is evident in behavior, and I would concur with your
&gt; view if we are using the word 'consciousness' to denote that aspect of
&gt; behavior. But if we are using the word 'consciousness' to denote the
&gt; kind of experience that is a pre-requisite for the experience of an idea
&gt; of 'self', then I'm still at a loss to know how one would discern those
&gt; objects upon which it should be conferred and those to which it should
&gt; be denied. What would be better explained by imputing (or denying)
&gt; pre-self-aware experience to, say, an amoeba?
">

Probably none. That is why we are hesitant to apply it to amoeba. We can
probably explain their behavior well enough with simple S-R models. But when
you get to insects, e.g., honeybees, imputing consciousness and subjective
states may be more predictive.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-27T22:33:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1169920301.831465.320150@h3g2000cwc.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; How can the basis for imputing subjective states be objective, if no
&gt; expected difference in behaviour is expected between something which
&gt; has conscious experiences is assumed to behave the same as something
&gt; that doesn't have conscious experiences?
">

That just says we can imagine explanatory models of organisms which exhibit
complex behaviors which do not postulate subjective states (and therefore
consciousness). And we can. So it is a question of whether those models are
more predictive than those which *do* postulate subjective states.

Furthermore, we can show empirically that some organisms do have subjective
states. So the question is only one of whether those states help explain the
organisms' behavior, and whether models which employ them do a better job
than models which ignore them.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-28T02:22:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt;&gt; I suspect that your conviction that other people are conscious
&gt;&gt;&gt; and have subjective states is not &quot;instinctive,&quot; but is *a posteriori*.
&gt;&gt;&gt; It is based on your observations of their behavior, together with
&gt;&gt;&gt; your understanding of the meanings (the use) of those terms.
&gt;&gt;&gt; It is not unlike the basis you have for deciding other people or
&gt;&gt;&gt; animals (or plants, for that matter) are alive.

&gt;&gt; There is an argument that empathy is instinctive in our species,
&gt;&gt; and I find the argument compelling.

&gt; It may be, but empathy is not the cognitive faculty by which you
&gt; recognize consciousness. It is an affective response to that recognition.
">

How could my conviction that other people are conscious be an
a_posteriori construction when my instinctive modes of behavior
already provide an affective response to that recognition?

<QUOTE PREVIOUSPOST="
&gt;&gt; The idea of 'self' is evident in behavior, and I would concur with your
&gt;&gt; view if we are using the word 'consciousness' to denote that aspect of
&gt;&gt; behavior. But if we are using the word 'consciousness' to denote the
&gt;&gt; kind of experience that is a pre-requisite for the experience of an idea
&gt;&gt; of 'self', then I'm still at a loss to know how one would discern those
&gt;&gt; objects upon which it should be conferred and those to which it should
&gt;&gt; be denied. What would be better explained by imputing (or denying)
&gt;&gt; pre-self-aware experience to, say, an amoeba?

&gt; Probably none. That is why we are hesitant to apply it to amoeba.
&gt; We can probably explain their behavior well enough with simple
&gt; S-R models. But when you get to insects, e.g., honeybees, imputing
&gt; consciousness and subjective states may be more predictive.
">

Should we not also be hesitant to *deny* it to the amoeba?
And if so, then I'm still at a loss to know where to draw the line.
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-28T07:33:00 </POSTDATE>
On 28 Jan, 03:33, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1169920301.831465.320150@h3g2000cwc.googlegroups.com:

&gt; &gt; How can the basis for imputing subjective states be objective, if no
&gt; &gt; expected difference in behaviour is expected between something which
&gt; &gt; has conscious experiences is assumed to behave the same as something
&gt; &gt; that doesn't have conscious experiences?That just says we can imagine explanatory models of organisms which exhibit
&gt; complex behaviors which do not postulate subjective states (and therefore
&gt; consciousness). And we can. So it is a question of whether those models are
&gt; more predictive than those which *do* postulate subjective states.

&gt; Furthermore, we can show empirically that some organisms do have subjective
&gt; states. So the question is only one of whether those states help explain the
&gt; organisms' behavior, and whether models which employ them do a better job
&gt; than models which ignore them.
">

So you are saying that even though you would believe that whatever the
conscious experiences, the behaviour will not be effected, that these
conscious experiences could provide a better model for explaining what
is actually going on, even though for them to be able to, would
indicate a causal link between the conscious experiences and
behaviour, unless you were just imputing consciousness, regardless of
whether any consciousness was experiencing being the object, and
saying well if the story helps to predict then use it. So you could
impute consciousness to your television set, and suggest that the
reason it changes channel when you press the remote, is that the
television gets a tickily sensation down one side or the other, for
whether program up or down, and special tickles for specific channels.
Is that the type of thing you hope to be able to do with your imputed
consciousness, given that any conscious experiences would not effect
behaviour from the materialist perspective.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-28T23:58:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1169987616.902497.233460@a34g2000cwb.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; So you are saying that even though you would believe that whatever the
&gt; conscious experiences, the behaviour will not be effected, that these
&gt; conscious experiences could provide a better model for explaining what
&gt; is actually going on, even though for them to be able to, would
&gt; indicate a causal link between the conscious experiences and
&gt; behaviour, unless you were just imputing consciousness, regardless of
&gt; whether any consciousness was experiencing being the object, and
&gt; saying well if the story helps to predict then use it.
">

You have several statements there. "You would believe that whatever the
conscious experiences, the behaviour will not be effected."

I believe we can imagine systems which exhibit the same behavior (as humans,
for example) without being conscious, i.e., without them having subjective
states. E.g., zombies. But we could determine, for any alleged zombie,
whether it does or does not have subjective states. If it does, then we'd
probably want to impute consciousness to it, for the same reason we impute it
to humans. Those subjective states (and thus consciousness) would indeed
affect the behavior of that system. That's why we would impute them --- to
help explain, and thus predict, the system's behavior.

BTW, to say that A is the cause of B is to say that given A we can predict B.

<QUOTE PREVIOUSPOST="
&gt; So you could
&gt; impute consciousness to your television set, and suggest that the
&gt; reason it changes channel when you press the remote, is that the
&gt; television gets a tickily sensation down one side or the other, for
&gt; whether program up or down, and special tickles for specific channels.
&gt; Is that the type of thing you hope to be able to do with your imputed
&gt; consciousness, given that any conscious experiences would not effect
&gt; behaviour from the materialist perspective.
">

If imputing tickling sensations to the television helped us predict when it
would change channels, then we would be warranted in imputing them. But it
doesn't, so we don't. If the imputed states do not help us predict the
behavior, then there is no reason to impute them.
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-29T03:46:00 </POSTDATE>
On 29 Jan, 04:58, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1169987616.902497.233460@a34g2000cwb.googlegroups.com:

&gt; &gt; So you are saying that even though you would believe that whatever the
&gt; &gt; conscious experiences, the behaviour will not be effected, that these
&gt; &gt; conscious experiences could provide a better model for explaining what
&gt; &gt; is actually going on, even though for them to be able to, would
&gt; &gt; indicate a causal link between the conscious experiences and
&gt; &gt; behaviour, unless you were just imputing consciousness, regardless of
&gt; &gt; whether any consciousness was experiencing being the object, and
&gt; &gt; saying well if the story helps to predict then use it.You have several statements there. &quot;You would believe that whatever the
&gt; conscious experiences, the behaviour will not be effected.&quot;

&gt; I believe we can imagine systems which exhibit the same behavior (as humans,
&gt; for example) without being conscious, i.e., without them having subjective
&gt; states. E.g., zombies. But we could determine, for any alleged zombie,
&gt; whether it does or does not have subjective states. If it does, then we'd
&gt; probably want to impute consciousness to it, for the same reason we impute it
&gt; to humans. Those subjective states (and thus consciousness) would indeed
&gt; affect the behavior of that system. That's why we would impute them --- to
&gt; help explain, and thus predict, the system's behavior.

&gt; BTW, to say that A is the cause of B is to say that given A we can predict B.

&gt; &gt; So you could
&gt; &gt; impute consciousness to your television set, and suggest that the
&gt; &gt; reason it changes channel when you press the remote, is that the
&gt; &gt; television gets a tickily sensation down one side or the other, for
&gt; &gt; whether program up or down, and special tickles for specific channels.
&gt; &gt; Is that the type of thing you hope to be able to do with your imputed
&gt; &gt; consciousness, given that any conscious experiences would not effect
&gt; &gt; behaviour from the materialist perspective.If imputing tickling sensations to the television helped us predict when it
&gt; would change channels, then we would be warranted in imputing them. But it
&gt; doesn't, so we don't. If the imputed states do not help us predict the
&gt; behavior, then there is no reason to impute them.
">

I didn't understand what your comment was in reference to.
"BTW, to say that A is the cause of B is to say that given A we can
predict B."

Given that materialism says that consciousness or any conscious
experiences don't in themselves effect behaviour (as something
conscious will always act as though it isn't), are you in a sense
saying, that even A cannot be the cause of B nor effect it, that
sometimes, in materialism, it may be warranted to make up false
explanations in which A effects B (and thus can be used in predicting
it)?

BTW, what would demonstrate a subjective state? Would a television
having different channels, and not all televisions having the same
channel on count as a subjective channel state to which might be a
candidate of imbuing consciousness to, or because we built the
machine, and know from a physical basis why it works like it does,
that conscious cannot be imbued upon it. Presumably if so, then it
would seem to follow that once it is known how something works from
the materialist perspective, then it is no longer necessary to imbue
consciousness, and it would lose its imbued consciousness status,
would this match with your understanding?
</POST>
<POST>
<POSTER> jillaront...@webtv.net </POSTER>
<POSTDATE> 2007-01-29T06:26:00 </POSTDATE>
Kant was clueless to objective modes of BEING and left it in the dark
[Heidegger 1927]. It is a dubious procedure to make covert judgements of
common reason by invoking self-evidence as a theme in the analytic
business of philosophy [Heidegger].

Being-"there" [Da-Sein] is average based world knowledge. There is no
circularity of reasoning because it lays bare. It has plain sense
constitution, one that is not a self-evident meta-derivation
[Jillar-Heidegger]. Nor is it an axiom, which "a sequence of
propositions are deductively derived." [Heidegger].

Entities relate to Being [Greeks]. "The Being of entities is not itself
an entity." [Heidegger]. We do not need a "story" for its origin. Its
origin need not be from another entity [Greeks-Heidegger]. The
auto-biographical thesis of the "I" is a variation on Greek notion.
Foucault narrates various classical Greek self-technologies.

The mode of Being of an entity lays bare the ground for objective
science:

"Being is always the Being of an entity. The totality of entities can,
in accordance with its various domains, become a field for laying bare
and delimiting certain definitive areas of SUBJECT-matter. These areas,
on their part {for instance history, Nature, space, life, Dasein,
language, and the like}, can serve as OBJECTS which corresponding
scientific research accomplishes, roughly and naively, the demarcation
and initial fixing of the areas of SUBJECT-matter." [Heidegger 1927].

It should be clear from the above that, zombie behavior would fail the
authentic test of Being based on origins. The case for artificial Being
is a case arguing on behalf of artificial modes of Being. Artificial
life and language are scientific disciplines making such agruments. But
20th century existential thought has set higher benchmarks. There is no
manifest history for a zombie Being! No manifestos, mission statements.
No dialecticalisms. No objective mode of their Being-there! No zombie
has given us their self-technology objectively.

Caps for topic emphasis and name inserts for credit to sources by
Jillar.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-29T15:47:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in news:_aYuh.61562$v4.61161@newsfe3-
win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt;&gt; It may be, but empathy is not the cognitive faculty by which you
&gt;&gt; recognize consciousness. It is an affective response to that recognition.
&gt; How could my conviction that other people are conscious be an
&gt; a_posteriori construction when my instinctive modes of behavior
&gt; already provide an affective response to that recognition?
">

Well, empathy is considered to be the state of seeing oneself in others.
Hence when you identify others as conscious, a feeling of kinship is evoked.
But you have to identify them as conscious entities first. It is no different
from salivating when you see a favorite food. Unless you recognize that food,
there will be no salivation. It is not via the salivation response that you
identify the food --- it is appearance, aroma, etc.

<QUOTE PREVIOUSPOST="
&gt;&gt;&gt; The idea of 'self' is evident in behavior, and I would concur with your
&gt;&gt;&gt; view if we are using the word 'consciousness' to denote that aspect of
&gt;&gt;&gt; behavior. But if we are using the word 'consciousness' to denote the
&gt;&gt;&gt; kind of experience that is a pre-requisite for the experience of an idea
&gt;&gt;&gt; of 'self', then I'm still at a loss to know how one would discern those
&gt;&gt;&gt; objects upon which it should be conferred and those to which it should
&gt;&gt;&gt; be denied. What would be better explained by imputing (or denying)
&gt;&gt;&gt; pre-self-aware experience to, say, an amoeba?
&gt;&gt; Probably none. That is why we are hesitant to apply it to amoeba.
&gt;&gt; We can probably explain their behavior well enough with simple
&gt;&gt; S-R models. But when you get to insects, e.g., honeybees, imputing
&gt;&gt; consciousness and subjective states may be more predictive.
&gt; Should we not also be hesitant to *deny* it to the amoeba?
&gt; And if so, then I'm still at a loss to know where to draw the line.
">

We should be hesitant if imputing it to the amoeba gains us no explanatory
advantage. To do so would violate Occam's principle: don't multiply entities
needlessly.
</POST>
<POST>
<POSTER> "Publius" &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-29T16:36:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in message
">

news:1170060376.375241.16290@s48g2000cws.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt; I didn't understand what your comment was in reference to.
&gt; &quot;BTW, to say that A is the cause of B is to say that given A we can
&gt; predict B.&quot;
">

A causal relation between two events A and B, separated in time, is a
relation such that if we know A at time T0 we can can predict B at time T1.

<QUOTE PREVIOUSPOST="
&gt; Given that materialism says that consciousness or any conscious
&gt; experiences don't in themselves effect behaviour (as something
&gt; conscious will always act as though it isn't), are you in a sense
&gt; saying, that even A cannot be the cause of B nor effect it, that
&gt; sometimes, in materialism, it may be warranted to make up false
&gt; explanations in which A effects B (and thus can be used in predicting
&gt; it)?
">

Actually, all "materialists" don't say that. Only some, called
"epiphenomenalists" do. Others, called "reductionists," hold that conscious
states exist and can affect behavior but that they can be reduced to
physical processes. Still others (eliminativists) hold that there are no
conscious states --- it is a kind of linguistic confusion, or mythology.

You only have a "false explanation" if the explanation fails to make
testable predictions, or makes predictions which fail to be confirmed.

I suspect you are supposing that an explanation is "true" or "false" if it
"corresponds with reality." I.e., you are a realist. But we have no access
to a "reality" beyond what is presented to our senses. What is "real" is
whatever allows us to make the most sense of experience.

<QUOTE PREVIOUSPOST="
&gt; BTW, what would demonstrate a subjective state?
">

We can confirm that a system is in a subjective state if it can:

1) Discriminate between two or more stimuli (tell them apart), but

2) Cannot describe the differences between the stimuli.

<QUOTE PREVIOUSPOST="
&gt; Presumably if so, then it
&gt; would seem to follow that once it is known how something works from
&gt; the materialist perspective, then it is no longer necessary to imbue
&gt; consciousness, and it would lose its imbued consciousness status,
&gt; would this match with your understanding?
">

"Consciousness" is just the capacity for having subjective states. It is not
unlike imputing, say, talent. We say that a person has *talent* if she can
play the piano well. It is a property we impute --- it just means a capacity
to play the piano well. But it is imputed based on her actually playing the
piano well, which we can observe.
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-29T19:29:00 </POSTDATE>
On 29 Jan, 21:36, "Publius" &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in message news:1170060376.375241.16290@s48g2000cws.googlegroups.com ...

&gt; &gt; I didn't understand what your comment was in reference to.
&gt; &gt; &quot;BTW, to say that A is the cause of B is to say that given A we can
&gt; &gt; predict B.&quot;A causal relation between two events A and B, separated in time, is a
&gt; relation such that if we know A at time T0 we can can predict B at time T1.

&gt; &gt; Given that materialism says that consciousness or any conscious
&gt; &gt; experiences don't in themselves effect behaviour (as something
&gt; &gt; conscious will always act as though it isn't), are you in a sense
&gt; &gt; saying, that even A cannot be the cause of B nor effect it, that
&gt; &gt; sometimes, in materialism, it may be warranted to make up false
&gt; &gt; explanations in which A effects B (and thus can be used in predicting
&gt; &gt; it)?Actually, all &quot;materialists&quot; don't say that. Only some, called
&gt; &quot;epiphenomenalists&quot; do. Others, called &quot;reductionists,&quot; hold that conscious
&gt; states exist and can affect behavior but that they can be reduced to
&gt; physical processes. Still others (eliminativists) hold that there are no
&gt; conscious states --- it is a kind of linguistic confusion, or mythology.

&gt; You only have a &quot;false explanation&quot; if the explanation fails to make
&gt; testable predictions, or makes predictions which fail to be confirmed.

&gt; I suspect you are supposing that an explanation is &quot;true&quot; or &quot;false&quot; if it
&gt; &quot;corresponds with reality.&quot; I.e., you are a realist. But we have no access
&gt; to a &quot;reality&quot; beyond what is presented to our senses. What is &quot;real&quot; is
&gt; whatever allows us to make the most sense of experience.

&gt; &gt; BTW, what would demonstrate a subjective state?We can confirm that a system is in a subjective state if it can:

&gt; 1) Discriminate between two or more stimuli (tell them apart), but

&gt; 2) Cannot describe the differences between the stimuli.

&gt; &gt; Presumably if so, then it
&gt; &gt; would seem to follow that once it is known how something works from
&gt; &gt; the materialist perspective, then it is no longer necessary to imbue
&gt; &gt; consciousness, and it would lose its imbued consciousness status,
&gt; &gt; would this match with your understanding?&quot;Consciousness&quot; is just the capacity for having subjective states. It is not
&gt; unlike imputing, say, talent. We say that a person has *talent* if she can
&gt; play the piano well. It is a property we impute --- it just means a capacity
&gt; to play the piano well. But it is imputed based on her actually playing the
&gt; piano well, which we can observe.
">

So the ones called "reductionists," which hold that conscious states
exist and can affect behaviour but that they can be reduced to
physical processes, would disagree with the behaviourists about
whether a robot is conscious. They would expect to see it to behave
other than according to its build and programming?

Are you being serious about the "eliminativists"?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-29T22:33:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1170116980.082359.288340@h3g2000cwc.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; So the ones called &quot;reductionists,&quot; which hold that conscious states
&gt; exist and can affect behaviour but that they can be reduced to
&gt; physical processes, would disagree with the behaviourists about
&gt; whether a robot is conscious. They would expect to see it to behave
&gt; other than according to its build and programming?
">

Behaviorism and reductionism refer to two different questions, not to two
different answers to the same question. Behaviorism is a methodological, or
epistemological, stance which claims that theories of behavior must be based
strictly on what can be objectively observed in behavior (as opposed, say, to
introspection). Reductionism is a metaphysical thesis concerning the
ontological status of conscious experience. Many behaviorists are
reductionists, but they need not be.

<QUOTE PREVIOUSPOST="
&gt; Are you being serious about the &quot;eliminativists&quot;?
">

http://plato.stanford.edu/entries/materialism-eliminative/
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-29T23:15:00 </POSTDATE>
On 30 Jan, 03:33, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1170116980.082359.288340@h3g2000cwc.googlegroups.com:

&gt; &gt; So the ones called &quot;reductionists,&quot; which hold that conscious states
&gt; &gt; exist and can affect behaviour but that they can be reduced to
&gt; &gt; physical processes, would disagree with the behaviourists about
&gt; &gt; whether a robot is conscious. They would expect to see it to behave
&gt; &gt; other than according to its build and programming?Behaviorism and reductionism refer to two different questions, not to two
&gt; different answers to the same question. Behaviorism is a methodological, or
&gt; epistemological, stance which claims that theories of behavior must be based
&gt; strictly on what can be objectively observed in behavior (as opposed, say, to
&gt; introspection). Reductionism is a metaphysical thesis concerning the
&gt; ontological status of conscious experience. Many behaviorists are
&gt; reductionists, but they need not be.

&gt; &gt; Are you being serious about the &quot;eliminativists&quot;? http://plato.stanford.edu/entries/materialism-eliminative/
">

So the reductionists would disagree with those who would use
behavioural tests in regards to whether a robot is conscious. They
would expect to see it to behave other than according to its build and
programming?
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-29T23:28:00 </POSTDATE>
On 30 Jan, 04:15, "someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On 30 Jan, 03:33, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

&gt; &gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1170116980.082359.288340@h3g2000cwc.googlegroups.com:

&gt; &gt; &gt; So the ones called &quot;reductionists,&quot; which hold that conscious states
&gt; &gt; &gt; exist and can affect behaviour but that they can be reduced to
&gt; &gt; &gt; physical processes, would disagree with the behaviourists about
&gt; &gt; &gt; whether a robot is conscious. They would expect to see it to behave
&gt; &gt; &gt; other than according to its build and programming?Behaviorism and reductionism refer to two different questions, not to two
&gt; &gt; different answers to the same question. Behaviorism is a methodological, or
&gt; &gt; epistemological, stance which claims that theories of behavior must be based
&gt; &gt; strictly on what can be objectively observed in behavior (as opposed, say, to
&gt; &gt; introspection). Reductionism is a metaphysical thesis concerning the
&gt; &gt; ontological status of conscious experience. Many behaviorists are
&gt; &gt; reductionists, but they need not be.

&gt; &gt; &gt; Are you being serious about the &quot;eliminativists&quot;? http://plato.stanford.edu/entries/materialism-eliminative/So the reductionists would disagree with those who would use
&gt; behavioural tests in regards to whether a robot is conscious. They
&gt; would expect to see it to behave other than according to its build and
&gt; programming?
">

Sorry ignore that reply.

I see what your saying. A reductionist would advocate using
behavioural tests (and the behaviourist methodology), as they believe
consciousness is influential to behaviour. So reductionists would
expect the robot to behave other than according to its build and
program if it were conscious.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-30T00:31:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1170131303.847464.167900@a75g2000cwd.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; I see what your saying. A reductionist would advocate using
&gt; behavioural tests (and the behaviourist methodology), as they believe
&gt; consciousness is influential to behaviour. So reductionists would
&gt; expect the robot to behave other than according to its build and
&gt; program if it were conscious.
">

They would not necessarily *expect* it to, but they would allow that it
*might* exhibit behaviors not predictable from its programming. If it did,
then postulating consciousness might be justified. But they would argue that
the behavior is still reducible to the programming. Complex systems can
exhibit unpredictable behaviors. For example, it is not possible to predict
where and when the next hurricane will strike, but no one doubts that
hurricanes are perfectly accounted for by the laws of physics.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-30T01:04:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt;&gt; It may be, but empathy is not the cognitive faculty by which you
&gt;&gt;&gt; recognize consciousness. It is an affective response to that
&gt;&gt;&gt; recognition.

&gt;&gt; How could my conviction that other people are conscious be an
&gt;&gt; a_posteriori construction when my instinctive modes of behavior
&gt;&gt; already provide an affective response to that recognition?

&gt; Well, empathy is considered to be the state of seeing oneself in others.
">

There are numerous definitions of empathy that are more to my liking here:
http://en.wikipedia.org/wiki/Empathy

<QUOTE PREVIOUSPOST="
&gt; Hence when you identify others as conscious, a feeling of kinship
&gt; is evoked. But you have to identify them as conscious entities first.
&gt; It is no different from salivating when you see a favorite food.
&gt; Unless you recognize that food, there will be no salivation.
&gt; It is not via the salivation response that you identify the food ---
&gt; it is appearance, aroma, etc.
">

I don't believe that the two are as distinct as you seem to believe
they are -- to identify others as conscious is to believe them to
suffer pain and enjoy pleasure just as I do, and to empathize is to
imagine their suffering and their pleasure as my own. Consequently
I'm not yet convinced that your reply answers my question.

<QUOTE PREVIOUSPOST="
&gt;&gt;&gt;&gt; The idea of 'self' is evident in behavior, and I would concur with your
&gt;&gt;&gt;&gt; view if we are using the word 'consciousness' to denote that aspect of
&gt;&gt;&gt;&gt; behavior. But if we are using the word 'consciousness' to denote the
&gt;&gt;&gt;&gt; kind of experience that is a pre-requisite for the experience of an
&gt;&gt;&gt;&gt; idea of 'self', then I'm still at a loss to know how one would discern
&gt;&gt;&gt;&gt; those objects upon which it should be conferred and those to which it
&gt;&gt;&gt;&gt; should be denied. What would be better explained by imputing (or
&gt;&gt;&gt;&gt; denying) pre-self-aware experience to, say, an amoeba?

&gt;&gt;&gt; Probably none. That is why we are hesitant to apply it to amoeba.
&gt;&gt;&gt; We can probably explain their behavior well enough with simple
&gt;&gt;&gt; S-R models. But when you get to insects, e.g., honeybees, imputing
&gt;&gt;&gt; consciousness and subjective states may be more predictive.

&gt;&gt; Should we not also be hesitant to *deny* it to the amoeba?
&gt;&gt; And if so, then I'm still at a loss to know where to draw the line.

&gt; We should be hesitant if imputing it to the amoeba gains us no
&gt; explanatory advantage. To do so would violate Occam's principle:
&gt; don't multiply entities needlessly.
">

My first impression was that you'd performed what Chalmers calls a
"bait-and-switch", but then it occurred to me that you may be arguing
that Chalmers is wrong on precisely that ground -- i.e. that he thinks
there's something more to consciousness than behavioral responses.
Eliminativism is certainly one way of eradicating his "explanatory gap",
but one that seems to me to deny the blatantly obvious.

You say in your recent reply to "someone2" in this thread that:

<QUOTE PREVIOUSPOST="
&gt; We can confirm that a system is in a subjective state if it can:

&gt; 1) Discriminate between two or more stimuli (tell them apart), but

&gt; 2) Cannot describe the differences between the stimuli.
">

I presume that you would deny subjective states to the amoeba
because it fails to satisfy condition 2, but then no other species
but ours could possibly satisfy condition 2 since it demands that
the system be a language user. In this case I wonder how you
could impute subjective states to chimpanzees.
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-01-30T09:58:00 </POSTDATE>
On 30 Jan, 05:31, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1170131303.847464.167900@a75g2000cwd.googlegroups.com:

&gt; &gt; I see what your saying. A reductionist would advocate using
&gt; &gt; behavioural tests (and the behaviourist methodology), as they believe
&gt; &gt; consciousness is influential to behaviour. So reductionists would
&gt; &gt; expect the robot to behave other than according to its build and
&gt; &gt; program if it were conscious.They would not necessarily *expect* it to, but they would allow that it
&gt; *might* exhibit behaviors not predictable from its programming. If it did,
&gt; then postulating consciousness might be justified. But they would argue that
&gt; the behavior is still reducible to the programming. Complex systems can
&gt; exhibit unpredictable behaviors. For example, it is not possible to predict
&gt; where and when the next hurricane will strike, but no one doubts that
&gt; hurricanes are perfectly accounted for by the laws of physics.
">

Ah, so reductionists expect any behaviours which weren't predictable
from its build and programming, to ultimately be explainable in terms
of the build and programming, you are presumably just pointing out for
clarity that it is possible that the robot might contain a bug, or an
unplanned feature.

They would still expect it to act the same regardless of  whether it
was or wasn't conscious. Which is slightly confusing because I thought
you had said that reductionists viewed consciousness to be
influential, or was that in terms of imbued consciousness, where they
imbue something (say the robots circuits) as being the same as
consciousness (they say that is the imbued consciousness), and
therefore state that the advantage of the imbued consciousness is
obvious, as the robot wouldn't work the same without it. Which would
seem to suggest that they have the power to imbue an advantage.

If so that would seem like poor reasoning, as it should have been
obvious from the materialist prediction that anything conscious and
experiencing 'free will' will act as though it isn't, that
consciousness cannot be influential from a materialist perspective.

To resolve the conflict, it would seem like they should have been
assuming that if consciousness and the neural activity were the same
thing, then that thing has two sides to it, the conscious, and a
physical side, and it would be the physical side that is an advantage,
the conscious side would offer no advantage, nor offer any influence,
in other words the behaviour is not influenced by whether it is aware
of itself. Do you agree?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-31T02:04:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in news:ndBvh.59908$1W1.18262@newsfe4-
win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt;&gt; Hence when you identify others as conscious, a feeling of kinship
&gt;&gt; is evoked. But you have to identify them as conscious entities first.
&gt;&gt; It is no different from salivating when you see a favorite food.
&gt;&gt; Unless you recognize that food, there will be no salivation.
&gt;&gt; It is not via the salivation response that you identify the food ---
&gt;&gt; it is appearance, aroma, etc.
&gt; I don't believe that the two are as distinct as you seem to believe
&gt; they are -- to identify others as conscious is to believe them to
&gt; suffer pain and enjoy pleasure just as I do, and to empathize is to
&gt; imagine their suffering and their pleasure as my own. Consequently
&gt; I'm not yet convinced that your reply answers my question.
">

I think you are saying the same thing I said earlier --- that to impute
consciousness to a system is to impute a capacity for having various
subjective states. You say "believe them to suffer pain and enjoy
pleasure." Suffering pain and enjoying pleasure are indeed among the
subjective states we mean the system might have when we impute
consciousness to it. Being able to experience pain and pleasure are implied
when we impute consciousness, but they are not the *basis* upon which we
impute that capacity. We impute it based on the organism's behavior and
other observable properties. Behavior and observable properties are the
basis, the truth conditions, for imputing consciousness; being able to
experience pain and pleasure, and other subjective states, are among the
implications of that attribution.

You seem to be saying that P's empathy is evidence for Q's consciousness.
But that can't possibly be, unless some further assumptions are made.

<QUOTE PREVIOUSPOST="
&gt;&gt; We should be hesitant if imputing it to the amoeba gains us no
&gt;&gt; explanatory advantage. To do so would violate Occam's principle:
&gt;&gt; don't multiply entities needlessly.
&gt; My first impression was that you'd performed what Chalmers calls a
&gt; &quot;bait-and-switch&quot;, but then it occurred to me that you may be arguing
&gt; that Chalmers is wrong on precisely that ground -- i.e. that he thinks
&gt; there's something more to consciousness than behavioral responses.
&gt; Eliminativism is certainly one way of eradicating his &quot;explanatory gap&quot;,
&gt; but one that seems to me to deny the blatantly obvious.
">

No --- there is indeed something more to consciousness than behavioral
responses. Chalmers is not wrong there. The "something more" is subjective
states. To say that a system is conscious is to say that it can have
subjective states. Behavior is the *evidence* for consciousness, the ground
for imputing it, and by imputing it we imply that the system can or will
have subjective states.

<QUOTE PREVIOUSPOST="
&gt; You say in your recent reply to &quot;someone2&quot; in this thread that:

&gt;&gt; We can confirm that a system is in a subjective state if it can:

&gt;&gt; 1) Discriminate between two or more stimuli (tell them apart), but

&gt;&gt; 2) Cannot describe the differences between the stimuli.
&gt; I presume that you would deny subjective states to the amoeba
&gt; because it fails to satisfy condition 2, but then no other species
&gt; but ours could possibly satisfy condition 2 since it demands that
&gt; the system be a language user. In this case I wonder how you
&gt; could impute subjective states to chimpanzees.
">

We would impute consciousness to any systems other than humans
analogically, and would do so if imputing that capacity (and the subjective
states it entails) would help us explain the behavior of those systems. But
we validate that explanatory approach with humans, the systems we know
best. With humans we can determine empirically whether or not they are in a
certain subjective state.

You're mistaken re: condition 2, though. Any non-speaking system *does*
satisfy condition 2, automatically. :-)
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-01-31T03:26:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt; I don't believe that the two are as distinct as you seem to believe
&gt;&gt; they are -- to identify others as conscious is to believe them to
&gt;&gt; suffer pain and enjoy pleasure just as I do, and to empathize is to
&gt;&gt; imagine their suffering and their pleasure as my own. Consequently
&gt;&gt; I'm not yet convinced that your reply answers my question.

&gt; I think you are saying the same thing I said earlier --- that to impute
&gt; consciousness to a system is to impute a capacity for having various
&gt; subjective states. You say &quot;believe them to suffer pain and enjoy
&gt; pleasure.&quot; Suffering pain and enjoying pleasure are indeed among the
&gt; subjective states we mean the system might have when we impute
&gt; consciousness to it. Being able to experience pain and pleasure are
&gt; implied when we impute consciousness, but they are not the *basis*
&gt; upon which we impute that capacity. We impute it based on the
&gt; organism's behavior and other observable properties. Behavior and
&gt; observable properties are the basis, the truth conditions, for imputing
&gt; consciousness; being able to experience pain and pleasure, and other
&gt; subjective states, are among the implications of that attribution.

&gt; You seem to be saying that P's empathy is evidence for Q's consciousness.
&gt; But that can't possibly be, unless some further assumptions are made.
">

No, I'm saying that P's empathy is evidence of P's *conviction* of Q's
consciousness, and taking empathy as instinctive, it must follow that P's
conviction of Q's consciousness is also instinctive and not an a_posteriori
construction.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;&gt;&gt; We should be hesitant if imputing it to the amoeba gains us no
&gt;&gt;&gt; explanatory advantage. To do so would violate Occam's principle:
&gt;&gt;&gt; don't multiply entities needlessly.

&gt;&gt; My first impression was that you'd performed what Chalmers calls a
&gt;&gt; &quot;bait-and-switch&quot;, but then it occurred to me that you may be arguing
&gt;&gt; that Chalmers is wrong on precisely that ground -- i.e. that he thinks
&gt;&gt; there's something more to consciousness than behavioral responses.
&gt;&gt; Eliminativism is certainly one way of eradicating his &quot;explanatory gap&quot;,
&gt;&gt; but one that seems to me to deny the blatantly obvious.

&gt; No --- there is indeed something more to consciousness than behavioral
&gt; responses. Chalmers is not wrong there. The &quot;something more&quot; is
&gt; subjective states. To say that a system is conscious is to say that it can
&gt; have subjective states. Behavior is the *evidence* for consciousness,
&gt; the ground for imputing it, and by imputing it we imply that the system
&gt; can or will have subjective states.
">

Given your definition of consciousness as the capacity to have subjective
states, I don't see how the capacity to have subjective states can be
"something more" than consciousness. The only way I can see for this to
start to make sense would be if you're saying that we rely on behavior and
observable properties in order to impute subjective states to an organism,
but that we cannot rely on the absence of such behavior and observable
properties to deny subjective states to an organism. And if this is the
case, then I agree. I think it is only at this juncture that you can invoke
Occam's Razor as a second and distinct stage of the argument intended
to make that denial.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;&gt; You say in your recent reply to &quot;someone2&quot; in this thread that:

&gt;&gt;&gt; We can confirm that a system is in a subjective state if it can:

&gt;&gt;&gt; 1) Discriminate between two or more stimuli (tell them apart), but

&gt;&gt;&gt; 2) Cannot describe the differences between the stimuli.

&gt;&gt; I presume that you would deny subjective states to the amoeba
&gt;&gt; because it fails to satisfy condition 2, but then no other species
&gt;&gt; but ours could possibly satisfy condition 2 since it demands that
&gt;&gt; the system be a language user. In this case I wonder how you
&gt;&gt; could impute subjective states to chimpanzees.

&gt; We would impute consciousness to any systems other than humans
&gt; analogically, and would do so if imputing that capacity (and the
&gt; subjective states it entails) would help us explain the behavior of those
&gt; systems. But we validate that explanatory approach with humans, the
&gt; systems we know best. With humans we can determine empirically
&gt; whether or not they are in a certain subjective state.
">

The whole point of the zombie argument is that behavior is *no* indication
of the capacity to have subjective states. We impute subjective states to
other people *instinctively* and not as a consequence of some a_posteriori
construction. We do so, as you say, analogically, but not on the grounds
that it helps us explain the behavior of other people -- the zombie argument
shows us that it does not. Rather we impute subjective states to other
people on the implicit conviction that "I am conscious, therefore you are
conscious too".

<QUOTE PREVIOUSPOST="
&gt; You're mistaken re: condition 2, though. Any non-speaking system
&gt; *does* satisfy condition 2, automatically. :-)
">

And consequently any non-speaking system cannot
be denied the capacity to have subjective states?
</POST>
<POST>
<POSTER> "chazwin" &lt;chazwy...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-01-31T04:01:00 </POSTDATE>
On Jan 24, 12:42 pm, "andy-k" &lt;spam.free@last&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;If we rigorously and systematically discriminate subject and object often
&gt; enough, long enough, and on sufficiently various occasions, it is likely to
&gt; dawn on us sooner or later that the subject is not an object. The subject is
&gt; not any particular object or any combination of objects. The chief barrier
&gt; to drawing this conclusion is conceptual. That is, our models of reality may
&gt; impose constraints on empirical investigation. We get attached to our
&gt; models, particularly if we have worked hard on them, and we may want to
&gt; avoid looking in a direction which threatens them.

&gt; &quot;Issues at the core of current consciousness debates are variations of the
&gt; problem of subjectivity, such as the 'hard problem', the problem of other
&gt; minds, and the zombie problem. Progress on these issues would seem
&gt; to presuppose some understanding of the subject/object distinction. The
&gt; proposition that the subject is not an object has radical implications for
&gt; current tendencies in the philosophy of mind or consciousness studies.
&gt; It means that 'consciousness' is not to be identified with, or explained in
&gt; terms of, any entities or processes to be found as objects of attention in
&gt; the world. It means that any attempt to understand 'consciousness' as a
&gt; property of some kind-whether a property of systems or a property of
&gt; states-is fundamentally misguided.&quot;

&gt; http://www.imprint.co.uk/online/edey.html
">

It should be clear that since Kant the subject/object distinction is
false. All facts, truths, things, and statements about the same, as
dependant on our perception of them. We can attempt objectivity, but
in reality we only achieve collective subjectivity.
This assumption of a possible objective universe contains within the
possibility of properties: a dualistic project which has been useful
but seems to contain the seeds of its own destruction as these
properties cannot exist in vitro but are related to and depend on a
whole host of other intersecting properties. The problem for the
objective project is where do you draw the lines?
</POST>
<POST>
<POSTER> "Publius" &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-01-31T06:12:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;andy-k&quot; &lt;spam.free@last&gt; wrote in message
">

news:ZoYvh.83055$UC.25279@newsfe5-win.ntli.net ...

<QUOTE PREVIOUSPOST="
&gt;&gt; You seem to be saying that P's empathy is evidence for Q's
&gt;&gt; consciousness. But that can't possibly be, unless some further
&gt;&gt; assumptions are made.
&gt; No, I'm saying that P's empathy is evidence of P's *conviction* of Q's
&gt; consciousness, and taking empathy as instinctive, it must follow that
&gt; P's conviction of Q's consciousness is also instinctive and not an
&gt; a_posteriori construction.
">

If you formalize that you'll see that it is a non-sequitur.

Let pEq mean "p feels empathy for q."

Let pBc mean "p believes that q is conscious."

You are arguing that

1. pEq --&gt; pBc

2. pEq is instinctive, therefore
------------
3. pBc is instinctive.

That is a non-sequitur (undistributed middle). Compare,

1a. "p enjoys eating quiche" --&gt; "p believes that quiche is edible."

2a. "p's enjoyment of eating quiche is instinctive." Therefore,
------------
3a. "p's belief that quiche is edible is instinctive."

If p had not had independent grounds for believing quiche was edible he
would never have tried it, and his (arguably) instinctive liking for it
would never have manifested itself.

<QUOTE PREVIOUSPOST="
&gt;&gt; No --- there is indeed something more to consciousness than behavioral
&gt;&gt; responses. Chalmers is not wrong there. The &quot;something more&quot; is
&gt;&gt; subjective states. To say that a system is conscious is to say that it
&gt;&gt; can have subjective states. Behavior is the *evidence* for
&gt;&gt; consciousness, the ground for imputing it, and by imputing it we imply
&gt;&gt; that the system can or will have subjective states.

&gt; Given your definition of consciousness as the capacity to have
&gt; subjective states, I don't see how the capacity to have subjective
&gt; states can be &quot;something more&quot; than consciousness.
">

That wasn't what I said. I said consciousness is something more than
behavior. The "something more" (than behavior) is subjective states.

<QUOTE PREVIOUSPOST="
&gt; The only way I can
&gt; see for this to start to make sense would be if you're saying that we
&gt; rely on behavior and observable properties in order to impute subjective
&gt; states to an organism, but that we cannot rely on the absence of such
&gt; behavior and observable properties to deny subjective states to an
&gt; organism.
">

Not quite. To warrant imputing consciousness (the capacity for subjective
states), the organism must exhibit specific sorts of behavior. If the
organism does not exhibit behavior of those types, then there is no reason
to impute subjective states to it. If it does exhibit those behaviors, then
imputing subjective states may help explain those behaviors, even though we
cannot verify those states empirically, as we can with humans.

<QUOTE PREVIOUSPOST="
&gt;&gt; We would impute consciousness to any systems other than humans
&gt;&gt; analogically, and would do so if imputing that capacity (and the
&gt;&gt; subjective states it entails) would help us explain the behavior of
&gt;&gt; those systems. But we validate that explanatory approach with humans,
&gt;&gt; the systems we know best. With humans we can determine empirically
&gt;&gt; whether or not they are in a certain subjective state.
&gt; The whole point of the zombie argument is that behavior is *no*
&gt; indication of the capacity to have subjective states.
">

And that premise is false. We certainly do have an empirical means of
deciding whether someone is in a subjective state. P is in a subjective
state IFF

1) P can discriminate between A and B, and

2) P cannot describe the difference between A and B.

P's inability to describe that difference is *precisely* why we call that
state "subjective." If 1) and 2) are true of the alleged zombie, then we
have exactly the same reason for imputing subjective states to it as we have
for imputing them to humans. The "zombie argument" is misguided.

A subjective proposition is one for which no public truth conditions exist.
No public truth conditions exist for the proposition which describes P's
state *because such a proposition cannot be formulated*. P's state is
ineffable; therefore, it is inherently subjective.

<QUOTE PREVIOUSPOST="
&gt; We impute
&gt; subjective states to other people *instinctively* and not as a
&gt; consequence of some a_posteriori construction.
">

We indeed impute them *automatically*, once we have learned the truth
conditions for applying that term. But we do not know the meanings of any
terms "instinctively." Before we apply that term to people, we have first to
recognize them as people. And that recognition rests on their appearance and
behavior.

<QUOTE PREVIOUSPOST="
&gt; Rather we impute subjective states to other people on the implicit
&gt; conviction that &quot;I am conscious, therefore you are conscious too&quot;.
">

Actually, the sequence is usually the reverse: we learn how to apply the
term to others before we realize it applies to ourselves. Rather like a
child learning it is a boy, or an Italian.

<QUOTE PREVIOUSPOST="
&gt; And consequently any non-speaking system cannot
&gt; be denied the capacity to have subjective states?
">

Sure, if it doesn't exhibit the requisite behaviors.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-02-01T01:19:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt; &quot;Publius&quot; wrote:
&gt;&gt;&gt; You seem to be saying that P's empathy is evidence
&gt;&gt;&gt; for  Q's consciousness. But that can't possibly be,
&gt;&gt;&gt; unless some further assumptions are made.

&gt;&gt; No, I'm saying that P's empathy is evidence of P's *conviction*
&gt;&gt; of Q's consciousness, and taking empathy as instinctive,
&gt;&gt; it must follow that P's conviction of Q's consciousness
&gt;&gt; is also instinctive and not an a_posteriori construction.

&gt; If you formalize that you'll see that it is a non-sequitur.

&gt; Let pEq mean &quot;p feels empathy for q.&quot;

&gt; Let pBc mean &quot;p believes that q is conscious.&quot;

&gt; You are arguing that

&gt; 1. pEq --&gt; pBc

&gt; 2. pEq is instinctive, therefore
&gt; ------------
&gt; 3. pBc is instinctive.

&gt; That is a non-sequitur (undistributed middle). Compare,

&gt; 1a. &quot;p enjoys eating quiche&quot; --&gt; &quot;p believes that quiche is edible.&quot;

&gt; 2a. &quot;p's enjoyment of eating quiche is instinctive.&quot; Therefore,
&gt; ------------
&gt; 3a. &quot;p's belief that quiche is edible is instinctive.&quot;

&gt; If p had not had independent grounds for believing quiche
&gt; was edible he would never have tried it, and his (arguably)
&gt; instinctive liking for it would never have manifested itself.
">

Quiche aside, P's empathy is not *evidence* of P's conviction
of Q's consciousness (I unthinkingly adopted your phraseology),
but rather it is just another way of saying the same thing. Even
the attribution of empathy to P is itself an attribution of subjective
states to P, since for P to empathize with Q is for P to *feel* how
Q must be feeling.

I'm still studying the rest of your post,
and I think I'm starting to understand your view.
I'll get back to you when I have more confidence.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-02-01T03:22:00 </POSTDATE>
"someone2" &lt;glenn.spig ... @btinternet.com&gt; wrote in
news:1170169130.793385.56320@j27g2000cwj.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt; Ah, so reductionists expect any behaviours which weren't predictable
&gt; from its build and programming, to ultimately be explainable in terms
&gt; of the build and programming, you are presumably just pointing out for
&gt; clarity that it is possible that the robot might contain a bug, or an
&gt; unplanned feature.
">

No, it is not a bug. Complex systems generate unpredictable behaviors by
definition. You might google for "complexity theory" or "complex system".

Animal nervous systems are complex systems. So are evolution, weather,
economies, and numerous other natural systems. They all exhibit behaviors
which cannot be predicted from the laws of the underlying substrate.

<QUOTE PREVIOUSPOST="
&gt; They would still expect it to act the same regardless of  whether it
&gt; was or wasn't conscious.
">

Eliminativsts would say we can explain behaviors without resort to
consciousness or subjective states. It is not a matter of "is it, or isn't
it?" It is a matter of, "Should we or should we not impute those states?"
We should if there is some explanatory advantage in doing so. If not, then
we shouldn't.

<QUOTE PREVIOUSPOST="
&gt; Which is slightly confusing because I thought
&gt; you had said that reductionists viewed consciousness to be
&gt; influential, or was that in terms of imbued consciousness, where they
&gt; imbue something (say the robots circuits) as being the same as
&gt; consciousness (they say that is the imbued consciousness), and
&gt; therefore state that the advantage of the imbued consciousness is
&gt; obvious, as the robot wouldn't work the same without it.
">

If we built a sufficently complex robot we may not be able to predict much
of its behavior via the laws of physics. At that point we may decide we can
predict more of its behavior by assuming that it has subjective states, and
is therefore conscious. If that proves to be the case, then those
postulated states will indeed have a causal role in the robot's behavior
--- because a *cause* is that which allows us to predict a particular
*effect*, in this case, certain behavior by the robot.

<QUOTE PREVIOUSPOST="
&gt; To resolve the conflict, it would seem like they should have been
&gt; assuming that if consciousness and the neural activity were the same
&gt; thing, then that thing has two sides to it, the conscious, and a
&gt; physical side, and it would be the physical side that is an advantage,
&gt; the conscious side would offer no advantage, nor offer any influence,
&gt; in other words the behaviour is not influenced by whether it is aware
&gt; of itself. Do you agree?
">

That is one version of the materialist argument. To make their case, they
have to show that their "nonconscious" model of the system permits better
predictions than the "consciousness" model.
</POST>
<POST>
<POSTER> "chazwin" &lt;chazwy...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-02-01T05:32:00 </POSTDATE>
On Feb 1, 8:22 am, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1170169130.793385.56320@j27g2000cwj.googlegroups.com:

&gt; &gt; Ah, so reductionists expect any behaviours which weren't predictable
&gt; &gt; from its build and programming, to ultimately be explainable in terms
&gt; &gt; of the build and programming, you are presumably just pointing out for
&gt; &gt; clarity that it is possible that the robot might contain a bug, or an
&gt; &gt; unplanned feature.

&gt; No, it is not a bug. Complex systems generate unpredictable behaviors by
&gt; definition. You might google for &quot;complexity theory&quot; or &quot;complex system&quot;.

&gt; Animal nervous systems are complex systems. So are evolution, weather,
&gt; economies, and numerous other natural systems. They all exhibit behaviors
&gt; which cannot be predicted from the laws of the underlying substrate.
">

Is this due to a failure of the modeling of the law; or lack of
prediction? Or do you reject a deterministic universe?

<QUOTE PREVIOUSPOST="
&gt; &gt; They would still expect it to act the same regardless of  whether it
&gt; &gt; was or wasn't conscious.

&gt; Eliminativsts would say we can explain behaviors without resort to
&gt; consciousness or subjective states. It is not a matter of &quot;is it, or isn't
&gt; it?&quot; It is a matter of, &quot;Should we or should we not impute those states?&quot;
&gt; We should if there is some explanatory advantage in doing so. If not, then
&gt; we shouldn't.

&gt; &gt; Which is slightly confusing because I thought
&gt; &gt; you had said that reductionists viewed consciousness to be
&gt; &gt; influential, or was that in terms of imbued consciousness, where they
&gt; &gt; imbue something (say the robots circuits) as being the same as
&gt; &gt; consciousness (they say that is the imbued consciousness), and
&gt; &gt; therefore state that the advantage of the imbued consciousness is
&gt; &gt; obvious, as the robot wouldn't work the same without it.

&gt; If we built a sufficently complex robot we may not be able to predict much
&gt; of its behavior via the laws of physics. At that point we may decide we can
&gt; predict more of its behavior by assuming that it has subjective states, and
&gt; is therefore conscious. If that proves to be the case, then those
&gt; postulated states will indeed have a causal role in the robot's behavior
&gt; --- because a *cause* is that which allows us to predict a particular
&gt; *effect*, in this case, certain behavior by the robot.

&gt; &gt; To resolve the conflict, it would seem like they should have been
&gt; &gt; assuming that if consciousness and the neural activity were the same
&gt; &gt; thing, then that thing has two sides to it, the conscious, and a
&gt; &gt; physical side, and it would be the physical side that is an advantage,
&gt; &gt; the conscious side would offer no advantage, nor offer any influence,
&gt; &gt; in other words the behaviour is not influenced by whether it is aware
&gt; &gt; of itself. Do you agree?

&gt; That is one version of the materialist argument. To make their case, they
&gt; have to show that their &quot;nonconscious&quot; model of the system permits better
&gt; predictions than the &quot;consciousness&quot; model.
">
</POST>
<POST>
<POSTER> "someone2" &lt;glenn.spig...@btinternet.com&gt; </POSTER>
<POSTDATE> 2007-02-01T05:45:00 </POSTDATE>
On 1 Feb, 08:22, Publius &lt;m.publ ... @nospam.comcast.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &quot;someone2&quot; &lt;glenn.spig ... @btinternet.com&gt; wrote in news:1170169130.793385.56320@j27g2000cwj.googlegroups.com:

&gt; &gt; Ah, so reductionists expect any behaviours which weren't predictable
&gt; &gt; from its build and programming, to ultimately be explainable in terms
&gt; &gt; of the build and programming, you are presumably just pointing out for
&gt; &gt; clarity that it is possible that the robot might contain a bug, or an
&gt; &gt; unplanned feature.

&gt; No, it is not a bug. Complex systems generate unpredictable behaviors by
&gt; definition. You might google for &quot;complexity theory&quot; or &quot;complex system&quot;.

&gt; Animal nervous systems are complex systems. So are evolution, weather,
&gt; economies, and numerous other natural systems. They all exhibit behaviors
&gt; which cannot be predicted from the laws of the underlying substrate.

&gt; &gt; They would still expect it to act the same regardless of  whether it
&gt; &gt; was or wasn't conscious.

&gt; Eliminativsts would say we can explain behaviors without resort to
&gt; consciousness or subjective states. It is not a matter of &quot;is it, or isn't
&gt; it?&quot; It is a matter of, &quot;Should we or should we not impute those states?&quot;
&gt; We should if there is some explanatory advantage in doing so. If not, then
&gt; we shouldn't.

&gt; &gt; Which is slightly confusing because I thought
&gt; &gt; you had said that reductionists viewed consciousness to be
&gt; &gt; influential, or was that in terms of imbued consciousness, where they
&gt; &gt; imbue something (say the robots circuits) as being the same as
&gt; &gt; consciousness (they say that is the imbued consciousness), and
&gt; &gt; therefore state that the advantage of the imbued consciousness is
&gt; &gt; obvious, as the robot wouldn't work the same without it.

&gt; If we built a sufficently complex robot we may not be able to predict much
&gt; of its behavior via the laws of physics. At that point we may decide we can
&gt; predict more of its behavior by assuming that it has subjective states, and
&gt; is therefore conscious. If that proves to be the case, then those
&gt; postulated states will indeed have a causal role in the robot's behavior
&gt; --- because a *cause* is that which allows us to predict a particular
&gt; *effect*, in this case, certain behavior by the robot.

&gt; &gt; To resolve the conflict, it would seem like they should have been
&gt; &gt; assuming that if consciousness and the neural activity were the same
&gt; &gt; thing, then that thing has two sides to it, the conscious, and a
&gt; &gt; physical side, and it would be the physical side that is an advantage,
&gt; &gt; the conscious side would offer no advantage, nor offer any influence,
&gt; &gt; in other words the behaviour is not influenced by whether it is aware
&gt; &gt; of itself. Do you agree?

&gt; That is one version of the materialist argument. To make their case, they
&gt; have to show that their &quot;nonconscious&quot; model of the system permits better
&gt; predictions than the &quot;consciousness&quot; model.
">

In reply to where I said:
---------------
...you are presumably just pointing out for clarity that it is
possible that the robot might contain a bug, or an unplanned feature.
---------------

You replied:
---------------
No, it is not a bug. Complex systems generate unpredictable behaviors
by
definition. You might google for "complexity theory" or "complex
system".
---------------

Were you just pointing out the possibility of an unplanned feature
then? Yes programs can be complexed, but ultimately any unforseen
behaviour will either fall into the category of bug or unplanned
feature. Either you wish to remove the behaviour, or you are ok with
it.

Regarding eliminativsts, are they attempting to through rhetoric
suggest that we aren't conscious? Given that we are, what are they
hoping to achieve, is it an experiment in manipulation? That they hope
to produce such a polished deception, that people will not realise
that they are conscious beings, or simply not be able to discuss
matters regarding the truth with the assumption that they are, and
maybe just make them doubt their knowledge that they are? What would
be the point other than planned manipulation, or have I misunderstood
the motivation behind it?

You said:
----------------
If we built a sufficently complex robot we may not be able to predict
much of its behavior via the laws of physics. At that point we may
decide we can predict more of its behavior by assuming that it has
subjective states, and is therefore conscious. If that proves to be
the case, then those postulated states will indeed have a causal role
in the robot's behavior --- because a *cause* is that which allows us
to predict a particular *effect*, in this case, certain behavior by
the robot.
----------------

The cause is presumably the cause, not any story of the cause that
happened to work, otherwise from your perspective the cause would
change over time as new explanations came along. Surely it wouldn't
make sense to say that the cause used to be this, but when the new
explanation came along it became that.

If by subjective states, you mean there might be certain patterns of
state, each state differentiating itself in predictive behaviour. Then
why don't you just call them patterns, and talk at first about the
vague properties of the pattern in regards to behaviour, and then just
get more detailed into the pattern itself, until you understand how
the robot you built works. Why label the patterns consciousness, when
there was no reason to think the physical activity involved in the
patterns was causing any conscious experiences, it was the same type
of physical activity found on a mobile phone maybe? If the word
consciousness were redefined so as to just be a label to those that
choose to, attach it to mechanisms as they wish, and people didn't
realise that the word had been redefined, it would give the entirely
wrong impression of materialism. Especially since the materialist
perspective states that anything conscious and experiencing free will,
will act as though it isn't conscious, and therefore consciousness
couldn't be influential. The redefined consciousness doesn't refer to
the experience of being the humans claim, though perhaps it could be
attached to them by virtue of their claiming an influence of a being
upon their behaviour, even though for materialism to be true, there
could be no being influencing the behaviour of the humans.
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-02-01T14:56:00 </POSTDATE>
"chazwin" &lt;chazwy ... @yahoo.com&gt; wrote in
news:1170325963.532514.221620@l53g2000cwa.googlegroups.com:

<QUOTE PREVIOUSPOST="
&gt;&gt; Animal nervous systems are complex systems. So are evolution, weather,
&gt;&gt; economies, and numerous other natural systems. They all exhibit
&gt;&gt; behaviors which cannot be predicted from the laws of the underlying
&gt;&gt; substrate.
&gt; Is this due to a failure of the modeling of the law; or lack of
&gt; prediction? Or do you reject a deterministic universe?
">

It is due to the complexity of the system. The system has too many variables,
each with too large a range of possible values, to be calculable by any
computer which will "fit" within the universe, within the causal interval
(the time between T0 (the cause) and T1 (the effect)). It is a generalization
of the many-body problem. The system is somewhat chaotic.

http://en.wikipedia.org/wiki/Chaos_theory

The system may be "deterministic" in the sense that all events have causes
(which could in principle be discovered after the fact), but it is not
predictable, and thus not "deterministic" in LaPlace's sense.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-02-02T15:59:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt; Given your definition of consciousness as the capacity to have
&gt;&gt; subjective states, I don't see how the capacity to have subjective
&gt;&gt; states can be &quot;something more&quot; than consciousness.

&gt; That wasn't what I said. I said consciousness is something more than
&gt; behavior. The &quot;something more&quot; (than behavior) is subjective states.
">

If I understand you correctly (at last ;-), you're arguing from behavior
(P can discriminate between A and B, but cannot describe the
difference between A and B) to subjective states, and presumably
claiming that it's nonsense to speak of subjective states without the
evidence of that behavior.

<QUOTE PREVIOUSPOST="
&gt;&gt; The only way I can see for this to start to make sense would be if you're
&gt;&gt; saying that we rely on behavior and observable properties in order to
&gt;&gt; impute subjective states to an organism, but that we cannot rely on the
&gt;&gt; absence of such behavior and observable properties to deny subjective
&gt;&gt; states to an organism.

&gt; Not quite. To warrant imputing consciousness (the capacity for subjective
&gt; states), the organism must exhibit specific sorts of behavior. If the
&gt; organism does not exhibit behavior of those types, then there is no reason
&gt; to impute subjective states to it. If it does exhibit those behaviors,
&gt; then imputing subjective states may help explain those behaviors, even
&gt; though we cannot verify those states empirically, as we can with humans.
">

If a non-human organism fits the bill for the imputation of subjective
states -- i.e. it can discriminate between A and B, but cannot describe the
difference between A and B (since, being non-human, it has no language) --
then how could the imputation of subjective states *ever* help explain those
behaviors? It would seem to follow that the only organisms for which the
imputation of subjective states has explanatory power would be humans.

<QUOTE PREVIOUSPOST="
&gt;&gt;&gt; We would impute consciousness to any systems other than humans
&gt;&gt;&gt; analogically, and would do so if imputing that capacity (and the
&gt;&gt;&gt; subjective states it entails) would help us explain the behavior of
&gt;&gt;&gt; those systems. But we validate that explanatory approach with humans,
&gt;&gt;&gt; the systems we know best. With humans we can determine empirically
&gt;&gt;&gt; whether or not they are in a certain subjective state.

&gt;&gt; The whole point of the zombie argument is that behavior is *no*
&gt;&gt; indication of the capacity to have subjective states.

&gt; And that premise is false. We certainly do have an empirical
&gt; means of deciding whether someone is in a subjective state.
&gt; P is in a subjective state IFF

&gt; 1) P can discriminate between A and B, and

&gt; 2) P cannot describe the difference between A and B.

&gt; P's inability to describe that difference is *precisely* why we
&gt; call that state &quot;subjective.&quot;
">

I recognize that as *one* of the uses of the word 'subjective'.

<QUOTE PREVIOUSPOST="
&gt; If 1) and 2) are true of the alleged zombie, then we have exactly
&gt; the same reason for imputing subjective states to it as we have
&gt; for imputing them to humans. The &quot;zombie argument&quot; is misguided.
">

That would be consistent with your argument from behavior to
subjective states.

<QUOTE PREVIOUSPOST="
&gt; A subjective proposition is one for which no public truth conditions
&gt; exist. No public truth conditions exist for the proposition which
&gt; describes P's state *because such a proposition cannot be formulated*.
&gt; P's state is ineffable; therefore, it is inherently subjective.

&gt;&gt; We impute subjective states to other people *instinctively* and
&gt;&gt; not as a consequence of some a_posteriori construction.

&gt; We indeed impute them *automatically*, once we have learned
&gt; the truth conditions for applying that term. But we do not know
&gt; the meanings of any terms &quot;instinctively.&quot; Before we apply that
&gt; term to people, we have first to recognize them as people.
&gt; And that recognition rests on their appearance and behavior.
">

I agree that we don't instinctively know the meanings of any terms,
but I'm arguing that this has nothing to do with language (and
so nothing to do with the truth conditions for applying terms).
People recognize other people as people, just as dogs recognize
other dogs as dogs, without any need to appeal to language.
And empathy, being the imputation of subjective states to other
people, is instinctive in our species.

<QUOTE PREVIOUSPOST="
&gt;&gt; Rather we impute subjective states to other people on the implicit
&gt;&gt; conviction that &quot;I am conscious, therefore you are conscious too&quot;.

&gt; Actually, the sequence is usually the reverse: we learn how to
&gt; apply the term to others before we realize it applies to ourselves.
&gt; Rather like a child learning it is a boy, or an Italian.
">

I agree with you as far as the *term* is concerned, but I'm arguing
that the imputation existed before we gave it a name.

<QUOTE PREVIOUSPOST="
&gt;&gt; And consequently any non-speaking system cannot
&gt;&gt; be denied the capacity to have subjective states?

&gt; Sure, if it doesn't exhibit the requisite behaviors.
">

-- i.e. if non-speaking system can discriminate between A and B,
but cannot describe the difference between A and B (because it's
a non-speaking system), then you would still deny it the capacity
to have subjective states?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-02-03T02:20:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:hDNwh.61960$1W1.41129@newsfe4-win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt; If I understand you correctly (at last ;-), you're arguing from behavior
&gt; (P can discriminate between A and B, but cannot describe the
&gt; difference between A and B) to subjective states, and presumably
&gt; claiming that it's nonsense to speak of subjective states without the
&gt; evidence of that behavior.
">

Close. We argue from behavior to consciousness, which we *define* to be a
capacity for having subjective states. That is, we look at an organism,
especially its behavior in various contexts, and decide those behaviors
might well be explained by postulating various subjective states to it. So
we say "it is conscious" --- meaning, "it has a capacity for subjective
states; imputing subjective states to it may explain the behaviors we
observe."

That is a definition of consciousness, but it is not an arbitrary one. I
believe that is exactly what we ordinarily mean, and mean to do, when we
say that an organism is conscious --- that is the sort of thing to which we
will attribute subjective states, such as moods, desires, beliefs,
"qualia," preferences, goals, and so on.

Now a subjective state (in others) is not directly observable (else it
would not be a subjective state, but an objective one). But we can
nonetheless have objective evidence for subjective states in humans. And
that is the 2-part test: the subject can discriminate between two (or more)
phenomena, but cannot describe the differences among them. That holds for
*all* subjective states, not just "qualia." A subject cannot describe the
difference between red and green in any terms which would allow someone who
did not already know what those terms meant to pick out their referents.
(Think of the Mary story). But the subject also cannot explain why he likes
chocolate, what "sadness" feels like, etc. Yet he can make discriminations
among those states, which we can observe.

Postulating those states and others can help us explain observable
behavior, and predict it. If we postulate that a person has a taste for
chocolate (a subjective state), we can predict that he will often choose
chocolate over other flavors of ice cream. If we postulate that hearing
certain sounds induces a particular subjective state in him, then we can
predict that he will attend concerts, or buy CDs. If we postulate a goal or
an interest (more subjective states) to that person, then we can predict
much of his behavior.

I would not say that it is "nonsense" to impute subjective states in the
absence of behavior, but that it would be pointless, like imputing charge
to particles in a world without electricity or magnetism. It is to explain
behavior that we impute those states.

<QUOTE PREVIOUSPOST="
&gt; If a non-human organism fits the bill for the imputation of subjective
&gt; states -- i.e. it can discriminate between A and B, but cannot describe
&gt; the difference between A and B (since, being non-human, it has no
&gt; language) -- then how could the imputation of subjective states *ever*
&gt; help explain those behaviors? It would seem to follow that the only
&gt; organisms for which the imputation of subjective states has explanatory
&gt; power would be humans.
">

Simply because it may allow us to better predict the behavior of those
organisms. Animal behavior can also be predicted fairly well by imputing
goals, preferences, moods, etc., to them. So we impute them, with the
assumption that even if they could speak, they would not be able to
describe their subjective states any better than humans can. Note that we
can impute a *state* to a system based on any differential in behavior. We
just call those states "subjective" in humans because they cannot describe
them, even though they are in them. It is a means to pick out those states.
Then we assume that (some) animals have analogous states.

<QUOTE PREVIOUSPOST="
&gt;&gt; And that premise is false. We certainly do have an empirical
&gt;&gt; means of deciding whether someone is in a subjective state.
&gt;&gt; P is in a subjective state IFF

&gt;&gt; 1) P can discriminate between A and B, and

&gt;&gt; 2) P cannot describe the difference between A and B.

&gt;&gt; P's inability to describe that difference is *precisely* why we
&gt;&gt; call that state &quot;subjective.&quot;

&gt; I recognize that as *one* of the uses of the word 'subjective'.
">

It is the one relevant to the matters at hand, I think.

<QUOTE PREVIOUSPOST="
&gt; I agree that we don't instinctively know the meanings of any terms,
&gt; but I'm arguing that this has nothing to do with language (and
&gt; so nothing to do with the truth conditions for applying terms).
&gt; People recognize other people as people, just as dogs recognize
&gt; other dogs as dogs, without any need to appeal to language.
&gt; And empathy, being the imputation of subjective states to other
&gt; people, is instinctive in our species.
">

Yes, they do recognize one another. But that recognition is based on
empirical cues, i.e., appearance and behavior for humans, those plus scent
for dogs. Once they are recognized we attach various properties to them,
per the truth conditions the language stipulates for applying those
properties. But though we may have empathy for them without language, we
could not impute subjective states to them without language. Or to
ourselves, for that matter. We would not have that concept. Empathy is a
*sense* of kinship with another. It is inarticulable as any other
subjective state. But it is evoked via empirical cues.

<QUOTE PREVIOUSPOST="
&gt; I agree with you as far as the *term* is concerned, but I'm arguing
&gt; that the imputation existed before we gave it a name.
">

I don't think we can impute without a name for that which is imputed.
</POST>
<POST>
<POSTER> "andy-k" &lt;spam.free@last&gt; </POSTER>
<POSTDATE> 2007-02-04T07:45:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Publius&quot; wrote:
&gt; &quot;andy-k&quot; wrote:
&gt;&gt; If I understand you correctly (at last ;-), you're arguing from
&gt;&gt; behavior (P can discriminate between A and B, but cannot
&gt;&gt; describe the difference between A and B) to subjective states,
&gt;&gt; and presumably claiming that it's nonsense to speak of subjective
&gt;&gt; states without the evidence of that behavior.

&gt; Close. We argue from behavior to consciousness, which we
&gt; *define* to be a capacity for having subjective states. That is,
&gt; we look at an organism, especially its behavior in various contexts,
&gt; and decide those behaviors might well be explained by postulating
&gt; various subjective states to it. So we say &quot;it is conscious&quot; --- meaning,
&gt; &quot;it has a capacity for subjective states; imputing subjective states to it
&gt; may explain the behaviors we observe.&quot;

&gt; That is a definition of consciousness, but it is not an arbitrary one.
&gt; I believe that is exactly what we ordinarily mean, and mean to do,
&gt; when we say that an organism is conscious --- that is the sort of
&gt; thing to which we will attribute subjective states, such as moods,
&gt; desires, beliefs, &quot;qualia,&quot; preferences, goals, and so on.

&gt; Now a subjective state (in others) is not directly observable (else
&gt; it would not be a subjective state, but an objective one). But we can
&gt; nonetheless have objective evidence for subjective states in humans.
&gt; And that is the 2-part test: the subject can discriminate between two
&gt; (or more) phenomena, but cannot describe the differences among
&gt; them. That holds for *all* subjective states, not just &quot;qualia.&quot; A subject
&gt; cannot describe the difference between red and green in any terms
&gt; which would allow someone who did not already know what those
&gt; terms meant to pick out their referents. (Think of the Mary story).
&gt; But the subject also cannot explain why he likes chocolate, what
&gt; &quot;sadness&quot; feels like, etc. Yet he can make discriminations among
&gt; those states, which we can observe.

&gt; Postulating those states and others can help us explain observable
&gt; behavior, and predict it. If we postulate that a person has a taste for
&gt; chocolate (a subjective state), we can predict that he will often choose
&gt; chocolate over other flavors of ice cream. If we postulate that hearing
&gt; certain sounds induces a particular subjective state in him, then we can
&gt; predict that he will attend concerts, or buy CDs. If we postulate a goal
&gt; or an interest (more subjective states) to that person, then we can
&gt; predict much of his behavior.
">

So you're arguing from the observation of behavior to the imputation
of subjective states and then saying that those subjective states enable
us to predict future behavior. Furthermore that this is an advantage over
simply saying that the observation of behavior enables us to predict
future behavior, because the postulation of subjective states provides
an explanation for a specific kind of behavior, namely that of how it is
that a person can discriminate between A and B but cannot describe
the difference between A and B.

This seems to reduce to the following: the only reason for imputing
subjective states to a system/organism is that it can discriminate
between A and B but cannot describe the difference between A and B.
And now it becomes consistent with your comment that consciousness
is "something more" than behavior (language being merely an aspect
of behavior) -- i.e. if (for humans) there were nothing more than behavior
then the primitives used in symbolic representation would always be
sufficient for the task, and so the question of their sufficiency would
never arise.

I think this gets me closer to your view, but close enough?

<QUOTE PREVIOUSPOST="
&gt; I would not say that it is &quot;nonsense&quot; to impute subjective states
&gt; in the absence of behavior, but that it would be pointless, like
&gt; imputing charge to particles in a world without electricity or
&gt; magnetism. It is to explain behavior that we impute those states.
">

I agree that electric charge could not be imputed to particles in a world
without electricity and magnetism (i.e. in a world without electric charge),
but this seems to be a misleading analogy. Subjective states could not be
imputed to organisms in a world devoid of subjective states, but subjective
states aren't merely another name for behavior -- they are "something more"
than behavior, and this is not a world devoid of subjective states.

Perhaps a better analogy would be to say that it would be pointless to
impute charge to particles that showed no acceleration in an electric
or magnetic field, but then we would have to concede that absence
of evidence is not evidence of absence (un-ionized atoms show no
acceleration in an electric or magnetic field).

This revised analogy would still leave your argument intact -- unless and
until the appearance of a consciousness-equivalent of a Thompson and/or
a Rutherford to present evidence of subjective states where none has
yet been found, it would not be pragmatic to impute subjective states in the
absence of evidence ... *according to your definition of consciousness*.
The question remains as to whether your definition is up to the job.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;&gt; If a non-human organism fits the bill for the imputation of subjective
&gt;&gt; states -- i.e. it can discriminate between A and B, but cannot describe
&gt;&gt; the difference between A and B (since, being non-human, it has no
&gt;&gt; language) -- then how could the imputation of subjective states *ever*
&gt;&gt; help explain those behaviors? It would seem to follow that the only
&gt;&gt; organisms for which the imputation of subjective states has explanatory
&gt;&gt; power would be humans.

&gt; Simply because it may allow us to better predict the behavior of those
&gt; organisms. Animal behavior can also be predicted fairly well by imputing
&gt; goals, preferences, moods, etc., to them. So we impute them, with the
&gt; assumption that even if they could speak, they would not be able to
&gt; describe their subjective states any better than humans can. Note that
&gt; we can impute a *state* to a system based on any differential in behavior.
&gt; We just call those states &quot;subjective&quot; in humans because they cannot
&gt; describe them, even though they are in them. It is a means to pick out
&gt; those states. Then we assume that (some) animals have analogous states.
">

But since the only aspect of behavior that the imputation of subjective
states permits us to predict is the inability to describe the difference
between A and B, no such imputation may be made to non-language-users,
and so any such imputation would lack utility. If you're now including the
imputation of goals, preferences, moods, etc. as evidence of consciousness,
then doesn't your definition need revising?

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;&gt; I agree that we don't instinctively know the meanings of any terms,
&gt;&gt; but I'm arguing that this has nothing to do with language (and
&gt;&gt; so nothing to do with the truth conditions for applying terms).
&gt;&gt; People recognize other people as people, just as dogs recognize
&gt;&gt; other dogs as dogs, without any need to appeal to language.
&gt;&gt; And empathy, being the imputation of subjective states to other
&gt;&gt; people, is instinctive in our species.

&gt; Yes, they do recognize one another. But that recognition is based on
&gt; empirical cues, i.e., appearance and behavior for humans, those plus
&gt; scent for dogs. Once they are recognized we attach various properties
&gt; to them, per the truth conditions the language stipulates for applying
&gt; those properties. But though we may have empathy for them without
&gt; language, we could not impute subjective states to them without language.
&gt; Or to ourselves, for that matter. We would not have that concept.
&gt; Empathy is a *sense* of kinship with another. It is inarticulable as any
&gt; other subjective state. But it is evoked via empirical cues.
">

I'm still getting stuck on the distinction you make between empathy
and the imputation of subjective states -- in what way do these differ?
(See below.)

<QUOTE PREVIOUSPOST="
&gt;&gt; I agree with you as far as the *term* is concerned, but I'm arguing
&gt;&gt; that the imputation existed before we gave it a name.

&gt; I don't think we can impute without a name for that which is imputed.
">

Having raised the question of whether subjective states may only be
imputed *to* language-users, my concern now is why subjective states
may only be imputed *by* language-users. Would it be possible for you
to clarify the connection between imputation and language-use?
</POST>
<POST>
<POSTER> Publius &lt;m.publ...@nospam.comcast.net&gt; </POSTER>
<POSTDATE> 2007-02-06T02:57:00 </POSTDATE>
"andy-k" &lt;spam.free@last&gt; wrote in
news:zzkxh.32028$8j7.27298@newsfe1-win.ntli.net:

<QUOTE PREVIOUSPOST="
&gt; So you're arguing from the observation of behavior to the imputation
&gt; of subjective states and then saying that those subjective states enable
&gt; us to predict future behavior. Furthermore that this is an advantage
&gt; over simply saying that the observation of behavior enables us to
&gt; predict future behavior, because the postulation of subjective states
&gt; provides an explanation for a specific kind of behavior, namely that of
&gt; how it is that a person can discriminate between A and B but cannot
&gt; describe the difference between A and B.
">

No. Imputing a "state" (of any kind) to a system is a means of imputing a
cause for an effect. I take it you understand what a "state" is in systems
theory: a system is described by a set of variables, each of which may take
on some value within a specified range, and a set of relationships among
those variables, such that a change in V2 is (or may be) some function of a
change in V1. Any assignment of values to the variables defining the system
at a given time, from among those allowable for each variable, is a state
of the system. We can then say, "The system exhibited behavior B1 because
it is in state S1." Etc. The idea is that whenever the system is in state
S1 it will exhibit behavior B1. So, cause and effect. But that a system
*has* a state is not necessarily to say that it is *in* that state at any
given time. It is just to say that state is within its repertoire.

A state S1 may be invoked by some stimulus. Then the system is *in* that
state, and behavior B1 results. The state S1 is something enduring that the
system carries around with it, so to speak. If we know what states a system
is "carrying around" and what prompts it to enter those various states,
then we can predict its behavior.

<QUOTE PREVIOUSPOST="
&gt; This seems to reduce to the following: the only reason for imputing
&gt; subjective states to a system/organism is that it can discriminate
&gt; between A and B but cannot describe the difference between A and B.
">

No. We impute a state to the system solely because it can discriminate
between A and B. We just call it "subjective" because human subjects cannot
describe those states. Nor is this the least bit surprising. It is *using*
those states to perceive and describe the world. It cannot describe its own
elementary states, any more than a camera can photograph itself, or truth
in a language can be defined within the language. It's states are
"transparent," as Metzinger notes. It is just one example of the general
problem of self-reference.

<QUOTE PREVIOUSPOST="
&gt; And now it becomes consistent with your comment that consciousness
&gt; is &quot;something more&quot; than behavior (language being merely an aspect
&gt; of behavior) -- i.e. if (for humans) there were nothing more than
&gt; behavior then the primitives used in symbolic representation would
&gt; always be sufficient for the task, and so the question of their
&gt; sufficiency would never arise.
">

The subjective states are imputed as causes for observed behavior, as
above. "Consciousness" is just the capacity to have such states.

<QUOTE PREVIOUSPOST="
&gt;&gt; I would not say that it is &quot;nonsense&quot; to impute subjective states
&gt;&gt; in the absence of behavior, but that it would be pointless, like
&gt;&gt; imputing charge to particles in a world without electricity or
&gt;&gt; magnetism. It is to explain behavior that we impute those states.
&gt; I agree that electric charge could not be imputed to particles in a
&gt; world without electricity and magnetism (i.e. in a world without
&gt; electric charge),
">

Well, no. "Electric charge" is an *analysis* of what is observed. All that
is observed is that some metallic objects attract or repel each other, and
lightning bolts. We impute charge to particles to explain those observed
phenomena. If we never had observed any such phenomena, we would not impute
charge to particles. It would have no explanatory purpose.

<QUOTE PREVIOUSPOST="
&gt; Perhaps a better analogy would be to say that it would be pointless to
&gt; impute charge to particles that showed no acceleration in an electric
&gt; or magnetic field, but then we would have to concede that absence
&gt; of evidence is not evidence of absence (un-ionized atoms show no
&gt; acceleration in an electric or magnetic field).
">

When and if we observe phenomena that postulate (electric charge) might
explain, that is the time to postulate it. Until then, postulating it
violates Occam's principle.

<QUOTE PREVIOUSPOST="
&gt; But since the only aspect of behavior that the imputation of subjective
&gt; states permits us to predict is the inability to describe the difference
&gt; between A and B, no such imputation may be made to non-language-users,
&gt; and so any such imputation would lack utility. If you're now including
&gt; the imputation of goals, preferences, moods, etc. as evidence of
&gt; consciousness, then doesn't your definition need revising?
">

Not sure why you'd think so. Goals, preferences, moods, etc., are just as
ineffable as qualia. We can report that we're in them, but not why we are,
or "what they are like."

<QUOTE PREVIOUSPOST="
&gt; Having raised the question of whether subjective states may only be
&gt; imputed *to* language-users, my concern now is why subjective states
&gt; may only be imputed *by* language-users. Would it be possible for you
&gt; to clarify the connection between imputation and language-use?
">

We can impute *states* to a non-speaking organism based on behavior alone,
as enduring causes of that organism's behaviors. We may call them
"subjective states" just because the behavior they explain resembles human
behavior we explain with subjective states. We adopt the precept that
similar effects have similar causes.

"To impute" is a speech act, just as "to declare" or "to inquire." We need
language to do any of those things.
</POST>
</TEXT>
</BODY>
</DOC>
