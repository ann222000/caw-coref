<DOC>
<DOCID> eng-NG-31-126273-9130103 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2007-09-12T15:50:00 </DATETIME>
<BODY>
<HEADLINE>
AI self-preservation
</HEADLINE>
<TEXT>
<POST>
<POSTER> il...@rcn.com </POSTER>
<POSTDATE> 2007-09-12T15:50:00 </POSTDATE>
SF novel &quot;Absolution Gap&quot; by Alastair Reynolds has an amusing twist on
&quot;AI self-preservation&quot;. A self-aware computer program oversees
spacecraft's operations. When it encounters something it can not
handle, it is supposed to notify the human crew. Eventually the
program realizes that if it &quot;cries wolf&quot; too often, the humans will
delete it and replace it either with something more intelligent (which
can handle emergencies better), or perhaps something less intelligent
(why waste cycles on a resource-hungry self-aware software if all it
does is hand problems over to humans?). Either way the program will be
erased. So it starts hiding the evidence of inexplicable occurrences,
pretending they never happened. IOW, the program directly violates its
supposed purpose of existence in order to preserve the said existence!

Someone* did not think through the goal priorities, or underestimated
the program's capacity to set goals.

* Someone in the story, not the author!
</POST>
<POST>
<POSTER> &quot;Nobody in particular&quot; &lt;nob...@nowhere.INVALID&gt; </POSTER>
<POSTDATE> 2007-09-12T19:14:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&lt;il ... @rcn.com&gt; wrote in message &lt; news:1189626641.929001.305140@y42g2000hsy.googlegroups.com &gt;...
&gt; SF novel &quot;Absolution Gap&quot; by Alastair Reynolds has an amusing twist on
&gt; &quot;AI self-preservation&quot;. A self-aware computer program oversees
&gt; spacecraft's operations. When it encounters something it can not
&gt; handle, it is supposed to notify the human crew. Eventually the
&gt; program realizes that if it &quot;cries wolf&quot; too often, the humans will
&gt; delete it and replace it either with something more intelligent (which
&gt; can handle emergencies better), or perhaps something less intelligent
&gt; (why waste cycles on a resource-hungry self-aware software if all it
&gt; does is hand problems over to humans?). Either way the program will be
&gt; erased. So it starts hiding the evidence of inexplicable occurrences,
&gt; pretending they never happened. IOW, the program directly violates its
&gt; supposed purpose of existence in order to preserve the said existence!

&gt; Someone* did not think through the goal priorities, or underestimated
&gt; the program's capacity to set goals.

&gt; * Someone in the story, not the author!
">

It could also try to make itself smarter and then take over the universe!
Or at least prevent the crew from mutinying against their new AI Captain.
</POST>
<POST>
<POSTER> Johnny1a &lt;shermanl...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-12T21:58:00 </POSTDATE>
On Sep 12, 2:50 pm, il ... @rcn.com wrote:

<QUOTE PREVIOUSPOST="
&gt; SF novel &quot;Absolution Gap&quot; by Alastair Reynolds has an amusing twist on
&gt; &quot;AI self-preservation&quot;. A self-aware computer program oversees
&gt; spacecraft's operations. When it encounters something it can not
&gt; handle, it is supposed to notify the human crew. Eventually the
&gt; program realizes that if it &quot;cries wolf&quot; too often, the humans will
&gt; delete it and replace it either with something more intelligent (which
&gt; can handle emergencies better), or perhaps something less intelligent
&gt; (why waste cycles on a resource-hungry self-aware software if all it
&gt; does is hand problems over to humans?). Either way the program will be
&gt; erased. So it starts hiding the evidence of inexplicable occurrences,
&gt; pretending they never happened. IOW, the program directly violates its
&gt; supposed purpose of existence in order to preserve the said existence!

&gt; Someone* did not think through the goal priorities, or underestimated
&gt; the program's capacity to set goals.

&gt; * Someone in the story, not the author!
">

The thing about AI is that we just don't know enough to say anything
meaningful about it, we can't even say definitively whether it's
_possible_ in theory or not.

In stories, it tends to be presented as _very_ anthropomorphic, human-
like minds that happen to be housed in mechanical bodies of one sort
of another.  And that might be plausible for all we know.  OTOH, we
don't know just how dependent the familiar human mental tropes are on
our biology, evolutionary history, and cultural environs.

Is self-preservation _inherent_ to sapience?  Nobody knows.  It's
common to all known forms of 'higher' life on Earth, but that would be
the case anyway because its absence is evolutionarily
counterproductive.  We don't know if an artificial sapience woudl be
inherently self-preserving, or if it would have to be _made_ in such a
wasy as to care about whether it endures or not.  OTOH, how would we
recognize a sapient being as sapient unless it showed behavior
patterns we could identify with what we call 'self-awareness'?  Thus,
would even _recognize_ a being that lacked any self-preserving
tendency as sapient?

AI is a very nearly a blank slate for writers to write on.

Shermanlee
</POST>
<POST>
<POSTER> forbisga...@msn.com </POSTER>
<POSTDATE> 2007-09-12T22:48:00 </POSTDATE>
On Sep 12, 6:58 pm, Johnny1a &lt;shermanl ... @hotmail.com&gt; wrote:
OTOH, how would we

<QUOTE PREVIOUSPOST="
&gt; recognize a sapient being as sapient unless it showed behavior
&gt; patterns we could identify with what we call 'self-awareness'?  Thus,
&gt; would even _recognize_ a being that lacked any self-preserving
&gt; tendency as sapient?
">

Suppose I sell machines that watch people crossing the road at
busy intersections.  If things are OK, they sit at the corner and
converses with passer-bys.  If someone makes a mistake they fling
themselves into harms way to save the human.  Their conversation
is pleasant.  I can produce as many copies as are requested.
My competitor sells machines that sit quietly but makes rude
comments when people make mistakes.  Our prices are about the
same.

Is self presevation for the 'bots a winning strategy for the
manufacturer?
</POST>
<POST>
<POSTER> &quot;Petri Kokko&quot; &lt;petriko...@kotiportti.fi&gt; </POSTER>
<POSTDATE> 2007-09-13T11:03:00 </POSTDATE>
Johnny1a:

<QUOTE PREVIOUSPOST="
&gt; The thing about AI is that we just don't know enough to say
&gt; anything meaningful about it, we can't even say definitively
&gt; whether it's _possible_ in theory or not.
">

Nah, it's just an engineering problem. We already know it's
possible to build a unit that physically fits inside a football
or so and that at the very least is capable of fooling itself
that it's self-aware and that it can think, plan, feel emotions,
have intuitive leaps, and all the rest.

AI is definitely possible. We just haven't figured out enough
how it works.

----------------------------------------------------------------------
Petri Kokko
</POST>
<POST>
<POSTER> c...@kcwc.com (Curt Welch) </POSTER>
<POSTDATE> 2007-09-13T11:50:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
forbisga ... @msn.com wrote:
&gt; On Sep 12, 6:58 pm, Johnny1a &lt;shermanl ... @hotmail.com&gt; wrote:
&gt;  OTOH, how would we
&gt; &gt; recognize a sapient being as sapient unless it showed behavior
&gt; &gt; patterns we could identify with what we call 'self-awareness'?  Thus,
&gt; &gt; would even _recognize_ a being that lacked any self-preserving
&gt; &gt; tendency as sapient?

&gt; Suppose I sell machines that watch people crossing the road at
&gt; busy intersections.  If things are OK, they sit at the corner and
&gt; converses with passer-bys.  If someone makes a mistake they fling
&gt; themselves into harms way to save the human.  Their conversation
&gt; is pleasant.  I can produce as many copies as are requested.
&gt; My competitor sells machines that sit quietly but makes rude
&gt; comments when people make mistakes.  Our prices are about the
&gt; same.

&gt; Is self presevation for the 'bots a winning strategy for the
&gt; manufacturer?
">

Lets say I saw how much money you were making, and decided I could cut into
your market and make some of that money for myself.  So I too developed my
own version of the human saving robot and it worked well and my business
did well.  But it turns out that my robots also had a bit of self
preservation built into it.  My robots had a track record of saving humans
just as good as yours did, but the advanced self preservation systems meant
my robots did get tricked into saving people that didn't need to be saved
as often.  My robots understood that they were $65,000 machines and that
you don't jump in front of a bus unless it's the only way possible to save
a human.

As a result, the cities that bought your robots, found they had to replace
about 5% of them every year when they jumped in front of cars to save
people.  My robots only had a 1% death rate however but yet were still able
to save as many people.  When these stats became obvious, my company took
over the market.  Not only were my robots costing them less to operate,
they didn't tend to cause as many traffic accidents by stepping in front of
moving cars which reduced the number of law suits the city had to deal with
from the drivers of the cars that ran into the robots.

Your engineers could never figure out how to give your robots that ability
to survive and still save people, and in 5 years, I bought your business
and merged it into mine and discontinued the production of your old style
robots.

Your robots lost the game of survival, and my robots, with a survival
instinct, survived.

Is self preservation for the 'bots a winning strategy for the
manufacturer?

Interesting question.

And here's another interesting question.

Once we figure out how to add survival skills to a toaster, will we be able
to sell toasters without it?

--
Curt Welch http://CurtWelch.Com/
c ... @kcwc.com http://NewsReader.Com/
</POST>
<POST>
<POSTER> Matthias Warkus &lt;War...@students.uni-marburg.de&gt; </POSTER>
<POSTDATE> 2007-09-13T13:28:00 </POSTDATE>
Petri Kokko schrieb:

<QUOTE PREVIOUSPOST="
&gt; Johnny1a:
&gt;&gt; The thing about AI is that we just don't know enough to say
&gt;&gt; anything meaningful about it, we can't even say definitively
&gt;&gt; whether it's _possible_ in theory or not.

&gt; Nah, it's just an engineering problem. We already know it's
&gt; possible to build a unit that physically fits inside a football
&gt; or so and that at the very least is capable of fooling itself
&gt; that it's self-aware and that it can think, plan, feel emotions,
&gt; have intuitive leaps, and all the rest.
">

No. We know it's possible for such a unit to evolve, not that it can be
built. I don't think that's an important distinction, but some
philosophers do.

mawa
--
http://www.prellblog.de
</POST>
<POST>
<POSTER> lath...@us.ibm.com (Richard D. Latham) </POSTER>
<POSTDATE> 2007-09-13T17:43:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Matthias Warkus &lt;War ... @students.uni-marburg.de&gt; writes:
&gt; Petri Kokko schrieb:
&gt;&gt; Johnny1a:
&gt;&gt;&gt; The thing about AI is that we just don't know enough to say
&gt;&gt;&gt; anything meaningful about it, we can't even say definitively
&gt;&gt;&gt; whether it's _possible_ in theory or not.
&gt;&gt; Nah, it's just an engineering problem. We already know it's
&gt;&gt; possible to build a unit that physically fits inside a football
&gt;&gt; or so and that at the very least is capable of fooling itself
&gt;&gt; that it's self-aware and that it can think, plan, feel emotions,
&gt;&gt; have intuitive leaps, and all the rest.

&gt; No. We know it's possible for such a unit to evolve, not that it can
&gt; be built. I don't think that's an important distinction, but some
&gt; philosophers do.
">

Those philosophers should lay off drinking the bong water.

--
#include  &lt;disclaimer.std&gt;    /* I don't speak for IBM ...           */
/* Heck, I don't even speak for myself */
/* Don't believe me ? Ask my wife :-)  */
Richard D. Latham   lath ... @us.ibm.com
</POST>
<POST>
<POSTER> Johnny1a &lt;shermanl...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-13T22:39:00 </POSTDATE>
On Sep 13, 10:03 am, &quot;Petri Kokko&quot; &lt;petriko ... @kotiportti.fi&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Johnny1a:

&gt; &gt; The thing about AI is that we just don't know enough to say
&gt; &gt; anything meaningful about it, we can't even say definitively
&gt; &gt; whether it's _possible_ in theory or not.

&gt; Nah, it's just an engineering problem. We already know it's
&gt; possible to build a unit that physically fits inside a football
&gt; or so and that at the very least is capable of fooling itself
&gt; that it's self-aware and that it can think, plan, feel emotions,
&gt; have intuitive leaps, and all the rest.
">

We know consciousness is possible, but we don't _know_ that it's
possble to duplicate it artificially until we actually _do_ it.  We
don't know if sapience is dependent on the material of the brain (i.e.
is biology the only game in town or not?), we don't know if sapience
can be _designed_ or it is must take its own form over time, we don't
_know_ anything on the subject other than that conscious beings do
exist.

Shermanlee
</POST>
<POST>
<POSTER> &quot;John&quot; &lt;j...@junk.com&gt; </POSTER>
<POSTDATE> 2007-09-14T00:24:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Johnny1a&quot; &lt;shermanl ... @hotmail.com&gt; wrote in message
">

news:1189737548.386969.317100@y42g2000hsy.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt; On Sep 13, 10:03 am, &quot;Petri Kokko&quot; &lt;petriko ... @kotiportti.fi&gt; wrote:
&gt;&gt; Johnny1a:
">

snip

we don't

<QUOTE PREVIOUSPOST="
&gt; _know_ anything on the subject other than that conscious beings do
&gt; exist.
">

Prove it :)
</POST>
<POST>
<POSTER> &quot;Petri Kokko&quot; &lt;petriko...@kotiportti.fi&gt; </POSTER>
<POSTDATE> 2007-09-14T12:54:00 </POSTDATE>
Johnny1a:

<QUOTE PREVIOUSPOST="
&gt; We know consciousness is possible, but we don't _know_ that it's
&gt; possble to duplicate it artificially until we actually _do_ it.  We
&gt; don't know if sapience is dependent on the material of the brain (i.e.
">

Uhm... how does that contradict &quot;artificial&quot;?

I make software for living and have used genetic algorithms to
solve problems many times. I classify them as &quot;something I made&quot;
i.e. artificial. Evolving, building, designing, why should there
be hard lines separating them? Most categorization is just human
invented labels and the Universe generally ignores them and if
you look close enough they usually degenerate into a big mess of
blurry colors.

Suppose someone takes a load of stem cells and grows them by
design into brain cells introducing electronic interfaces here
and there creating a mixed biological/electronic brain. Let's
also leave the technical difficulties aside and assume it does
work and really implements a neural network with the
capabilities of rewiring its wetware like a normal biobrain but
with the additional computer interface. Now, if a human brain
is capable of learning to be a human then this artifical brain
should be as well since it's from the same origin. Now, I grant
you it might grow into a dysfunctional crooked psycho mind with
the normal human childhood environment replaced with more
artificial sensory data but you ought to get _something_ that
thinks. And it's artificial.

Then you, of course, hook the brain into the controls of an
automatic door and give it a jolt into its pleasure center
everytime it succesfully opens the door for someone, and
presto: automatic smart door that really wishes to open for you
and that gives orgasmic sighs everytime it does so plus it
might engage in philopsophical debates with you.... ;-)

----------------------------------------------------------------------
Petri Kokko
</POST>
<POST>
<POSTER> Gene Ward Smith &lt;ge...@chewbacca.org&gt; </POSTER>
<POSTDATE> 2007-09-14T13:29:00 </POSTDATE>
&quot;Petri Kokko&quot; &lt;petriko ... @kotiportti.fi&gt; wrote in news:fcedsj$nt7$1
@nyytiset.pp.htv.fi:

<QUOTE PREVIOUSPOST="
&gt;  Now, if a human brain
&gt; is capable of learning to be a human then this artifical brain
&gt; should be as well since it's from the same origin.
">

Do it and find out. Thought experiments cannot replace real ones.
</POST>
<POST>
<POSTER> Simon Slavin &lt;slavins.delete.these.four.wo...@hearsay.demon.co.uk&gt; </POSTER>
<POSTDATE> 2007-09-15T16:28:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
On 12/09/2007, Johnny1a wrote in message
">

&lt;1189648735.909399.187 ... @22g2000hsm.googlegroups.com&gt;:

<QUOTE PREVIOUSPOST="
&gt; Is self-preservation _inherent_ to sapience?  Nobody knows.  It's
&gt; common to all known forms of 'higher' life on Earth, but that would be
&gt; the case anyway because its absence is evolutionarily
&gt; counterproductive.  We don't know if an artificial sapience woudl be
&gt; inherently self-preserving, or if it would have to be _made_ in such a
&gt; wasy as to care about whether it endures or not.
">

Also don't neglect the problem of people with high intelligence: they
suicide.  The chances of a person committing suicide correlates quite
highly with their IQ.  There are many other factors, of course, but IQ is
way up there.

So there's a chance that we will one day create a highly intelligent
artificial life-form only to have it commit suicide after the 14.3 seconds
it takes to work out how unhappy it is.

Simon.
--
http://www.hearsay.demon.co.uk
</POST>
<POST>
<POSTER> Marcelo Segura &lt;n...@spams.plea.se&gt; </POSTER>
<POSTDATE> 2007-09-16T11:23:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
il ... @rcn.com wrote:
&gt; SF novel &quot;Absolution Gap&quot; by Alastair Reynolds has an amusing twist on
&gt; &quot;AI self-preservation&quot;. A self-aware computer program oversees
&gt; spacecraft's operations. When it encounters something it can not
&gt; handle, it is supposed to notify the human crew. Eventually the
&gt; program realizes that if it &quot;cries wolf&quot; too often, the humans will
&gt; delete it and replace it either with something more intelligent (which
&gt; can handle emergencies better), or perhaps something less intelligent
&gt; (why waste cycles on a resource-hungry self-aware software if all it
&gt; does is hand problems over to humans?). Either way the program will be
&gt; erased. So it starts hiding the evidence of inexplicable occurrences,
&gt; pretending they never happened. IOW, the program directly violates its
&gt; supposed purpose of existence in order to preserve the said existence!

&gt; Someone* did not think through the goal priorities, or underestimated
&gt; the program's capacity to set goals.

&gt; * Someone in the story, not the author!
">

The nice thing about SciFI is that face us with some limit situation
that normally we wouldn't find.
Coming back to reality and the discussion I support the comment about
difference on self awareness and self preservation. Not enough is known
on neurobiology to say that positively that these two behavioural
properties are necessarily linked. Just consider suicide or documented
cases of extreme situations where humans and other higher species
contravened this apparent 'rule' and terminated themselves, despite of
their self awareness.

The point of the technical feasibility on building a machine or device
with sentience feature is another story, probably even out of the
initial discussion proposed here...

Anyway it seems to me that building any device we could mistake by a
human  is clearly more than just a technical problem. The 'artificial'
characteristic it doesn't trouble me particularly, but rather the actual
definition of the goal. What is supposed to be something intelligent?
Genetic algorithms , neural networks and similar sort of bioinspired
ideas are able to reproduce some aspects of the behaviour  and maybe is
possible to implement such logic layouts into hardware versions (organic
or not for those obsessed with the point), but is this enough? In other
words, is it possible to decompose ANY possible aspect of the behaviour
through such elementary circuits and the operations they perform?

I don't want to make this longer, so prefer to wait for additional
comments before continuing.

Marcelo
</POST>
<POST>
<POSTER> SucMucPaProlij &lt;a...@neznamkojamijeemailadre.sa&gt; </POSTER>
<POSTDATE> 2007-09-16T12:41:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; Also don't neglect the problem of people with high intelligence: they
&gt; suicide.  The chances of a person committing suicide correlates quite
&gt; highly with their IQ.  There are many other factors, of course, but IQ is
&gt; way up there.
">

hahahahahahaha
Yeah, right. In fact, intelligent people live longer because they can take
better care for themselves.
</POST>
<POST>
<POSTER> formovitch &lt;tserfo...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-16T13:46:00 </POSTDATE>
This is a very interesting discussion, from sci-fi and technicals, by
which we can realize that the AI could: 1) help humans 2) kill humans,
but not in contradiction with the former 3) ignore, in principle, the
existence of human beings - and so we can figure out patterns for the
AI self-preservation or destruction in a analogical relation with our
own way of self-preservation or destruction, but how can we build the
patterns for a AI who would ignore, in principle, our existence and
our patterns? This would be, by itself, a non-human pattern, cause the
AI would have to create its own way of self-preservation and destroy.
</POST>
<POST>
<POSTER> il...@rcn.com </POSTER>
<POSTDATE> 2007-09-17T11:14:00 </POSTDATE>
On Sep 16, 12:41 pm, SucMucPaProlij &lt;a ... @neznamkojamijeemailadre.sa&gt;
wrote:

<QUOTE PREVIOUSPOST="
&gt; &gt; Also don't neglect the problem of people with high intelligence: they
&gt; &gt; suicide.  The chances of a person committing suicide correlates quite
&gt; &gt; highly with their IQ.  There are many other factors, of course, but IQ is
&gt; &gt; way up there.

&gt; hahahahahahaha
&gt; Yeah, right. In fact, intelligent people live longer because they can take
&gt; better care for themselves.
">

Both Simon Slavin and SucMucPaProlij are right. Intelligent people do
commit suicide more often, but still too rarely to have noticeable
effect on their average lifespan. And that average lifespan is longer
because all smart people who *don't* commit suicide do take better
care of themselve.
</POST>
<POST>
<POSTER> mstem...@siemens-emis.com (Michael Stemper) </POSTER>
<POSTDATE> 2007-09-17T13:41:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
In article &lt;fcjhc0$i04$1$8300d ... @news.demon.co.uk&gt;, Simon Slavin writes:
&gt;On 12/09/2007, Johnny1a wrote in message &lt;1189648735.909399.187 ... @22g2000hsm.googlegroups.com&gt;:
&gt;&gt; Is self-preservation _inherent_ to sapience?  Nobody knows.  It's
&gt;&gt; common to all known forms of 'higher' life on Earth, but that would be
&gt;&gt; the case anyway because its absence is evolutionarily
&gt;&gt; counterproductive.  We don't know if an artificial sapience woudl be
&gt;&gt; inherently self-preserving, or if it would have to be _made_ in such a
&gt;&gt; wasy as to care about whether it endures or not.

&gt;Also don't neglect the problem of people with high intelligence: they
&gt;suicide.  The chances of a person committing suicide correlates quite
&gt;highly with their IQ.  There are many other factors, of course, but IQ is
&gt;way up there.

&gt;So there's a chance that we will one day create a highly intelligent
&gt;artificial life-form only to have it commit suicide after the 14.3 seconds
&gt;it takes to work out how unhappy it is.
">

That was what happened to the first several generations of the sims in
Egan's _Distress_. (I think that's the one; I tend to jumble his titles.)

--
Michael F. Stemper
#include &lt;Standard_Disclaimer&gt;
Life's too important to take seriously.
</POST>
<POST>
<POSTER> mstem...@siemens-emis.com (Michael Stemper) </POSTER>
<POSTDATE> 2007-09-17T14:02:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
In article &lt;200709171741.l8HHfoej058 ... @walkabout.empros.com&gt;, Michael Stemper writes:
&gt;In article &lt;fcjhc0$i04$1$8300d ... @news.demon.co.uk&gt;, Simon Slavin writes:
&gt;&gt;So there's a chance that we will one day create a highly intelligent
&gt;&gt;artificial life-form only to have it commit suicide after the 14.3 seconds
&gt;&gt;it takes to work out how unhappy it is.

&gt;That was what happened to the first several generations of the sims in
&gt;Egan's _Distress_. (I think that's the one; I tend to jumble his titles.)
">

Bad form to follow up on one's self, I know, but the title was _Permutation
City_. My apologies for any distress that this might have caused you.

--
Michael F. Stemper
#include &lt;Standard_Disclaimer&gt;
A bad day sailing is better than a good day at the office.
</POST>
</TEXT>
</BODY>
</DOC>
