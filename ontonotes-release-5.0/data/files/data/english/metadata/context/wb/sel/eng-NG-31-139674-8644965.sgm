<DOC>
<DOCID> eng-NG-31-139674-8644965 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2007-09-13T21:23:00 </DATETIME>
<BODY>
<HEADLINE>
Bridge Rating System
</HEADLINE>
<TEXT>
<POST>
<POSTER> Sartaj Hans &lt;spadede...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-13T21:23:00 </POSTDATE>
Interested in some thoughts about a Bridge Rating System. Both Chess
and (internet) Backgammon have well established, time tested rating
systems that are reasonable predictors of (approximate) playing
strength.
Has someone worked out one for bridge ?
What would its elements be ?
How do we address the peculiar considerations due to nature of bridge
competitions ?
Would it be effective ?

Also see another post : (Sample bridge rating system)
</POST>
<POST>
<POSTER> ted &lt;morris...@bellsouth.net&gt; </POSTER>
<POSTDATE> 2007-09-13T23:10:00 </POSTDATE>
On Sep 13, 8:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?

&gt; Also see another post : (Sample bridge rating system)
">

The one OK Bridge used was called the Lehmans so if you do a Google
search on this group or the old rec.games.bridge.okbridge for Lehmans
you will get a lot of verbage on it. I won't comment on it because I
think it sucked particularly for IMPS. The best one I liked was one
used at the Dutch bridge site &quot;StepBridge&quot; It did not grind
particularly fine but had the essence of being a good measure. These
are of course online rating systems and applying them outside could
make it a calculation nightmare. BTW Swan Bridge also had one but I
never had a chance to really experience theirs.
</POST>
<POST>
<POSTER> patpowerss...@gmail.com </POSTER>
<POSTDATE> 2007-09-14T00:16:00 </POSTDATE>
On Sep 14, 9:23 am, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?

&gt; Also see another post : (Sample bridge rating system)
">

The trouble with the chess/backgammon system is that those games are
win/lose: there is no score.  There is the Butler system, I don't know
the details.
</POST>
<POST>
<POSTER> &quot;richard_wil...@hotmail.com&quot; &lt;richard.wil...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-14T08:43:00 </POSTDATE>
On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?
">

Hi Sartaj:
This is a topic that I have given a lot of thought to over the years.
Here's what I (eventually) arrived at:  I think that it is almost
impossible to implement a real rating system because there are
mutually inconsistent policy objectives.

In order for a rating system to have any value, it needs to be
accurate.  The system needs to have real predictive value.  (I don't
think that this statement is particularly controversial)

In order for a rating system to be accepted, it needs to be simple.
People need to understand and accept how the ratings are calculated.

The problem with rating schemes is that (by definition) half the folks
about there are going to be below average.  They aren't going to be
happy about this and they are going to be looking to nick pick the
system.  Back in the days when OKB had an active discussion forum,
there were long and complex fights over the implementation of the
Lehman schemes.  The Lehmans system uses a DIRT simple algorithm.
Most people still couldn't understand the algebra behind it.

I recently started working at a company called the MathWorks as a
product manager in the Math and Data Analysis group.  I have access to
some absolutely amazing software.  Furthermore, I'm working with an
engineering team whose technical background scares me at times.  I
suspect that I could convince folks to spend some spare cycles working
on this type of problem.  We're geeky enough that this would
(probably) be a fun extra-curricular.  However, I'm not particularly
interested in doing so.  Simply put, I think that it is impossible to
design a system that is

(a)  Accurate enough to have any value
(b)  Simple enough that people will accept

Recall the discussions a few months back surrounding &quot;optimal&quot; designs
for a tournament and all the objections that people raised about
strength of schedule corrections and the like.  Imagine that same
fight multiplied a hundred fold, as a multitude of second and third
rate players start screaming that the rating system doesn't rank them
nearly as strongly as it should.

Then try to imagine telling them that they should have faith in the
system because Discrete Time Kalman filters are very accurate...
</POST>
<POST>
<POSTER> David Babcock &lt;d...@fastmail.fm&gt; </POSTER>
<POSTDATE> 2007-09-14T09:57:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; The trouble with the chess/backgammon system is that those games are
&gt; win/lose: there is no score.
">

In the Elo system in chess, a player has a certain expectation of
winning a game based on his rating vs. his opponent's rating.  There
is no reason (in principle) why this expectation could not map to an
expected finishing position in a large field.

David
</POST>
<POST>
<POSTER> patpowerss...@gmail.com </POSTER>
<POSTDATE> 2007-09-14T10:19:00 </POSTDATE>
On Sep 14, 9:57 pm, David Babcock &lt;d ... @fastmail.fm&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &gt; The trouble with the chess/backgammon system is that those games are
&gt; &gt; win/lose: there is no score.

&gt; In the Elo system in chess, a player has a certain expectation of
&gt; winning a game based on his rating vs. his opponent's rating.  There
&gt; is no reason (in principle) why this expectation could not map to an
&gt; expected finishing position in a large field.

&gt; David
">

That is correct, but you are throwing away information if ignoring the
margin of victory/loss.
</POST>
<POST>
<POSTER> Sartaj Hans &lt;spadede...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-14T10:30:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; That is correct, but you are throwing away information if ignoring the
&gt; margin of victory/loss.
">

Unlike chess, my aim is not to evaluate (in effect) every game. But to
just use the final standings in the tournament.

Winning a knockout match by 10 or 90 will rarely equate to a
corresponding skill differential.
</POST>
<POST>
<POSTER> Phil &lt;psugar...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-14T10:37:00 </POSTDATE>
On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
">

250 - (percent of hands after which player gives advice to partner) -
(Average decibel level of advice given to partner)
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-14T11:00:00 </POSTDATE>
On Fri, 14 Sep 2007 05:43:07 -0700, &quot;richard_wil ... @hotmail.com&quot;

<QUOTE PREVIOUSPOST="
&lt;richard.wil ... @gmail.com&gt; wrote:
&gt;On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt;&gt; strength.
&gt;&gt; Has someone worked out one for bridge ?
&gt;&gt; What would its elements be ?
&gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt;&gt; competitions ?
&gt;&gt; Would it be effective ?

&gt;Hi Sartaj:
&gt;This is a topic that I have given a lot of thought to over the years.
&gt;Here's what I (eventually) arrived at:  I think that it is almost
&gt;impossible to implement a real rating system because there are
&gt;mutually inconsistent policy objectives.

&gt;In order for a rating system to have any value, it needs to be
&gt;accurate.  The system needs to have real predictive value.  (I don't
&gt;think that this statement is particularly controversial)

&gt;In order for a rating system to be accepted, it needs to be simple.
&gt;People need to understand and accept how the ratings are calculated.
">

No. Very few chess players understand the reasoning behind the Elo
system. However, one can estimate the effect of a score with
sufficient accuracy quite easily.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;The problem with rating schemes is that (by definition) half the folks
&gt;about there are going to be below average.  They aren't going to be
&gt;happy about this and they are going to be looking to nick pick the
&gt;system.  Back in the days when OKB had an active discussion forum,
&gt;there were long and complex fights over the implementation of the
&gt;Lehman schemes.  The Lehmans system uses a DIRT simple algorithm.
&gt;Most people still couldn't understand the algebra behind it.

&gt;I recently started working at a company called the MathWorks as a
&gt;product manager in the Math and Data Analysis group.  I have access to
&gt;some absolutely amazing software.  Furthermore, I'm working with an
&gt;engineering team whose technical background scares me at times.  I
&gt;suspect that I could convince folks to spend some spare cycles working
&gt;on this type of problem.  We're geeky enough that this would
&gt;(probably) be a fun extra-curricular.  However, I'm not particularly
&gt;interested in doing so.  Simply put, I think that it is impossible to
&gt;design a system that is

&gt;(a)  Accurate enough to have any value
">

Why shouldn't this be possible? The random element in bridge scores is
very large.This makes results of short tournaments unreliable but it
isn't a problem in the long run.

<QUOTE PREVIOUSPOST="
&gt;(b)  Simple enough that people will accept
">

I don't believe the issue is simplicity. Before Elo there were lots of
chess rating systems around. Every national federation had one. Most
of these weren't as dumb as the Lehman system, but they were all
logically simpler than the Elo system. Today all national federations
have switched to Elo.

I think the problem of acceptance has to do with the randomness of
bridge scores. This allows players to believe that their good scores
are an accurate reflection of their skill and that their bad scores
are due to bad luck. The last thing these players want is an accurate
measure of skill. Similarly the majority of pros have no interest in
such a measure for obvious reasons.

<QUOTE PREVIOUSPOST="
&gt;Recall the discussions a few months back surrounding &quot;optimal&quot; designs
&gt;for a tournament and all the objections that people raised about
&gt;strength of schedule corrections and the like.  Imagine that same
&gt;fight multiplied a hundred fold, as a multitude of second and third
&gt;rate players start screaming that the rating system doesn't rank them
&gt;nearly as strongly as it should.

&gt;Then try to imagine telling them that they should have faith in the
&gt;system because Discrete Time Kalman filters are very accurate...
">

There is no need for bullshit. The effect of a single tournament will
be so small that a linear approximation will be sufficiently accurate.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="

">
</POST>
<POST>
<POSTER> ted &lt;morris...@bellsouth.net&gt; </POSTER>
<POSTDATE> 2007-09-14T11:14:00 </POSTDATE>
On Sep 14, 9:30 am, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &gt; That is correct, but you are throwing away information if ignoring the
&gt; &gt; margin of victory/loss.

&gt; Unlike chess, my aim is not to evaluate (in effect) every game. But to
&gt; just use the final standings in the tournament.
">

Are you trying to rate players, pairs, or teams?

<QUOTE PREVIOUSPOST="
&gt; Winning a knockout match by 10 or 90 will rarely equate to a
&gt; corresponding skill differential.
">
</POST>
<POST>
<POSTER> Tim &lt;t...@pobox.com&gt; </POSTER>
<POSTDATE> 2007-09-14T11:34:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; In order for a rating system to be accepted, it needs to be simple.
&gt; People need to understand and accept how the ratings are calculated.
">

I don't think this is true.  If the rating system told people what
they wanted to hear, it wouldn't matter how complex it was.

I think Jürgen is spot on here:

<QUOTE PREVIOUSPOST="
&gt; I think the problem of acceptance has to do with the
&gt; randomness of bridge scores. This allows players to
&gt; believe that their good scores are an accurate reflection
&gt; of their skill and that their bad scores are due to bad
&gt; luck. The last thing these players want is an accurate
&gt; measure of skill.
">

In my opinion, the more complex, the better.  That way it is less
likely that the system could be easily manipulated.

Tim
</POST>
<POST>
<POSTER> &quot;richard_wil...@hotmail.com&quot; &lt;richard.wil...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-14T12:14:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; &gt;Then try to imagine telling them that they should have faith in the
&gt; &gt;system because Discrete Time Kalman filters are very accurate...
&gt; There is no need for bullshit. The effect of a single tournament will
&gt; be so small that a linear approximation will be sufficiently accurate.
">

I stand by my basic point:

I throw out a hypothetical example and folks are already nitpicking
over methodology
</POST>
<POST>
<POSTER> Tysen Streib &lt;tyse...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-14T12:59:00 </POSTDATE>
On Sep 14, 8:14 am, ted &lt;morris ... @bellsouth.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Are you trying to rate players, pairs, or teams?
">

This is the key point that has made these discussions fall apart in
the past.  If the rating is applied to individuals, how do you rate
the strength of a pair with one strong player and one weak one.  It
can't be just an average b/c the stronger member can usually carry
more than his fair share and the pair won't do so bad.

Tysen
</POST>
<POST>
<POSTER> &quot;richard_wil...@hotmail.com&quot; &lt;richard.wil...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-14T14:04:00 </POSTDATE>
On Sep 14, 12:59 pm, Tysen Streib &lt;tyse ... @gmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On Sep 14, 8:14 am, ted &lt;morris ... @bellsouth.net&gt; wrote:

&gt; &gt; Are you trying to rate players, pairs, or teams?

&gt; This is the key point that has made these discussions fall apart in
&gt; the past.  If the rating is applied to individuals, how do you rate
&gt; the strength of a pair with one strong player and one weak one.  It
&gt; can't be just an average b/c the stronger member can usually carry
&gt; more than his fair share and the pair won't do so bad.

&gt; Tysen
">

I would think that the logical answer is to start by rating teams.  If
you aren't able to rate teams, give up then and there.

If you are able to rate teams accurately, try to extend your analysis
to rating pairs...

yada, yada, yada
</POST>
<POST>
<POSTER> Ron Johnson &lt;john...@ccrs.nrcan.gc.ca&gt; </POSTER>
<POSTDATE> 2007-09-14T15:40:00 </POSTDATE>
On Sep 14, 8:43 am, &quot;richard_wil ... @hotmail.com&quot;

<QUOTE PREVIOUSPOST="
&lt;richard.wil ... @gmail.com&gt; wrote:
&gt; On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

&gt; &gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; &gt; and (internet) Backgammon have well established, time tested rating
&gt; &gt; systems that are reasonable predictors of (approximate) playing
&gt; &gt; strength.
&gt; &gt; Has someone worked out one for bridge ?
&gt; &gt; What would its elements be ?
&gt; &gt; How do we address the peculiar considerations due to nature of bridge
&gt; &gt; competitions ?
&gt; &gt; Would it be effective ?

&gt; Hi Sartaj:
&gt; This is a topic that I have given a lot of thought to over the years.
&gt; Here's what I (eventually) arrived at:  I think that it is almost
&gt; impossible to implement a real rating system because there are
&gt; mutually inconsistent policy objectives.

&gt; In order for a rating system to have any value, it needs to be
&gt; accurate.  The system needs to have real predictive value.  (I don't
&gt; think that this statement is particularly controversial)

&gt; In order for a rating system to be accepted, it needs to be simple.
&gt; People need to understand and accept how the ratings are calculated.

&gt; The problem with rating schemes is that (by definition) half the folks
&gt; about there are going to be below average.  They aren't going to be
&gt; happy about this and they are going to be looking to nick pick the
&gt; system.  Back in the days when OKB had an active discussion forum,
&gt; there were long and complex fights over the implementation of the
&gt; Lehman schemes.  The Lehmans system uses a DIRT simple algorithm.
&gt; Most people still couldn't understand the algebra behind it.

&gt; I recently started working at a company called the MathWorks as a
&gt; product manager in the Math and Data Analysis group.  I have access to
&gt; some absolutely amazing software.  Furthermore, I'm working with an
&gt; engineering team whose technical background scares me at times.  I
&gt; suspect that I could convince folks to spend some spare cycles working
&gt; on this type of problem.  We're geeky enough that this would
&gt; (probably) be a fun extra-curricular.  However, I'm not particularly
&gt; interested in doing so.  Simply put, I think that it is impossible to
&gt; design a system that is

&gt; (a)  Accurate enough to have any value
&gt; (b)  Simple enough that people will accept
">

(c) accurately captures the partnership nature of the game.
(I'm confident that the truly contentious part of any ranking
system would be the issue of people who play only team
games with professional teammates.

While Nick Nickell for instance is a capable player -- far
better than I am -- he'd have won precisely zero major
events playing with teammates as good as he is.

That said, I think you could probably get a limited buy-in if
it was promoted with a &quot;handle with care -- huge initial
margin of error&quot;. Within a few years you'd have general acceptance
of a well-designed system. While nobody would be happy with
&quot;C class&quot;, &quot;about as good as Betty and better than Sam&quot;
would make sense to most.
</POST>
<POST>
<POSTER> &quot;pumpkin_...@hotmail.com&quot; &lt;pumpkin_...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-14T18:24:00 </POSTDATE>
On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?

&gt; Also see another post : (Sample bridge rating system)
">

Your idea is excellent. However, in order for it to work it will be
necessary to overcome not only some considerable technical obstacles
but also the considerable opposition that can be expected from
entrenched interests.

In particular, assuming the measurement problems can be overcome, I
suspect that it will be quite difficult to persuade the majority of
NCBOs and the bulk of the rank-and-file players to participate in such
a system.

For example, in North America both the ACBL and a large chunk of its
membership have a strong self-interest in preserving the current
rating scheme, masterpoints, which is predicated upon the quite flawed
notion that quantity is a substitute for quality. As such, both the
League and the bulk of its membership will likely resist a rating
scheme that seeks to objectively measure the quality of each player's
bridge skills.

I suspect that without the participation of the League and its members-
the bulk of whom are quite happy deluding themselves that more means
better-an ELO type system will have little practical impact.

Consequently, in addition to solving technical problems, it will also
be necessary to spend considerable time developing arguments in favour
of adopting an ELO-type system that both the NCBOs and the rank-and-
file members will find persuasive.

Cheers.

Nick
</POST>
<POST>
<POSTER> Derek Broughton &lt;n...@pointerstop.ca&gt; </POSTER>
<POSTDATE> 2007-09-14T19:25:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Jürgen R. wrote:
&gt; I think the problem of acceptance has to do with the randomness of
&gt; bridge scores. This allows players to believe that their good scores
&gt; are an accurate reflection of their skill and that their bad scores
&gt; are due to bad luck. The last thing these players want is an accurate
&gt; measure of skill. Similarly the majority of pros have no interest in
&gt; such a measure for obvious reasons.
">

Do we actually know the &quot;majority of pros&quot; would have a problem with a
rating system?  ime, most of the pros would rate pretty well.  There are
sure to be some pros out there who aren't nearly as good as they tell their
clients they are, but I think the &quot;majority&quot; will actually believe that
they're not in that group, and would like a system that would show the
charlatans as what they are.
--
derek
</POST>
<POST>
<POSTER> Michael Angelo Ravera &lt;marav...@prodigy.net&gt; </POSTER>
<POSTDATE> 2007-09-14T20:09:00 </POSTDATE>
On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?

&gt; Also see another post : (Sample bridge rating system)
">

The system that I have used for our club is like this:

1 Calculate the average adjusted percentile order of finish according
to the method of scoring and movement and assume that partners and
teammates have tied with them. This gives a slight (but probably
justified) advantage to larger games. Finishing as top pair in a 5-
table two-winner game gives each first place winner approximately a
90. First out of 15 is more like 95. The average of these scores
weighted accoring to the number of competitors and length of event (we
can count boards, sessions, or get fancy about length, but I have, for
the sake of simplicity assumed that all events were of equal length)
This is the R0 of the player.

2) Calculate the raiting factor &quot;G0&quot; of each event to be 1/50 of the
average R0 of the players in the game. This will give you a rating
factor for each event where 1.00 is the average. We may choose to
avoid figuring the average based upon players without meaningful
ratings as determined in step 4.

3) Now go back an rerate each player by taking the weighted average of
R0 multiplied by the G0 event rating. This is the player's R1.

4) Discard (or don't publish) the rating of any player who hasn't
played in enough events to obtain a meaningful rating. For our club, I
decided that this was 6 events for the month. This can be increased
for ratings over longer periods.

5) R1s are only meaningful for a period of time. If long term ratings
are published, &quot;Last 10&quot;, &quot;Last 25&quot;, &quot;Last 100&quot; events, and &quot;Career&quot;
would make sense.

6) You can, if you are interested, work out an R2 based upon the
rerating of events (a G1) based upon players' R1s, but it probably
won't change the order of ratings very much.

Players who never play in any other configuration such as always
playing with the same partner and only playing with the same partner
who never plays with anyone else (or always only in team events with
exactly the same teammates) will be rated identically. Players who
play enough in different configurations (or who play with partners or
teammates who play in other configurations)  will sort themselves out.

If you are further interested, you can do some Bayesian analysis for
partnerships with infrequent (but sufficient) outside plays or where
one partner never plays with anyone else, but that's a little more
complicated.

The beauty of this exact system is that a player who gets a 50 is an
average player. A player who can't ever stay out last place even in a
low rated game would be rated very low. A player who would be expected
to win EVERY TIME in an average field would be rated in excess of 100
(although his R0 would only asymptotically approach 100).
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-15T04:12:00 </POSTDATE>
On Fri, 14 Sep 2007 20:25:17 -0300, Derek Broughton

<QUOTE PREVIOUSPOST="
&lt;n ... @pointerstop.ca&gt; wrote:
&gt;Jürgen R. wrote:

&gt;&gt; I think the problem of acceptance has to do with the randomness of
&gt;&gt; bridge scores. This allows players to believe that their good scores
&gt;&gt; are an accurate reflection of their skill and that their bad scores
&gt;&gt; are due to bad luck. The last thing these players want is an accurate
&gt;&gt; measure of skill. Similarly the majority of pros have no interest in
&gt;&gt; such a measure for obvious reasons.

&gt;Do we actually know the &quot;majority of pros&quot; would have a problem with a
&gt;rating system?
">

I can't claim to have surveyed them. However, by definition the
majority of pros are not the best and the clients are sure to look at
the ratings.
I suspect that, even regardless of clients, the majority of pros don't
want to know where they rank. Much nicer to have won multiple world
and national championships than to know you are number 73 in the
world.

<QUOTE PREVIOUSPOST="
&gt;  ime, most of the pros would rate pretty well.  There are
&gt;sure to be some pros out there who aren't nearly as good as they tell their
&gt;clients they are, but I think the &quot;majority&quot; will actually believe that
&gt;they're not in that group, and would like a system that would show the
&gt;charlatans as what they are.
">
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-15T04:28:00 </POSTDATE>
On Fri, 14 Sep 2007 17:09:51 -0700, Michael Angelo Ravera

<QUOTE PREVIOUSPOST="
&lt;marav ... @prodigy.net&gt; wrote:
&gt;On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt;&gt; strength.
&gt;&gt; Has someone worked out one for bridge ?
&gt;&gt; What would its elements be ?
&gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt;&gt; competitions ?
&gt;&gt; Would it be effective ?

&gt;&gt; Also see another post : (Sample bridge rating system)

&gt;The system that I have used for our club is like this:

&gt;1 Calculate the average adjusted percentile order of finish according
&gt;to the method of scoring and movement and assume that partners and
&gt;teammates have tied with them. This gives a slight (but probably
&gt;justified) advantage to larger games. Finishing as top pair in a 5-
&gt;table two-winner game gives each first place winner approximately a
&gt;90. First out of 15 is more like 95. The average of these scores
&gt;weighted accoring to the number of competitors and length of event (we
&gt;can count boards, sessions, or get fancy about length, but I have, for
&gt;the sake of simplicity assumed that all events were of equal length)
&gt;This is the R0 of the player.

&gt;2) Calculate the raiting factor &quot;G0&quot; of each event to be 1/50 of the
&gt;average R0 of the players in the game. This will give you a rating
&gt;factor for each event where 1.00 is the average. We may choose to
&gt;avoid figuring the average based upon players without meaningful
&gt;ratings as determined in step 4.

&gt;3) Now go back an rerate each player by taking the weighted average of
&gt;R0 multiplied by the G0 event rating. This is the player's R1.
">

Doesn't this have the defect that the winner may lose rating points?

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;4) Discard (or don't publish) the rating of any player who hasn't
&gt;played in enough events to obtain a meaningful rating. For our club, I
&gt;decided that this was 6 events for the month. This can be increased
&gt;for ratings over longer periods.

&gt;5) R1s are only meaningful for a period of time. If long term ratings
&gt;are published, &quot;Last 10&quot;, &quot;Last 25&quot;, &quot;Last 100&quot; events, and &quot;Career&quot;
&gt;would make sense.

&gt;6) You can, if you are interested, work out an R2 based upon the
&gt;rerating of events (a G1) based upon players' R1s, but it probably
&gt;won't change the order of ratings very much.

&gt;Players who never play in any other configuration such as always
&gt;playing with the same partner and only playing with the same partner
&gt;who never plays with anyone else (or always only in team events with
&gt;exactly the same teammates) will be rated identically. Players who
&gt;play enough in different configurations (or who play with partners or
&gt;teammates who play in other configurations)  will sort themselves out.

&gt;If you are further interested, you can do some Bayesian analysis for
&gt;partnerships with infrequent (but sufficient) outside plays or where
&gt;one partner never plays with anyone else, but that's a little more
&gt;complicated.

&gt;The beauty of this exact system is that a player who gets a 50 is an
&gt;average player. A player who can't ever stay out last place even in a
&gt;low rated game would be rated very low. A player who would be expected
&gt;to win EVERY TIME in an average field would be rated in excess of 100
&gt;(although his R0 would only asymptotically approach 100).
">

Undoubtedly this kind of scheme is sufficient for a club. It has the
advantage of being transparent.
In a wider context you would need to justify the weights in your
averages (because presumably you would be interested in accuracy) and
the right procedure is not simpy arithmetical.
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-15T04:56:00 </POSTDATE>
On Fri, 14 Sep 2007 15:24:38 -0700, &quot;pumpkin_ ... @hotmail.com&quot;

<QUOTE PREVIOUSPOST="
&lt;pumpkin_ ... @hotmail.com&gt; wrote:
&gt;On Sep 13, 9:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt;&gt; strength.
&gt;&gt; Has someone worked out one for bridge ?
&gt;&gt; What would its elements be ?
&gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt;&gt; competitions ?
&gt;&gt; Would it be effective ?

&gt;&gt; Also see another post : (Sample bridge rating system)

&gt;Your idea is excellent. However, in order for it to work it will be
&gt;necessary to overcome not only some considerable technical obstacles
&gt;but also the considerable opposition that can be expected from
&gt;entrenched interests.

&gt;In particular, assuming the measurement problems can be overcome, I
&gt;suspect that it will be quite difficult to persuade the majority of
&gt;NCBOs and the bulk of the rank-and-file players to participate in such
&gt;a system.
">

The rank-and-file players are not the primary problem. The federations
could easily make a new method of classification palatable - the
current method has enough defects. However, the NCBO's (not just the
ACBL) are not interested for two reasons: They make money selling
masterpoints and the influential players have no interest.

<QUOTE PREVIOUSPOST="
&gt;For example, in North America both the ACBL and a large chunk of its
&gt;membership have a strong self-interest in preserving the current
&gt;rating scheme, masterpoints, which is predicated upon the quite flawed
&gt;notion that quantity is a substitute for quality. As such, both the
&gt;League and the bulk of its membership will likely resist a rating
&gt;scheme that seeks to objectively measure the quality of each player's
&gt;bridge skills.
">

Masterpoint schemes don't need to end with the introduction of a
rating system.

<QUOTE PREVIOUSPOST="
&gt;I suspect that without the participation of the League and its members-
&gt;the bulk of whom are quite happy deluding themselves that more means
&gt;better-an ELO type system will have little practical impact.

&gt;Consequently, in addition to solving technical problems, it will also
&gt;be necessary to spend considerable time developing arguments in favour
&gt;of adopting an ELO-type system that both the NCBOs and the rank-and-
&gt;file members will find persuasive.
">

I don't think there is a way to get the attention of the NCBO's. The
arguments in favor of Elo-type system are well known. The problem is
that it isn't obvious what the NCBO's will gain by introducing such a
system. Bridge simply isn't the kind of game where the competitors
seek to find out who is best.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt;Cheers.

&gt;Nick
">
</POST>
<POST>
<POSTER> Michael Angelo Ravera &lt;marav...@prodigy.net&gt; </POSTER>
<POSTDATE> 2007-09-15T06:58:00 </POSTDATE>
On Sep 15, 1:28 am, Jürgen R. &lt;jurg ... @web.de&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On Fri, 14 Sep 2007 17:09:51 -0700, Michael Angelo Ravera

&gt; &lt;marav ... @prodigy.net&gt; wrote:
&gt; &gt;On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt; &gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; &gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt; &gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt; &gt;&gt; strength.
&gt; &gt;&gt; Has someone worked out one for bridge ?
&gt; &gt;&gt; What would its elements be ?
&gt; &gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt; &gt;&gt; competitions ?
&gt; &gt;&gt; Would it be effective ?

&gt; &gt;&gt; Also see another post : (Sample bridge rating system)

&gt; &gt;The system that I have used for our club is like this:

&gt; &gt;1 Calculate the average adjusted percentile order of finish according
&gt; &gt;to the method of scoring and movement and assume that partners and
&gt; &gt;teammates have tied with them. This gives a slight (but probably
&gt; &gt;justified) advantage to larger games. Finishing as top pair in a 5-
&gt; &gt;table two-winner game gives each first place winner approximately a
&gt; &gt;90. First out of 15 is more like 95. The average of these scores
&gt; &gt;weighted accoring to the number of competitors and length of event (we
&gt; &gt;can count boards, sessions, or get fancy about length, but I have, for
&gt; &gt;the sake of simplicity assumed that all events were of equal length)
&gt; &gt;This is the R0 of the player.

&gt; &gt;2) Calculate the raiting factor &quot;G0&quot; of each event to be 1/50 of the
&gt; &gt;average R0 of the players in the game. This will give you a rating
&gt; &gt;factor for each event where 1.00 is the average. We may choose to
&gt; &gt;avoid figuring the average based upon players without meaningful
&gt; &gt;ratings as determined in step 4.

&gt; &gt;3) Now go back an rerate each player by taking the weighted average of
&gt; &gt;R0 multiplied by the G0 event rating. This is the player's R1.

&gt; Doesn't this have the defect that the winner may lose rating points?
">

They won't lose rating points due specifically to step 3. If your
comment is that someone who did nothing in his first events used to
establish is meaningful rating except finish first in 1.60 rated 100
table events (of which there are not many), then I guess it is
possible to lose rating points by playing in a small game with a 0.80
rating, but I haven't even heard of anyone who finshes first in every
event in which they have ever played and played in enough events to
get a meaningful rating and only playing in large events and then
wanting to play in a very small low rated game.

Average percentile order of finish has the desirable characteristic
that average is really average. Finishing third out of 5 and 8th out
of 15 are treated the same. It and all of the iterations are going to
be approximately normally distributed with 50 as a mean.

As I recall, the meaningful R1s in our club (last time I calculated
them about 3 years ago) ranged approximately from 27 to 72 or
thereabouts (about 150 people with meaningful ratings).

The monthly average G1 ratings of our club games ranged from about
0.80 (limited to non-Life masters) to about 1.20 (the Game in which
Rose Meltzer, Ed Barlow, and Jim Hayashi [as well as a few other
assorted flighted national champions like Stephen Tu whose names you
might not recognize] regularly played at the time).

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt; &gt;4) Discard (or don't publish) the rating of any player who hasn't
&gt; &gt;played in enough events to obtain a meaningful rating. For our club, I
&gt; &gt;decided that this was 6 events for the month. This can be increased
&gt; &gt;for ratings over longer periods.

&gt; &gt;5) R1s are only meaningful for a period of time. If long term ratings
&gt; &gt;are published, &quot;Last 10&quot;, &quot;Last 25&quot;, &quot;Last 100&quot; events, and &quot;Career&quot;
&gt; &gt;would make sense.

&gt; &gt;6) You can, if you are interested, work out an R2 based upon the
&gt; &gt;rerating of events (a G1) based upon players' R1s, but it probably
&gt; &gt;won't change the order of ratings very much.

&gt; &gt;Players who never play in any other configuration such as always
&gt; &gt;playing with the same partner and only playing with the same partner
&gt; &gt;who never plays with anyone else (or always only in team events with
&gt; &gt;exactly the same teammates) will be rated identically. Players who
&gt; &gt;play enough in different configurations (or who play with partners or
&gt; &gt;teammates who play in other configurations)  will sort themselves out.

&gt; &gt;If you are further interested, you can do some Bayesian analysis for
&gt; &gt;partnerships with infrequent (but sufficient) outside plays or where
&gt; &gt;one partner never plays with anyone else, but that's a little more
&gt; &gt;complicated.

&gt; &gt;The beauty of this exact system is that a player who gets a 50 is an
&gt; &gt;average player. A player who can't ever stay out last place even in a
&gt; &gt;low rated game would be rated very low. A player who would be expected
&gt; &gt;to win EVERY TIME in an average field would be rated in excess of 100
&gt; &gt;(although his R0 would only asymptotically approach 100).

&gt; Undoubtedly this kind of scheme is sufficient for a club. It has the
&gt; advantage of being transparent.
&gt; In a wider context you would need to justify the weights in your
&gt; averages (because presumably you would be interested in accuracy) and
&gt; the right procedure is not simpy arithmetical.
">

I'd love to iterate with GNs and RNs until there either there are no
changes of positions worldwide or groups start to thrash. It would
also be more accurate to exclude the R of the player himself from the
weight the G of the event for that player. I suspect that interation
beyond one or two steps will not materially alter either the order of
the player ratings nor with it materially alter the actual value of
the ratings. My sense of the problem space is that after 3 iterations,
you probably have 6 digits of accuracy. One more iteration will
roughly get you  accuracy to sort out the entire world's population.

Keep in mind that the average G for games worldwide will have to
remain at or near 1.00 to avoid rating inflation. As such this rating
system only compares contemporary players. I suppose that we could
find some way of comparing players of different eras by allowing the
average game rating to float with the skill of the players relative to
a certain date.

Notice that there is no system available to improve your rating except
to play in higher rated events and finish above average. If you have a
72 rating, you can't go beat up a 0.75 game and inflate your rating
even if you always win first place every time. If you only have a 62
rating, you probably aren't a good enough player to always with in the
0.75 game every time (You'd need to finish second or above just to
keep your rating stable).

Of course, it would work even better to get the average of only the
people whom you beat.
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-15T07:31:00 </POSTDATE>
On Sat, 15 Sep 2007 03:58:20 -0700, Michael Angelo Ravera

<QUOTE PREVIOUSPOST="
&lt;marav ... @prodigy.net&gt; wrote:
&gt;On Sep 15, 1:28 am, Jürgen R. &lt;jurg ... @web.de&gt; wrote:
&gt;&gt; On Fri, 14 Sep 2007 17:09:51 -0700, Michael Angelo Ravera

&gt;&gt; &lt;marav ... @prodigy.net&gt; wrote:
&gt;&gt; &gt;On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt;&gt; &gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt;&gt; &gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt;&gt; &gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt;&gt; &gt;&gt; strength.
&gt;&gt; &gt;&gt; Has someone worked out one for bridge ?
&gt;&gt; &gt;&gt; What would its elements be ?
&gt;&gt; &gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt;&gt; &gt;&gt; competitions ?
&gt;&gt; &gt;&gt; Would it be effective ?

&gt;&gt; &gt;&gt; Also see another post : (Sample bridge rating system)

&gt;&gt; &gt;The system that I have used for our club is like this:

&gt;&gt; &gt;1 Calculate the average adjusted percentile order of finish according
&gt;&gt; &gt;to the method of scoring and movement and assume that partners and
&gt;&gt; &gt;teammates have tied with them. This gives a slight (but probably
&gt;&gt; &gt;justified) advantage to larger games. Finishing as top pair in a 5-
&gt;&gt; &gt;table two-winner game gives each first place winner approximately a
&gt;&gt; &gt;90. First out of 15 is more like 95. The average of these scores
&gt;&gt; &gt;weighted accoring to the number of competitors and length of event (we
&gt;&gt; &gt;can count boards, sessions, or get fancy about length, but I have, for
&gt;&gt; &gt;the sake of simplicity assumed that all events were of equal length)
&gt;&gt; &gt;This is the R0 of the player.

&gt;&gt; &gt;2) Calculate the raiting factor &quot;G0&quot; of each event to be 1/50 of the
&gt;&gt; &gt;average R0 of the players in the game. This will give you a rating
&gt;&gt; &gt;factor for each event where 1.00 is the average. We may choose to
&gt;&gt; &gt;avoid figuring the average based upon players without meaningful
&gt;&gt; &gt;ratings as determined in step 4.

&gt;&gt; &gt;3) Now go back an rerate each player by taking the weighted average of
&gt;&gt; &gt;R0 multiplied by the G0 event rating. This is the player's R1.

&gt;&gt; Doesn't this have the defect that the winner may lose rating points?

&gt;They won't lose rating points due specifically to step 3. If your
&gt;comment is that someone who did nothing in his first events used to
&gt;establish is meaningful rating except finish first in 1.60 rated 100
&gt;table events (of which there are not many), then I guess it is
&gt;possible to lose rating points by playing in a small game with a 0.80
&gt;rating, but I haven't even heard of anyone who finshes first in every
&gt;event in which they have ever played and played in enough events to
&gt;get a meaningful rating and only playing in large events and then
&gt;wanting to play in a very small low rated game.

&gt;Average percentile order of finish has the desirable characteristic
&gt;that average is really average. Finishing third out of 5 and 8th out
&gt;of 15 are treated the same. It and all of the iterations are going to
&gt;be approximately normally distributed with 50 as a mean.

&gt;As I recall, the meaningful R1s in our club (last time I calculated
&gt;them about 3 years ago) ranged approximately from 27 to 72 or
&gt;thereabouts (about 150 people with meaningful ratings).

&gt;The monthly average G1 ratings of our club games ranged from about
&gt;0.80 (limited to non-Life masters) to about 1.20 (the Game in which
&gt;Rose Meltzer, Ed Barlow, and Jim Hayashi [as well as a few other
&gt;assorted flighted national champions like Stephen Tu whose names you
&gt;might not recognize] regularly played at the time).

&gt;&gt; &gt;4) Discard (or don't publish) the rating of any player who hasn't
&gt;&gt; &gt;played in enough events to obtain a meaningful rating. For our club, I
&gt;&gt; &gt;decided that this was 6 events for the month. This can be increased
&gt;&gt; &gt;for ratings over longer periods.

&gt;&gt; &gt;5) R1s are only meaningful for a period of time. If long term ratings
&gt;&gt; &gt;are published, &quot;Last 10&quot;, &quot;Last 25&quot;, &quot;Last 100&quot; events, and &quot;Career&quot;
&gt;&gt; &gt;would make sense.

&gt;&gt; &gt;6) You can, if you are interested, work out an R2 based upon the
&gt;&gt; &gt;rerating of events (a G1) based upon players' R1s, but it probably
&gt;&gt; &gt;won't change the order of ratings very much.

&gt;&gt; &gt;Players who never play in any other configuration such as always
&gt;&gt; &gt;playing with the same partner and only playing with the same partner
&gt;&gt; &gt;who never plays with anyone else (or always only in team events with
&gt;&gt; &gt;exactly the same teammates) will be rated identically. Players who
&gt;&gt; &gt;play enough in different configurations (or who play with partners or
&gt;&gt; &gt;teammates who play in other configurations)  will sort themselves out.

&gt;&gt; &gt;If you are further interested, you can do some Bayesian analysis for
&gt;&gt; &gt;partnerships with infrequent (but sufficient) outside plays or where
&gt;&gt; &gt;one partner never plays with anyone else, but that's a little more
&gt;&gt; &gt;complicated.

&gt;&gt; &gt;The beauty of this exact system is that a player who gets a 50 is an
&gt;&gt; &gt;average player. A player who can't ever stay out last place even in a
&gt;&gt; &gt;low rated game would be rated very low. A player who would be expected
&gt;&gt; &gt;to win EVERY TIME in an average field would be rated in excess of 100
&gt;&gt; &gt;(although his R0 would only asymptotically approach 100).

&gt;&gt; Undoubtedly this kind of scheme is sufficient for a club. It has the
&gt;&gt; advantage of being transparent.
&gt;&gt; In a wider context you would need to justify the weights in your
&gt;&gt; averages (because presumably you would be interested in accuracy) and
&gt;&gt; the right procedure is not simpy arithmetical.

&gt;I'd love to iterate with GNs and RNs until there either there are no
&gt;changes of positions worldwide or groups start to thrash. It would
&gt;also be more accurate to exclude the R of the player himself from the
&gt;weight the G of the event for that player. I suspect that interation
&gt;beyond one or two steps will not materially alter either the order of
&gt;the player ratings nor with it materially alter the actual value of
&gt;the ratings. My sense of the problem space is that after 3 iterations,
&gt;you probably have 6 digits of accuracy.
">

You are joking, I hope. What do you think you are actually
'measuring'? What do you think a rating value is? What should it be?

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt; One more iteration will
&gt;roughly get you  accuracy to sort out the entire world's population.

&gt;Keep in mind that the average G for games worldwide will have to
&gt;remain at or near 1.00 to avoid rating inflation. As such this rating
&gt;system only compares contemporary players. I suppose that we could
&gt;find some way of comparing players of different eras by allowing the
&gt;average game rating to float with the skill of the players relative to
&gt;a certain date.

&gt;Notice that there is no system available to improve your rating except
&gt;to play in higher rated events and finish above average. If you have a
&gt;72 rating, you can't go beat up a 0.75 game and inflate your rating
&gt;even if you always win first place every time. If you only have a 62
&gt;rating, you probably aren't a good enough player to always with in the
&gt;0.75 game every time (You'd need to finish second or above just to
&gt;keep your rating stable).

&gt;Of course, it would work even better to get the average of only the
&gt;people whom you beat.
">
</POST>
<POST>
<POSTER> Mephistopheles Jones &lt;rrre...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-15T07:41:00 </POSTDATE>
On Sep 14, 9:59 am, Tysen Streib &lt;tyse ... @gmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On Sep 14, 8:14 am, ted &lt;morris ... @bellsouth.net&gt; wrote:

&gt; &gt; Are you trying to rate players, pairs, or teams?

&gt; This is the key point that has made these discussions fall apart in
&gt; the past.  If the rating is applied to individuals, how do you rate
&gt; the strength of a pair with one strong player and one weak one.  It
&gt; can't be just an average b/c the stronger member can usually carry
&gt; more than his fair share and the pair won't do so bad.

&gt; Tysen
">

I'm sure everyone knows it is silly to rate individuals in a non-
individual event.  Since every pair in a pairs game is usually
constant, rating pairs as single entities seems like a reasonable
first step.  A fair rating system for team events seems like a
challenge since players take turns playing on teams with &gt;4 members.

But there are social ramifications to consider, in say, the ACBL.  For
example, if there were individual ratings, people would be even *more*
picky at partnership desks at tournaments, since they risk lowering
their individual rating if they play with a perceived weaker partner.
And Pro-Am games would be unrated.  Also, any event with an upper- or
lower-limits on master points should be unrated, IMO, lest it skew the
results.
</POST>
<POST>
<POSTER> patmpow...@gmail.com </POSTER>
<POSTDATE> 2007-09-15T08:17:00 </POSTDATE>
On Sep 15, 7:41 pm, Mephistopheles Jones &lt;rrre ... @gmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; But there are social ramifications to consider, in say, the ACBL.  For
&gt; example, if there were individual ratings, people would be even *more*
&gt; picky at partnership desks at tournaments, since they risk lowering
&gt; their individual rating if they play with a perceived weaker partner.
&gt; And Pro-Am games would be unrated.  Also, any event with an upper- or
&gt; lower-limits on master points should be unrated, IMO, lest it skew the
&gt; results.
">

No no, if the ranking system is built correctly it would AVOID these
problems.
</POST>
<POST>
<POSTER> David Babcock &lt;d...@fastmail.fm&gt; </POSTER>
<POSTDATE> 2007-09-15T09:45:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; If the rating is applied to individuals, how do you rate
&gt; the strength of a pair with one strong player and one weak one.  It
&gt; can't be just an average b/c the stronger member can usually carry
&gt; more than his fair share and the pair won't do so bad.
">

The extent to which a strong player can carried a weaker
can be quantified based on available data.  I have no idea
whether anyone has actually done that work though.

Games where a pro does not carry a student because the arrangement
is that the student wants to learn rather than to win will corrupt
this to some degree, I grant.

David
</POST>
<POST>
<POSTER> dak...@aol.com </POSTER>
<POSTDATE> 2007-09-15T10:53:00 </POSTDATE>
Interested in some thoughts about a Bridge Rating System.

Take a collection of Pavlicek/Goren/'pet' teacher quizzes.
Score players who take those quizzes.
Weight the answers for frequency that eg. holdup/falsecard/combo/%/etc.
( whichever is topic of a given quiz) is needed. Award 'master' to
only those surpassing a threshhold score.
</POST>
<POST>
<POSTER> &quot;Eric Kehr&quot; &lt;eric.k...@dsl.pipex.com&gt; </POSTER>
<POSTDATE> 2007-09-15T13:20:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
On Sat, 15 Sep 2007 09:12:28 +0100, Jürgen R. &lt;jurg ... @web.de&gt; wrote:
&gt; I suspect that, even regardless of clients, the majority of pros don't
&gt; want to know where they rank. Much nicer to have won multiple world
&gt; and national championships than to know you are number 73 in the
&gt; world.
">

But conversely, it is much better to know you are number 20 in the world
than just to know that you have never managed to win a world championship.

And for every world champion there will be hundreds of extremely good
non-world champions.

Eric Kehr
London, England
</POST>
<POST>
<POSTER> &quot;Lorne&quot; &lt;lorne_ander...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-15T19:32:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Sartaj Hans&quot; &lt;spadede ... @yahoo.com&gt; wrote in message
">

news:1189733038.580554.76680@r34g2000hsd.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?
">

I believe the EBU is planning to introduce one for UK players.

The Young Chelsea BC also has one though I am not sure if it is still in
use.  My understanding is that it worked something like this:

each player has a ranking expressed as a %.
a partnership is ranked at the average of the 2 players (so if you are 53%
and todays partner is 51% you play off 52%.
the field is ranked at the average of all players - say for tonight it is
49%.

Your partnership is expected to score 53% (ie 2% above average because you
are ranked as 52% plus 1% because the filed is below average.

If you actually score 55% that is 2% above expectation and your individual
rankings get adjusted by (I think, but not cetain) 10% of this so you now
play off 53.2% and partner is 51.2%.
</POST>
<POST>
<POSTER> dfm &lt;daniel.f.mor...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-16T00:36:00 </POSTDATE>
I have just reread the brief but interesting section on rating systems
in &quot;The Mathematics of Games&quot; by John D. Beasley (Dover, 2006). One of
its conclusions is that any &quot;reasonable&quot; rating system is good enough.
Chance fluctuations in results will always overwhelm systemic rating
inaccuracies. In this spirit, I introduce the very simple rating
system used in the lunchtime game at my workplace:

Almost every weekday we get up a single table that plays rubber bridge
for an hour without changing partners. New players start out with a
rating of 500. Suppose that today Alan and Bill play against Charles
and Dave, and their current ratings are a, b, c, and d. The predicted
net score for Alan and Bill at the end of the hour (i.e. the spread)
is (a+b)-(c+d). If they beat the spread by x (i.e. if they score (a+b)-
(c+d)+x) then their ratings are increased by 1% of x and Charles and
Dave's ratings are reduced by the same amount.

Our foursomes are drawn from a pool of half a dozen regulars, plus a
similar number of occasionals, depending on who is available at noon
on a given day. Our ratings range from about 200 to about 800. The
ratings are fairly stable, except that new players whose initial 500
rating is very wrong take a while to reach their correct level. The
top two or three players are almost always the same, and the bottom
two or three players are almost always the same. It's very easy to see
the improvement of two players who were near-total novices when they
started playing with us a few years ago. For our purposes this system
is entirely satisfactory, and keeping the results up to date in Excel
takes me maybe 30 seconds per day.

The initial 500 rating is completely arbitrary. Indeed, starting
everyone at zero might make more sense, except that people might not
like to have a negative rating. The 1% factor is also arbitrary, but
after experimenting with other choices, it seems to work fine, and
it's a nice round number.

This system could very easily be adapted for IMPs or matchpoints.
There would need to be a scaling factor. For example, at matchpoints a
difference of 100 rating points might represent a spread of 10% of a
top per board (rather than 100 total points per hour) or at IMPs a
difference of 100 rating points might represent a spread of 1 IMP per
board. Calculating this manually would be a pain, but presumably it
could be programmed easily in a club or tournament that already
produces results in electronic form. In fact, I think it may be
equivalent to the Young Chelsea system described elsewhere in this
thread by Lorne. I suspect it's also not very different from the much-
reviled Lehman system at OKbridge, though I've never really bothered
to understand how that works.
</POST>
<POST>
<POSTER> Michael Angelo Ravera &lt;marav...@prodigy.net&gt; </POSTER>
<POSTDATE> 2007-09-16T05:26:00 </POSTDATE>
On Sep 15, 4:31 am, Jürgen R. &lt;jurg ... @web.de&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On Sat, 15 Sep 2007 03:58:20 -0700, Michael Angelo Ravera

&gt; &lt;marav ... @prodigy.net&gt; wrote:
&gt; &gt;On Sep 15, 1:28 am, Jürgen R. &lt;jurg ... @web.de&gt; wrote:
&gt; &gt;&gt; On Fri, 14 Sep 2007 17:09:51 -0700, Michael Angelo Ravera

&gt; &gt;&gt; &lt;marav ... @prodigy.net&gt; wrote:
&gt; &gt;&gt; &gt;On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt; &gt;&gt; &gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; &gt;&gt; &gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt; &gt;&gt; &gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt; &gt;&gt; &gt;&gt; strength.
&gt; &gt;&gt; &gt;&gt; Has someone worked out one for bridge ?
&gt; &gt;&gt; &gt;&gt; What would its elements be ?
&gt; &gt;&gt; &gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt; &gt;&gt; &gt;&gt; competitions ?
&gt; &gt;&gt; &gt;&gt; Would it be effective ?

&gt; &gt;&gt; &gt;&gt; Also see another post : (Sample bridge rating system)

&gt; &gt;&gt; &gt;The system that I have used for our club is like this:

&gt; &gt;&gt; &gt;1 Calculate the average adjusted percentile order of finish according
&gt; &gt;&gt; &gt;to the method of scoring and movement and assume that partners and
&gt; &gt;&gt; &gt;teammates have tied with them. This gives a slight (but probably
&gt; &gt;&gt; &gt;justified) advantage to larger games. Finishing as top pair in a 5-
&gt; &gt;&gt; &gt;table two-winner game gives each first place winner approximately a
&gt; &gt;&gt; &gt;90. First out of 15 is more like 95. The average of these scores
&gt; &gt;&gt; &gt;weighted accoring to the number of competitors and length of event (we
&gt; &gt;&gt; &gt;can count boards, sessions, or get fancy about length, but I have, for
&gt; &gt;&gt; &gt;the sake of simplicity assumed that all events were of equal length)
&gt; &gt;&gt; &gt;This is the R0 of the player.

&gt; &gt;&gt; &gt;2) Calculate the raiting factor &quot;G0&quot; of each event to be 1/50 of the
&gt; &gt;&gt; &gt;average R0 of the players in the game. This will give you a rating
&gt; &gt;&gt; &gt;factor for each event where 1.00 is the average. We may choose to
&gt; &gt;&gt; &gt;avoid figuring the average based upon players without meaningful
&gt; &gt;&gt; &gt;ratings as determined in step 4.

&gt; &gt;&gt; &gt;3) Now go back an rerate each player by taking the weighted average of
&gt; &gt;&gt; &gt;R0 multiplied by the G0 event rating. This is the player's R1.

&gt; &gt;&gt; Doesn't this have the defect that the winner may lose rating points?

&gt; &gt;They won't lose rating points due specifically to step 3. If your
&gt; &gt;comment is that someone who did nothing in his first events used to
&gt; &gt;establish is meaningful rating except finish first in 1.60 rated 100
&gt; &gt;table events (of which there are not many), then I guess it is
&gt; &gt;possible to lose rating points by playing in a small game with a 0.80
&gt; &gt;rating, but I haven't even heard of anyone who finshes first in every
&gt; &gt;event in which they have ever played and played in enough events to
&gt; &gt;get a meaningful rating and only playing in large events and then
&gt; &gt;wanting to play in a very small low rated game.

&gt; &gt;Average percentile order of finish has the desirable characteristic
&gt; &gt;that average is really average. Finishing third out of 5 and 8th out
&gt; &gt;of 15 are treated the same. It and all of the iterations are going to
&gt; &gt;be approximately normally distributed with 50 as a mean.

&gt; &gt;As I recall, the meaningful R1s in our club (last time I calculated
&gt; &gt;them about 3 years ago) ranged approximately from 27 to 72 or
&gt; &gt;thereabouts (about 150 people with meaningful ratings).

&gt; &gt;The monthly average G1 ratings of our club games ranged from about
&gt; &gt;0.80 (limited to non-Life masters) to about 1.20 (the Game in which
&gt; &gt;Rose Meltzer, Ed Barlow, and Jim Hayashi [as well as a few other
&gt; &gt;assorted flighted national champions like Stephen Tu whose names you
&gt; &gt;might not recognize] regularly played at the time).

&gt; &gt;&gt; &gt;4) Discard (or don't publish) the rating of any player who hasn't
&gt; &gt;&gt; &gt;played in enough events to obtain a meaningful rating. For our club, I
&gt; &gt;&gt; &gt;decided that this was 6 events for the month. This can be increased
&gt; &gt;&gt; &gt;for ratings over longer periods.

&gt; &gt;&gt; &gt;5) R1s are only meaningful for a period of time. If long term ratings
&gt; &gt;&gt; &gt;are published, &quot;Last 10&quot;, &quot;Last 25&quot;, &quot;Last 100&quot; events, and &quot;Career&quot;
&gt; &gt;&gt; &gt;would make sense.

&gt; &gt;&gt; &gt;6) You can, if you are interested, work out an R2 based upon the
&gt; &gt;&gt; &gt;rerating of events (a G1) based upon players' R1s, but it probably
&gt; &gt;&gt; &gt;won't change the order of ratings very much.

&gt; &gt;&gt; &gt;Players who never play in any other configuration such as always
&gt; &gt;&gt; &gt;playing with the same partner and only playing with the same partner
&gt; &gt;&gt; &gt;who never plays with anyone else (or always only in team events with
&gt; &gt;&gt; &gt;exactly the same teammates) will be rated identically. Players who
&gt; &gt;&gt; &gt;play enough in different configurations (or who play with partners or
&gt; &gt;&gt; &gt;teammates who play in other configurations)  will sort themselves out.

&gt; &gt;&gt; &gt;If you are further interested, you can do some Bayesian analysis for
&gt; &gt;&gt; &gt;partnerships with infrequent (but sufficient) outside plays or where
&gt; &gt;&gt; &gt;one partner never plays with anyone else, but that's a little more
&gt; &gt;&gt; &gt;complicated.

&gt; &gt;&gt; &gt;The beauty of this exact system is that a player who gets a 50 is an
&gt; &gt;&gt; &gt;average player. A player who can't ever stay out last place even in a
&gt; &gt;&gt; &gt;low rated game would be rated very low. A player who would be expected
&gt; &gt;&gt; &gt;to win EVERY TIME in an average field would be rated in excess of 100
&gt; &gt;&gt; &gt;(although his R0 would only asymptotically approach 100).

&gt; &gt;&gt; Undoubtedly this kind of scheme is sufficient for a club. It has the
&gt; &gt;&gt; advantage of being transparent.
&gt; &gt;&gt; In a wider context you would need to justify the weights in your
&gt; &gt;&gt; averages (because presumably you would be interested in accuracy) and
&gt; &gt;&gt; the right procedure is not simpy arithmetical.

&gt; &gt;I'd love to iterate with GNs and RNs until there either there are no
&gt; &gt;changes of positions worldwide or groups start to thrash. It would
&gt; &gt;also be more accurate to exclude the R of the player himself from the
&gt; &gt;weight the G of the event for that player. I suspect that interation
&gt; &gt;beyond one or two steps will not materially alter either the order of
&gt; &gt;the player ratings nor with it materially alter the actual value of
&gt; &gt;the ratings. My sense of the problem space is that after 3 iterations,
&gt; &gt;you probably have 6 digits of accuracy.

&gt; You are joking, I hope. What do you think you are actually
&gt; 'measuring'? What do you think a rating value is? What should it be?
">

The rating should asymptoticially approach the expected percentile
order of finish in an average game. If you scaled the 1200 &quot;average&quot;
rating to my 50, you'd find that the top players in the world would
end up with a rating of about 120-130. I expect that the same would
happen in Bridge. In Chess, a player with a rating that is 350 points
higher has only about 1 chance in 16 of losing a game. I suspect that
under my system, a rating increase of 35 points means that your
average order of finish is closer to first than to second in 15-table
section.
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-16T08:02:00 </POSTDATE>
[...]

<QUOTE PREVIOUSPOST="
&gt;The rating should asymptoticially approach the expected percentile
&gt;order of finish in an average game.
">

This doesn't in any way follow from your description.

<QUOTE PREVIOUSPOST="
&gt; If you scaled the 1200 &quot;average&quot;
&gt;rating to my 50, you'd find that the top players in the world would
&gt;end up with a rating of about 120-130. I expect that the same would
&gt;happen in Bridge. In Chess, a player with a rating that is 350 points
&gt;higher has only about 1 chance in 16 of losing a game. I suspect
">

You won't get very far arguing for a system based on your suspicions.
Some kowledge of probability theory might help.
The scale of the Elo system in chess is determined by setting the
standard deviation to 200 points. To apply analogous reasoning to your
system you would need to know the variance of the thing you are
measuring. The variance of results in bridge is vastly larger than in
chess, which is one reason why I thought your 6-decimal accuracy was
meant as a joke.

<QUOTE PREVIOUSPOST="
&gt; that
&gt;under my system, a rating increase of 35 points means that your
&gt;average order of finish is closer to first than to second in 15-table
&gt;section.
">
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-16T08:35:00 </POSTDATE>
On Sun, 16 Sep 2007 04:36:08 -0000, dfm &lt;daniel.f.mor ... @gmail.com&gt;
wrote:

<QUOTE PREVIOUSPOST="
&gt;I have just reread the brief but interesting section on rating systems
&gt;in &quot;The Mathematics of Games&quot; by John D. Beasley (Dover, 2006). One of
&gt;its conclusions is that any &quot;reasonable&quot; rating system is good enough.
">

No it isn't. His claim is that any reasonable distribution function
will do, since the error thus introduced is small relative to other
sources of inaccuracy.

The reason why one needs a statistically sound rating system - if one
needs a rating system at all - is in order to measure differences in
expected performance consistently accross to whole scale. The purpose
isn't to determine playing strength, which is a ficticious quantity
analogous to IQ, with enormous accuracy.
</POST>
<POST>
<POSTER> dfm &lt;daniel.f.mor...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-16T11:21:00 </POSTDATE>
On Sep 16, 7:35 am, Jürgen R. &lt;jurg ... @web.de&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; On Sun, 16 Sep 2007 04:36:08 -0000, dfm &lt;daniel.f.mor ... @gmail.com&gt;
&gt; wrote:

&gt; &gt;I have just reread the brief but interesting section on rating systems
&gt; &gt;in &quot;The Mathematics of Games&quot; by John D. Beasley (Dover, 2006). One of
&gt; &gt;its conclusions is that any &quot;reasonable&quot; rating system is good enough.

&gt; No it isn't. His claim is that any reasonable distribution function
&gt; will do, since the error thus introduced is small relative to other
&gt; sources of inaccuracy.
">

OK. I was trying to paraphrase and summarize.

<QUOTE PREVIOUSPOST="
&gt; The reason why one needs a statistically sound rating system - if one
&gt; needs a rating system at all - is in order to measure differences in
&gt; expected performance consistently accross to whole scale.
">

The reason I personally would like a rating system is to tell whether
Player A is a lot better than Player B, or better but not by a lot, or
about the same, or somewhat worse, or a lot worse. Just that would be
a good start. I'd like to see a ranking of the top 100 players, and
I'd like to see a ranking of the players at my club, but I already
know that Meckwell can kick my ass, and I have no interest in
quantifying that. And I very much doubt that it means anything to ask
whether the difference between the #1 player and the #100 player is
more or less than the difference between Joe and Bill at the club.

<QUOTE PREVIOUSPOST="
&gt; The purpose
&gt; isn't to determine playing strength, which is a ficticious quantity
&gt; analogous to IQ, with enormous accuracy.
">

Can you explain your distinction between expected performance and
playing strength?

I believe that to first order in expected performance or differences
in playing strength or whatever else you want to call it, the
simplistic system I described is equivalent to any other. IMHO, it is
worrying about higher orders when there's currently no rating system
at all that is likely to be unsound and overprecise. Unless, of
course, someone knows a lot more than I imagine about the statistics
of bridge performance. (If someone does, please enlighten me.)
</POST>
<POST>
<POSTER> moja...@mojaveg.lsan.mdsg-pacwest.com (Everett M. Greene) </POSTER>
<POSTDATE> 2007-09-17T13:28:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
dfm &lt;daniel.f.mor ... @gmail.com&gt; writes:
&gt; On Sep 16, 7:35 am, J=FCrgen R. &lt;jurg ... @web.de&gt; wrote:
&gt; &gt; dfm &lt;daniel.f.mor ... @gmail.com&gt; wrote:

&gt; &gt; &gt;I have just reread the brief but interesting section on rating systems
&gt; &gt; &gt;in &quot;The Mathematics of Games&quot; by John D. Beasley (Dover, 2006). One of
&gt; &gt; &gt;its conclusions is that any &quot;reasonable&quot; rating system is good enough.

&gt; &gt; No it isn't. His claim is that any reasonable distribution function
&gt; &gt; will do, since the error thus introduced is small relative to other
&gt; &gt; sources of inaccuracy.

&gt; OK. I was trying to paraphrase and summarize.

&gt; &gt; The reason why one needs a statistically sound rating system - if one
&gt; &gt; needs a rating system at all - is in order to measure differences in
&gt; &gt; expected performance consistently accross to whole scale.

&gt; The reason I personally would like a rating system is to tell whether
&gt; Player A is a lot better than Player B, or better but not by a lot, or
&gt; about the same, or somewhat worse, or a lot worse. Just that would be
&gt; a good start. I'd like to see a ranking of the top 100 players, and
&gt; I'd like to see a ranking of the players at my club, but I already
&gt; know that Meckwell can kick my ass, and I have no interest in
&gt; quantifying that. And I very much doubt that it means anything to ask
&gt; whether the difference between the #1 player and the #100 player is
&gt; more or less than the difference between Joe and Bill at the club.

&gt; &gt; The purpose
&gt; &gt; isn't to determine playing strength, which is a ficticious quantity
&gt; &gt; analogous to IQ, with enormous accuracy.

&gt; Can you explain your distinction between expected performance and
&gt; playing strength?

&gt; I believe that to first order in expected performance or differences
&gt; in playing strength or whatever else you want to call it, the
&gt; simplistic system I described is equivalent to any other. IMHO, it is
&gt; worrying about higher orders when there's currently no rating system
&gt; at all that is likely to be unsound and overprecise. Unless, of
&gt; course, someone knows a lot more than I imagine about the statistics
&gt; of bridge performance. (If someone does, please enlighten me.)
">

Paraphrasing your statements, you are looking for a rating system that
is good enough and it need not be highly accurate to umpteen decimal
places.
</POST>
<POST>
<POSTER> ewleong...@hotmail.com </POSTER>
<POSTDATE> 2007-09-20T03:55:00 </POSTDATE>
On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt; and (internet) Backgammon have well established, time tested rating
&gt; systems that are reasonable predictors of (approximate) playing
&gt; strength.
&gt; Has someone worked out one for bridge ?
&gt; What would its elements be ?
&gt; How do we address the peculiar considerations due to nature of bridge
&gt; competitions ?
&gt; Would it be effective ?

&gt; Also see another post : (Sample bridge rating system)
">

You are trying to objectively extrapolate and individual rating for
players playing as a pair or on a team which I think is hopeless. A
partnership of players outside of the top fifty individual rankings
could easily be well within the top fifty partnerships. Also, a
partnership of players within the top fifty individual rankings could
easily be well outside the top fifty partnerships. Also, a partnership
could well play just with each other but the the partner could be
uneveningly matched. But if they mainly just play with each other how
can you be objective? If a sponsor plays with a pro how do you know if
the sponsor isn't better.  If you really want to find the top fifty
players, you are simply going to have to create individual events
where each player plays with a many other partners. It is a little
like finding the best money bridge player at the club. It is the
player with the highest dollar winnings after a certain period.

Eric Leong
</POST>
<POST>
<POSTER> Jürgen R. &lt;jurg...@web.de&gt; </POSTER>
<POSTDATE> 2007-09-20T04:59:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
On Thu, 20 Sep 2007 00:55:30 -0700, ewleong ... @hotmail.com wrote:
&gt;On Sep 13, 6:23 pm, Sartaj Hans &lt;spadede ... @yahoo.com&gt; wrote:
&gt;&gt; Interested in some thoughts about a Bridge Rating System. Both Chess
&gt;&gt; and (internet) Backgammon have well established, time tested rating
&gt;&gt; systems that are reasonable predictors of (approximate) playing
&gt;&gt; strength.
&gt;&gt; Has someone worked out one for bridge ?
&gt;&gt; What would its elements be ?
&gt;&gt; How do we address the peculiar considerations due to nature of bridge
&gt;&gt; competitions ?
&gt;&gt; Would it be effective ?

&gt;&gt; Also see another post : (Sample bridge rating system)

&gt;You are trying to objectively extrapolate and individual rating for
&gt;players playing as a pair or on a team which I think is hopeless. A
&gt;partnership of players outside of the top fifty individual rankings
&gt;could easily be well within the top fifty partnerships. Also, a
&gt;partnership of players within the top fifty individual rankings could
&gt;easily be well outside the top fifty partnerships. Also, a partnership
&gt;could well play just with each other but the the partner could be
&gt;uneveningly matched. But if they mainly just play with each other how
&gt;can you be objective? If a sponsor plays with a pro how do you know if
&gt;the sponsor isn't better.  If you really want to find the top fifty
&gt;players, you are simply going to have to create individual events
&gt;where each player plays with a many other partners. It is a little
&gt;like finding the best money bridge player at the club. It is the
&gt;player with the highest dollar winnings after a certain period.

&gt;Eric Leong
">

A rating system would necessarily rate partnerships, not individuals,
because this is the only information available.

Individual ratings are logically questionable if the players don't mix
sufficiently. They cannot be anything but (some kind of) average of
partnership performance.

However, if ratings are used mainly for the purpose of improving
tournament conditions (i.e. forming sections according to strength)
then individual ratings matter only when a new partnerships needs to
be seeded.

Obviously partnership performance can depend strongly upon factors not
represented by performance ratings.
</POST>
</TEXT>
</BODY>
</DOC>
