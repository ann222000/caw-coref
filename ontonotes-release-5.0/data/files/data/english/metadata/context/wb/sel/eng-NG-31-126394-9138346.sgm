<DOC>
<DOCID> eng-NG-31-126394-9138346 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2007-09-26T07:26:00 </DATETIME>
<BODY>
<HEADLINE>
Urgent!!! UPGRADE METHODOLOGY
</HEADLINE>
<TEXT>
<POST>
<POSTER> dondora &lt;koni...@hanmail.net&gt; </POSTER>
<POSTDATE> 2007-09-26T07:26:00 </POSTDATE>
Hey. Hi~!

I created a beauty salon management program for a small project.
It has a few funtions but centainly necessary.
The 2nd semeter began already. I'm given a new project class
and I decided just to enhance my former program.

While I was making the program,
I comformed to the design procedure which is widely used in practice.
I drew up requirement specifications and drew actor-class diagram, use
case and related diagrams.
And then I extracted candidate classes and drew up data dictionary and
blah blah sequence, class diagrams also.

<QUOTE PREVIOUSPOST="
&gt;From beginning, I was thinking It'd be enough for me to comform to the
">

early procedure.
But now I wonder and am confused after look over the former
requirement specification.
I think to upgrade a program there will be a scheme or methodology
applied generally over software development field.

To sum up, is there a scheme or methodology to upgrade programs?
If It is, Could you let me know where I can learn it in the internet
or a book?
</POST>
<POST>
<POSTER> Erik Wikström &lt;Erik-wikst...@telia.com&gt; </POSTER>
<POSTDATE> 2007-09-26T08:23:00 </POSTDATE>
On 2007-09-26 13:26, dondora wrote:

<QUOTE PREVIOUSPOST="
&gt; Hey. Hi~!

&gt; I created a beauty salon management program for a small project.
&gt; It has a few funtions but centainly necessary.
&gt; The 2nd semeter began already. I'm given a new project class
&gt; and I decided just to enhance my former program.

&gt; While I was making the program,
&gt; I comformed to the design procedure which is widely used in practice.
&gt; I drew up requirement specifications and drew actor-class diagram, use
&gt; case and related diagrams.
&gt; And then I extracted candidate classes and drew up data dictionary and
&gt; blah blah sequence, class diagrams also.

&gt;&gt;From beginning, I was thinking It'd be enough for me to comform to the
&gt; early procedure.
&gt; But now I wonder and am confused after look over the former
&gt; requirement specification.
&gt; I think to upgrade a program there will be a scheme or methodology
&gt; applied generally over software development field.

&gt; To sum up, is there a scheme or methodology to upgrade programs?
&gt; If It is, Could you let me know where I can learn it in the internet
&gt; or a book?
">

This question is better answered in a general programming group like
comp.programming or even better a software engineering group like
comp.software-eng or comp.softwareeng, or perhaps a group discussing
object oriented programming such as comp.object.

--
Erik Wikström
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-26T09:07:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
dondora wrote:
&gt; While I was making the program,
&gt; I comformed to the design procedure which is widely used in practice.
&gt; I drew up requirement specifications and drew actor-class diagram, use
&gt; case and related diagrams.
&gt; And then I extracted candidate classes and drew up data dictionary and
&gt; blah blah sequence, class diagrams also.
">

I don't know what the professors have told you, but that's not a primary
development &quot;methodology&quot;. Modeling is just a way to visual a proposed or
existing design; it's not a complete system to verify that design.

One primary development methodology that is deceptively simple but extremely
effective is Test Driven Development, with Refactoring.

That means, for each line of code you intend to write, you first write a
simple test case that fails because the line is not there yet. This is not a
&quot;unit test&quot; or a &quot;QA test&quot; - it's just a test that can fail for the correct
reason - the line is not there yet. You run the test and successfully
predict it will fail, before you upgrade the tested code.

When you pass the test, you write whatever sloppy bad design you need. It
will only be a few edits-worth of sloppy code, so it's safe. When all the
tests pass, only then do you upgrade the design. You try to see how many
lines of code you can delete, and how you can simplify the design. You
should only make small edits and pass all the tests after each one.

If at any time the tests fail unexpectedly, you should revert and try again.
People using that system always report these benefits:

- almost no debugging
- simple clear designs
- no bugs released to production
- your project velocity does not decay over time
- you can deploy to production daily

&quot;Project velocity&quot; is the average time required to implement one feature.

This system has a lot of mindshare among our industry's leading
consultants - the people whose job is rescuing huge projects from years of
junior programmers attempting to over-design everything the way their
professors told them to.

--
Phlip
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-27T03:18:00 </POSTDATE>
On Sep 26, 1:26 pm, dondora &lt;koni ... @hanmail.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; I created a beauty salon management program for a small project.
&gt; It has a few funtions but centainly necessary.
&gt; The 2nd semeter began already. I'm given a new project class
&gt; and I decided just to enhance my former program.
&gt; While I was making the program,
&gt; I comformed to the design procedure which is widely used in practice.
&gt; I drew up requirement specifications and drew actor-class diagram, use
&gt; case and related diagrams.
&gt; And then I extracted candidate classes and drew up data dictionary and
&gt; blah blah sequence, class diagrams also.
&gt; From beginning, I was thinking It'd be enough for me to comform to the
&gt; early procedure.
&gt; But now I wonder and am confused after look over the former
&gt; requirement specification.
&gt; I think to upgrade a program there will be a scheme or methodology
&gt; applied generally over software development field.
&gt; To sum up, is there a scheme or methodology to upgrade programs?
&gt; If It is, Could you let me know where I can learn it in the internet
&gt; or a book?
">

There is no one simple answer; it depends on the type of
upgrade.  The important thing, always, is simply not to cut
corners; if the upgrade requires modifications in the design,
you modify the design; you don't just hack the code.  There's
even something to be said for rethinking the design each time
(at least whenever there's a major upgrade), refactoring common
parts again (since the upgrade may end up creating additional
common parts, or require that previously common parts behave
differently).  Only once you're sure that the design for the new
requirements is correct should you start to look at the code;
typically, you will find a lot of the existing code which you
can reuse, but that should be because it fulfills the new design
requirements, and not because you've forced the design in such a
way as to reuse it.

Failure to do this will lead very quickly to a &quot;hacked&quot; design
and unmaintainable code.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-27T04:14:00 </POSTDATE>
On Sep 26, 3:07 pm, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; dondora wrote:
&gt; &gt; While I was making the program,
&gt; &gt; I comformed to the design procedure which is widely used in practice.
&gt; &gt; I drew up requirement specifications and drew actor-class diagram, use
&gt; &gt; case and related diagrams.
&gt; &gt; And then I extracted candidate classes and drew up data dictionary and
&gt; &gt; blah blah sequence, class diagrams also.
&gt; I don't know what the professors have told you, but that's not
&gt; a primary development &quot;methodology&quot;. Modeling is just a way to
&gt; visual a proposed or existing design; it's not a complete
&gt; system to verify that design.
">

Obviously, design is a creative activity, which takes place
first in the designer's head.  However, in a very real sense,
there is no design until it's on paper (or &quot;electronic&quot; paper,
written down in the computer somewhere).  UML is probably the
most widespread way of doing this, at least for larger projects.

(And just as obviously, until the design has been written down
somehow, it's impossible to verify it.)

<QUOTE PREVIOUSPOST="
&gt; One primary development methodology that is deceptively simple
&gt; but extremely effective is Test Driven Development, with
&gt; Refactoring.
">

No professional would make such a silly statement.  There's no
silver bullet.  Testing doesn't drive design; in some cases, you
can't even know what to test until part of the design has been
specified.  (Don't get me wrong: testing is important, to catch
out the cases where you've done something else wrong.  But
anytime a test fails, the first thing you do is revisit your
process, to see what you did wrong upstream.)

Don't put the cart before the horse.

<QUOTE PREVIOUSPOST="
&gt; That means, for each line of code you intend to write, you
&gt; first write a simple test case that fails because the line is
&gt; not there yet.
">

The order is irrelevant.  The important thing is that before you
write any line of code (test or not), you have some sort of
design.

<QUOTE PREVIOUSPOST="
&gt; This is not a &quot;unit test&quot; or a &quot;QA test&quot; - it's just a test
&gt; that can fail for the correct reason - the line is not there
&gt; yet. You run the test and successfully predict it will fail,
&gt; before you upgrade the tested code.
">

Running a test that you know will fail, because you've not
written the code yet, is just a waste of time.

<QUOTE PREVIOUSPOST="
&gt; When you pass the test, you write whatever sloppy bad design
&gt; you need. It will only be a few edits-worth of sloppy code, so
&gt; it's safe. When all the tests pass, only then do you upgrade
&gt; the design. You try to see how many lines of code you can
&gt; delete, and how you can simplify the design. You should only
&gt; make small edits and pass all the tests after each one.
&gt; If at any time the tests fail unexpectedly, you should revert
&gt; and try again.  People using that system always report these
&gt; benefits:
&gt;  - almost no debugging
&gt;  - simple clear designs
&gt;  - no bugs released to production
&gt;  - your project velocity does not decay over time
&gt;  - you can deploy to production daily
">

People who use that system don't produce high quality code,
which can be used reliably in large systems.

<QUOTE PREVIOUSPOST="
&gt; &quot;Project velocity&quot; is the average time required to implement one feature.
">

An application is more than just a collection of features.

<QUOTE PREVIOUSPOST="
&gt; This system has a lot of mindshare among our industry's leading
&gt; consultants
">

You mean you and a couple of other amateurs who aren't involved
in serious software?  I don't know of any serious specialist in
software engineering who recommends anything so silly.

<QUOTE PREVIOUSPOST="
&gt; - the people whose job is rescuing huge projects from years of
&gt; junior programmers attempting to over-design everything the way their
&gt; professors told them to.
">

Does it occur to you that in well run companies, the design
isn't done by junior programmers, but by professionals, applying
professional methodology (which includes modeling, and a lot of
other things).  You might want to take a look at
http://www.sei.cmu.edu/ , for example (which is the ultimate
reference for software engineering issues);
http://www.idinews.com also has some good articles about
software development.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> Ian Collins &lt;ian-n...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-27T04:32:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; On Sep 26, 3:07 pm, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

&gt;&gt; One primary development methodology that is deceptively simple
&gt;&gt; but extremely effective is Test Driven Development, with
&gt;&gt; Refactoring.

&gt; No professional would make such a silly statement.
">

I would, and so would any member of my team.

<QUOTE PREVIOUSPOST="
&gt; There's no silver bullet.
">

No one disputes that.

<QUOTE PREVIOUSPOST="
&gt; Testing doesn't drive design; in some cases, you
&gt; can't even know what to test until part of the design has been
&gt; specified.
">

We probably work in different worlds, my clients often either don't
really know what they want or they are chasing a rapidly evolving
market, so at the beginning of a project, there is little, if anything
to design.  I'm sure there are domains where the requirements are an
invariant and a well though out design is a good approach.  One of these
days I might get to work on one!

<QUOTE PREVIOUSPOST="
&gt; Don't put the cart before the horse.
">

Writing tests before the code is both more enjoyable and leads to
better, more thorough tests.  Developers hate going back to write tests
for exiting code and tend to do a piss poor job when they do.

--
Ian Collins.
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T04:40:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; Obviously, design is a creative activity, which takes place first in the
&gt; designer's head.  However, in a very real sense, there is no design until
&gt; it's on paper
">

You have a typo there. You were clearly trying to write &quot;there is no design
until it's in code&quot;.

--
Phlip
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T04:49:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ian Collins wrote:
&gt;&gt;&gt; One primary development methodology that is deceptively simple
&gt;&gt;&gt; but extremely effective is Test Driven Development, with
&gt;&gt;&gt; Refactoring.

&gt;&gt; No professional would make such a silly statement.

&gt; I would, and so would any member of my team.
">

One of the hardest sells in TDD is to people who understand and practice
high-end automated QA testing. For example, one of them read &quot;write a simple
test case, first&quot;, and then got mired in writing a complex QA test case,
first.

Don't do that! It's not what we are talking about...

--
Phlip
</POST>
<POST>
<POSTER> Erik Wikström &lt;Erik-wikst...@telia.com&gt; </POSTER>
<POSTDATE> 2007-09-27T05:54:00 </POSTDATE>
On 2007-09-27 10:40, Phlip wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:

&gt;&gt; Obviously, design is a creative activity, which takes place first in the
&gt;&gt; designer's head.  However, in a very real sense, there is no design until
&gt;&gt; it's on paper

&gt; You have a typo there. You were clearly trying to write &quot;there is no design
&gt; until it's in code&quot;.
">

No, you have to make a difference between design and implementation, a
design is at a higher abstraction layer. One design can result in
several different (though quite similar) implementations in different
languages. An implementation on the other hand maps only to one design.
Of course, one should not assume that the design will not have to be
adjusted while implementing, since implementation can bring into light
issues that were not considered during the initial design.

--
Erik Wikström
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-27T08:21:00 </POSTDATE>
On Sep 27, 10:32 am, Ian Collins &lt;ian-n ... @hotmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:
&gt; &gt; On Sep 26, 3:07 pm, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:
&gt; &gt;&gt; One primary development methodology that is deceptively simple
&gt; &gt;&gt; but extremely effective is Test Driven Development, with
&gt; &gt;&gt; Refactoring.
&gt; &gt; No professional would make such a silly statement.
&gt; I would, and so would any member of my team.
&gt; &gt; There's no silver bullet.
&gt; No one disputes that.
">

That's apparently what Philip was claiming.  Use TDD, and you
don't need anything else.  Testing is an essential part of
software development, but it isn't everything.

<QUOTE PREVIOUSPOST="
&gt; &gt; Testing doesn't drive design; in some cases, you
&gt; &gt; can't even know what to test until part of the design has been
&gt; &gt; specified.
&gt; We probably work in different worlds, my clients often either
&gt; don't really know what they want or they are chasing a rapidly
&gt; evolving market, so at the beginning of a project, there is
&gt; little, if anything to design.
">

If you don't know what the project is supposed to do, you can't
very well design it.  Of course, you can't code it either, and
above all, you can't write tests to verify that it does it.

Requirements do evolve.  The user's idea of his requirements
also may become more precise as time goes on.  But that doesn't
mean you don't design---just the opposite: you intentionally
design in flexibility where you know things are going to change.
And you rework the design each time his requirements evolve.

(One frequent problem, of course, is that the user wants
flexible requirements, but a fixed price.  But that just doesn't
work; you can't fix a price without knowing the actual
requirements.  And of course, if the user then changes them, you
re-estimate, and fix a new price.  After which, he either
forgoes the changes, or accepts the new price.)

<QUOTE PREVIOUSPOST="
&gt; I'm sure there are domains where the requirements are an
&gt; invariant and a well though out design is a good approach.
&gt; One of these days I might get to work on one!
">

I don't think it's that black and white.  Every project I've
ever seen or heard of has some fixed requirements (it shouldn't
crash, regardless of the input), and some that evolve.  A well
thought out design isn't cast in stone; it will evolve, just as
anything else will.  (A well thought out design may help in
estimating the cost of a given evolution, of course.)

<QUOTE PREVIOUSPOST="
&gt; &gt; Don't put the cart before the horse.
&gt; Writing tests before the code is both more enjoyable and leads
&gt; to better, more thorough tests.
">

If you find that to be true, do so.  I prefer the inverse, but
with regards to implemention code and tests (which are, of
course, also code), the order is really irrelevant, and each
developer can do whatever he feels like.  The question is rather
one of design vs. code/tests: without the design, how do you
know what classes will even be needed.

<QUOTE PREVIOUSPOST="
&gt; Developers hate going back to write tests for exiting code and
&gt; tend to do a piss poor job when they do.
">

And then their code doesn't pass review (which, of course,
includes the unit tests, and validates their completeness).

In this regard, you might care to read
http://www.idinews.com/agileDoc.html .  It doesn't cover the
question of TDD so much as basic professionalism; a professional
doesn't &quot;do a piss poor job&quot; just because he doesn't consider
something very interesting.  And if he does, then that's a
problem, regardless of the methodology being used.

Having said that, of course, I would repeat what I said before.
Both the code and the tests must be written, regardless.  And
the order isn't really that important; the code isn't finished
until it passes the tests.  If you find it more agreeable to
write the tests first, and then the code, I don't see where that
would cause any problem.  I generally do the reverse, but that's
just because that's the way I feel most comfortable.  And in
practice, it's probably mixed for both of us: I'll write the
constructors, and the accessor functions used in the tests, and
test them, then generally add one or two functions at a time,
with their tests, until the class is complete.  But whichever
way you do it, you have to know what to write, which means that
you have to know what the class is supposed to do, which means
that you must know the requirements.  (In just about every place
I've worked, too, you have to make an estimation of cost and
time before you begin writing the code.  On the basis of the
stated requirements, of course.)

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-27T08:24:00 </POSTDATE>
On Sep 27, 10:40 am, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:
&gt; &gt; Obviously, design is a creative activity, which takes place
&gt; &gt; first in the designer's head.  However, in a very real
&gt; &gt; sense, there is no design until it's on paper
&gt; You have a typo there. You were clearly trying to write &quot;there
&gt; is no design until it's in code&quot;.
">

There's no implementation of the design until the product has
been deployed.  And you can't really be certain that your design
was correct until then.  But you can't write a line of code
until you know what classes are going to be there, and
determining that is part of design.  Until you have documented
the class interactions, for example, you don't know what you're
going to have to test.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T10:20:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; That's apparently what Philip was claiming.  Use TDD, and you don't need
&gt; anything else.  Testing is an essential part of software development, but
&gt; it isn't everything.
">

TDD is not testing, it's just writing the code twice, once backwards. It
works within a methodology that's extraordinarily effective. Nobody said you
only need testing.

If you want a &quot;silver bullet&quot;, I think the definition was &quot;a new technique
providing an order of magnitude improvement in less than a decade of normal
growth&quot;. So if a high-end shop that was already using whatever methodology
you advocate switches to TDD (all the practices, not just the test-like
things), and if they report a 10x drop in bugs reported from their
customers, that might just qualify...

<QUOTE PREVIOUSPOST="
&gt; Requirements do evolve.  The user's idea of his requirements also may
&gt; become more precise as time goes on.  But that doesn't mean you don't
&gt; design---just the opposite: you intentionally design in flexibility where
&gt; you know things are going to change.
">

Given feature X, you implement it with simple code. Even though X2 and X3
are very near term, you _don't_ design-ahead for them. When you have X2, you
add it (under matching test-like things), and you refactor the code until
its simple again. When X3 comes along - the third requirement along the same
kind of feature - you probably won't need to refactor very much.

<QUOTE PREVIOUSPOST="
&gt; And you rework the design each time his requirements evolve.
">

If you order him to provide new, narrow requirements, once per week, then
you improve his ability to steer a project in realtime. This allows you to
right-size a program, without adding to many speculative features.

<QUOTE PREVIOUSPOST="
&gt; I generally do the reverse, but that's
">

just because that's the way I feel most comfortable.

People using test-first routinely report surprise at how soon a design locks
into the &quot;Open Closed Principle&quot;, and stops changing. This implies they are
rapidly under-engineering a project, instead of over-engineering it.

--
Phlip
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T10:22:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; There's no implementation of the design until the product has been
&gt; deployed.  And you can't really be certain that your design was correct
&gt; until then.  But you can't write a line of code until you know what
&gt; classes are going to be there
">

I don't have that problem. I go test -&gt; behavior -&gt; little method. Over
time, that method might sprout into an object, or even a module. If it
doesn't, then I right-sized the design.

--
Phlip
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T20:47:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Erik Wikström wrote:
&gt; No, you have to make a difference between design and implementation, a
&gt; design is at a higher abstraction layer. One design can result in
&gt; several different (though quite similar) implementations in different
&gt; languages. An implementation on the other hand maps only to one design.
&gt; Of course, one should not assume that the design will not have to be
&gt; adjusted while implementing, since implementation can bring into light
&gt; issues that were not considered during the initial design.
">

Paraphrasing that great methodologist, Bill Clinton, that depends on the
definition of &quot;is&quot;.

A team should invest its designing energy into code first, diagrams and
plans second. If the boss asks &quot;how's the design coming along?&quot;, the answer
should be deployed features, not speculation and documentation.

I'm aware some people don't have experience producing clean designs without
up-front planning. This thread started when a student, for a very small
project, planned its design, then accepted new requirements, and discovered
the design did not magically cover them. That's how it works; no matter how
awesome our design skills, we will always discover requirements that force
rework.

So as part of my design goals, I intend to produce code, with tests, that
are all highly resilient to change. And I get there by refactoring the
design, as it grows, and forcing it to actually change as it grows. This
tests that it can.

--
Phlip
</POST>
<POST>
<POSTER> dondora &lt;koni...@hanmail.net&gt; </POSTER>
<POSTDATE> 2007-09-27T21:33:00 </POSTDATE>
Well, my question causes a dispute.
I've determined to conform to the design methodology I
talked(requirement specifications, use casees, etc).
There's no problem with things I've done in my project as you ask me.
I just wanted to know there's systematic methodology as I did.
Anyway, TDD looks bad. When it comes to time to handing over your own
program in industry, you just give code?
Let us think the situation about when you are given just a
tremendously amount of sources without anything explained.
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-27T22:39:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
dondora wrote:
&gt; Well, my question causes a dispute.
">

Sorry it looks like that. In many circles the matter is quite settled. And
you could also try &quot;Design by Contract&quot;, to much the same effect.

The best way to do something is often the simplest, but there are always
newbies who need to be brought up to speed. The only &quot;debate&quot; here has been
whether we should write test cases just before or just after writing the
tested code. Nobody here has advocated you _not_ write automated tests.

<QUOTE PREVIOUSPOST="
&gt; I've determined to conform to the design methodology I
&gt; talked(requirement specifications, use casees, etc).
">

Again: Those are not a methodology. And if you describing doing all of them
first, before any coding, then that is &quot;Waterfall&quot;, which is among the worst
known methodologies.

<QUOTE PREVIOUSPOST="
&gt; There's no problem with things I've done in my project as you ask me.
&gt; I just wanted to know there's systematic methodology as I did.
&gt; Anyway, TDD looks bad.
">

What have you read about it? Try Steve McConnell's /Code Complete, 2nd Ed/.
And nobody has said that people using TDD never document what they are
doing. Read more.

<QUOTE PREVIOUSPOST="
&gt; When it comes to time to handing over your own
&gt; program in industry, you just give code?
">

And tests.

I want you to imagine picking one of two new jobs. This example is
contrived - the real life example is always somewhere in between - but it
illustrates the situation. At either job, your first task will be adding a
feature to 1 million lines of well-written C++ code.

At Job A, the code comes with lots of nice, accurate, reliable, indexed
requirements documents, design model diagrams, and use cases.

At Job B, the code comes with almost no documents, 1.5 million lines of
clearly written and simple test cases, and a Wiki documenting and running
test cases covering all the inputs and outputs the users expect.

Now lets see what you do at your first day at Job A. You make a change.
Then, for hours, you slowly read all that documentation, and you manually
operate the program, making sure your change did not break any of the
existing features. When you make that change, you have the odious choice to
add new code, or to change existing code. If you get this choice wrong
(likely), the design quality will go down. Further, if you make any mistake,
you will probably spend a long time debugging to figure out what went wrong.

At Job B, during and after your first change, you quickly run all the tests.
They work like little elves reading all that documentation, and applying all
those checks for you. If you break something - or even if the elves
_suspect_ you might break something - you have the option to revert your
change and try again.

You have the option to _not_ debug.

Understand the elves are not omniscient - they only know what they are told.
So did the documentation at Job A. But the elves prefer to err on the side
of caution. Many of your edits that should have worked, the test cases will
reject them!

You will work faster and safer at Job B. If a test case fails, its assertion
diagnostic should describe what went wrong. These test cases form a living
documentation, showing you what systems, structures, and behaviors the code
should reveal.

Next, each &quot;use case&quot; was expressed as a high-level test in that Wiki. This
forced the code to be testable, which overwhelmingly improved its design,
and decoupled its objects. This improves communication with your users'
representatives. No more hand-waving or white-boarding when discussing
features. You can see them in action.

Real life, of course, is not so distinct. Many projects have no tests
whatsoever (and many also have no documentation!). Well-managed projects
usually find some balance between automated tests and of _reliable_
documentation. (Tests can't lie like some documentation can!) So the
question resolves to one point: At crunch time, when the programmers are
doing something important, would you rather they devote their energy to
documentation? or to automated tests? Which one is more important for your
project's success?

--
Phlip
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-28T06:27:00 </POSTDATE>
On Sep 28, 4:39 am, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; dondora wrote:
&gt; &gt; Well, my question causes a dispute.
&gt; Sorry it looks like that. In many circles the matter is quite
&gt; settled.
">

Quite.  Take a look at the SEI site, for example.  Software
engineering is actual a fairly mature discipline, even if a lot
of developers (including some who are experts in other things,
such as software design) choose to ignore it.

<QUOTE PREVIOUSPOST="
&gt; And you could also try &quot;Design by Contract&quot;, to much the same
&gt; effect.
&gt; The best way to do something is often the simplest, but there
&gt; are always newbies who need to be brought up to speed. The
&gt; only &quot;debate&quot; here has been whether we should write test cases
&gt; just before or just after writing the tested code.
">

I'm not even sure that that's being debated; I certainly don't
think it matters (and have expressed that opinion).  My
impression was that the debate was over whether there were
phases that should precede writing tests or code: a separate
design phase.

<QUOTE PREVIOUSPOST="
&gt; Nobody here has advocated you _not_ write automated tests.
">

Very true.  NO development methodology would ever allow that.
In industry, typically, the check-in procedures for the software
will run the unit tests, and won't accept the check-in if they
fail.

<QUOTE PREVIOUSPOST="
&gt; &gt; I've determined to conform to the design methodology I
&gt; &gt; talked(requirement specifications, use casees, etc).
&gt; Again: Those are not a methodology. And if you describing
&gt; doing all of them first, before any coding, then that is
&gt; &quot;Waterfall&quot;, which is among the worst known methodologies.
">

There, you're being intellectually dishonest.  There is no such
thing as a &quot;waterfall&quot; methodology, and never has been; it's a
strawman that was invented for the sole purpose of criticising
it, and justifying some new approach.  If you don't know what
the code you want to write is supposed to do, then you can't
write either the tests or the code.  And if you haven't put it
down in writing, then you don't know it.  It's that simple.  The
&quot;requirements specification&quot; must be complete for the code you
write.  (That doesn't mean, and has never meant, that it is
complete for every aspect of the final system.  The requirements
specification may evolve, just like everything else in the
system.)

You might want to read http://www.idinews.com/waterfall.html for
more details.

<QUOTE PREVIOUSPOST="
&gt; &gt; There's no problem with things I've done in my project as
&gt; &gt; you ask me.  I just wanted to know there's systematic
&gt; &gt; methodology as I did.  Anyway, TDD looks bad.
&gt; What have you read about it? Try Steve McConnell's /Code
&gt; Complete, 2nd Ed/.  And nobody has said that people using TDD
&gt; never document what they are doing. Read more.
&gt; &gt; When it comes to time to handing over your own
&gt; &gt; program in industry, you just give code?
&gt; And tests.
">

What you hand over depends on the contract:-).  Code, tests,
documentation... Whatever the customer wants (and is willing to
pay for).  I'm sure, for example, that you provide user manuals,
if that's part of your responsibility in the project---you don't
really expect users to figure it out from the tests.

Typically, of course, you will provide a requirements
specification (at least partial) much, much earlier.  When you
specify the price.  Because most customers don't particularly
like writing blank checks: they want to know what they will get,
for what price.

<QUOTE PREVIOUSPOST="
&gt; I want you to imagine picking one of two new jobs. This
&gt; example is contrived - the real life example is always
&gt; somewhere in between - but it illustrates the situation. At
&gt; either job, your first task will be adding a feature to 1
&gt; million lines of well-written C++ code.
&gt; At Job A, the code comes with lots of nice, accurate,
&gt; reliable, indexed requirements documents, design model
&gt; diagrams, and use cases.
&gt; At Job B, the code comes with almost no documents, 1.5 million
&gt; lines of clearly written and simple test cases, and a Wiki
&gt; documenting and running test cases covering all the inputs and
&gt; outputs the users expect.
">

Again: intellectual dishonesty.  Have you ever heard of a
company that had a good enough process to produce the
documentation of job A, which didn't have automated tests as
part of the process.

<QUOTE PREVIOUSPOST="
&gt; Now lets see what you do at your first day at Job A. You make
&gt; a change.  Then, for hours, you slowly read all that
&gt; documentation, and you manually operate the program, making
&gt; sure your change did not break any of the existing features.
&gt; When you make that change, you have the odious choice to add
&gt; new code, or to change existing code. If you get this choice
&gt; wrong (likely), the design quality will go down. Further, if
&gt; you make any mistake, you will probably spend a long time
&gt; debugging to figure out what went wrong.
&gt; At Job B, during and after your first change, you quickly run
&gt; all the tests.  They work like little elves reading all that
&gt; documentation, and applying all those checks for you. If you
&gt; break something - or even if the elves _suspect_ you might
&gt; break something - you have the option to revert your change
&gt; and try again.
">

You forget the essential: if the role and the responsibilities
of the class in the project are well defined and documented (job
A), you understand what you are doing, and your code will be
correct first time.  If they're not (job B), you guess, run the
tests, they fail, guess something else, run the tests, that
fails as well, etc., until you guess right.

<QUOTE PREVIOUSPOST="
&gt; You have the option to _not_ debug.
&gt; Understand the elves are not omniscient - they only know what
&gt; they are told.  So did the documentation at Job A. But the
&gt; elves prefer to err on the side of caution. Many of your edits
&gt; that should have worked, the test cases will reject them!
&gt; You will work faster and safer at Job B.
">

Have you any real measured studies to support such a ridiculous
claim.

<QUOTE PREVIOUSPOST="
&gt; If a test case fails, its assertion diagnostic should describe
&gt; what went wrong. These test cases form a living documentation,
&gt; showing you what systems, structures, and behaviors the code
&gt; should reveal.
&gt; Next, each &quot;use case&quot; was expressed as a high-level test in
&gt; that Wiki. This forced the code to be testable, which
&gt; overwhelmingly improved its design, and decoupled its objects.
&gt; This improves communication with your users' representatives.
&gt; No more hand-waving or white-boarding when discussing
&gt; features. You can see them in action.
&gt; Real life, of course, is not so distinct. Many projects have
&gt; no tests whatsoever (and many also have no documentation!).
">

In practice, such companies went out of business a long time
ago.  At least in the fields I work (where software usually has
to run 24 hours a day, 7 days a week, with contractual penalties
for down time).

<QUOTE PREVIOUSPOST="
&gt; Well-managed projects usually find some balance between
&gt; automated tests and of _reliable_ documentation. (Tests can't
&gt; lie like some documentation can!) So the question resolves to
&gt; one point: At crunch time, when the programmers are doing
&gt; something important, would you rather they devote their energy
&gt; to documentation? or to automated tests? Which one is more
&gt; important for your project's success?
">

Unless you have both, you've failed.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> Ian Collins &lt;ian-n...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-28T07:09:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:

&gt; Typically, of course, you will provide a requirements
&gt; specification (at least partial) much, much earlier.  When you
&gt; specify the price.  Because most customers don't particularly
&gt; like writing blank checks: they want to know what they will get,
&gt; for what price.
">

The process tends to be different with agile projects, where the
customer pays by iteration, with the option to pull that pug after the
iteration is complete.  The level of required documentation is
considerably less because the risk to the client is much lower, the most
they will loose is a week or two of the suppliers time.

I've run a couple of successful projects this way, one in particular was
very successful, delivering the client exactly what they wanted, which
turned out to be considerably more than they would have specified up
front.  They got extra business value, I got several more months work.
At no time were the requirements anything more than 4x6 cards.

--
Ian Collins.
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-28T08:48:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; Quite.  Take a look at the SEI site, for example.  Software engineering is
&gt; actual a fairly mature discipline, even if a lot of developers (including
&gt; some who are experts in other things, such as software design) choose to
&gt; ignore it.
">

Fallacy of excluded middle and argumentum ad hominem in one 'graph.

--
Phlip
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-28T09:08:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; Quite.  Take a look at the SEI site, for example.
">

Okay.

http://www.sei.cmu.edu/productlines/frame_report/testing.htm

----8&lt;-----------------------------------------

TDD is one practice of the agile development community. Its goal is &quot;clean
code that works&quot; [Beck 2002a]. In this practice, developers define
requirements for the piece they are assigned to construct by maintaining
close communication with the developers who are constructing related units
and writing test cases that serve as the specification for the unit. The
developer then writes and revises code until the unit passes all the tests.
The rhythm of TDD is very short cycles of these steps:

Define a new test.
Execute all tests.
Write code to fix tests that fail.
Execute all tests.

TDD has the advantage that the test code is always in synch with the product
code, because the test code defines the product code. The disadvantage of
TDD is that there is not a good method for determining whether the set of
test cases is complete, since the completeness of a test set is usually
determined by comparing it to the specification.

TDD is applicable to product line organizations provided it is applied to
units that are first defined in the context of the product line
architecture. TDD does not provide tools and techniques for balancing the
diverse quality attributes usually present in a product line. TDD can be
successful if applied to units that a small group of developers, often a
two-person or pair programming team, can produce in a timely manner. The
range of variability for the unit should also be sufficiently narrow to
allow for timely completion. The success of TDD depends on the availability
of tools, such as JUnit, to assist with development and automate testing.

----8&lt;-----------------------------------------

ftp://ftp.sei.cmu.edu/pub/documents/articles/pdf/xp-from-a-cmm-perspe...

XP satisfaction of key process areas, given the appropriate environment

Level Satisfaction Key process area

2 ++ Requirements management
2 ++ Software project planning
2 ++ Software project tracking and oversight
2 -  Software subcontract management
2 +  Software quality assurance
2 +  Software configuration management
3 +  Organization process focus
3 +  Organization process definition
3 -  Training program
3 -  Integrated software management
3 ++ Software product engineering
3 ++ Intergroup coordination
3 ++ Peer reviews
4 -  Quantitative process management
4 -  Software quality management
5 +  Defect prevention
5 -  Technology change management
5 -  Process change management

+ Partially addressed in XP
++ Largely addressed in XP (perhaps by inference)
- Not addressed in XP

----8&lt;-----------------------------------------

Note that this survey only compares XP's documentation and verbiage to
CMMi's verbiage. It is not a study of real projects in action. So under
&quot;Training program&quot;, the - represents the author, Dr. Mark Paulk, declines to
speculate that pair programming could be used as an ideal training program.

Next, all Agile projects, in practice, automate their entire build chain.
Maybe the CMMi has higher goals for its &quot;Integrated software management&quot;
KPA.

And note that &quot;Defect prevention&quot; gets only one +. The actual response from
folks who switched to XP (and did all its practices, not just the convenient
ones) is their code grows very robust and difficult to break over time.
Agile development provides aspects of design and teamwork which the SEI is
not yet capable of interpreting.

So, in conclusion, I don't think it's the Agile community who is being
immature here.

--
Phlip
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-28T09:17:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ian Collins wrote:
&gt; The process tends to be different with agile projects, where the
&gt; customer pays by iteration, with the option to pull that pug after the
&gt; iteration is complete.  The level of required documentation is
&gt; considerably less because the risk to the client is much lower, the most
&gt; they will loose is a week or two of the suppliers time.

&gt; I've run a couple of successful projects this way, one in particular was
&gt; very successful, delivering the client exactly what they wanted, which
&gt; turned out to be considerably more than they would have specified up
&gt; front.  They got extra business value, I got several more months work.
&gt; At no time were the requirements anything more than 4x6 cards.
">

It's funny how the dominos fall. If you assume that software must grow
harder to change as it matures, then you must assume that it must be planned
up-front, designed up-front, and required up-front. This passes the buck
back to the client - they must perform the superhuman task of predicting
which set of _detailed_ requirements they will need.

Then this system devolves into a game of brinkmanship, where at delivery
time the client checks what the code actually does, and then attempts to
re-specify for the things that are now missing. If these things are in the
contract but not delivered, the client orders the programmers to work for
free, and the programmers take the hit. If these things are not in the
contract, the client takes the hit. So the client has an incentive to
over-specify the up-front contract, and the programmers have the incentive
to try to guess what's really important.

Adaptive planning and just-in-time requirements balance all that out.

--
Phlip
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-28T18:35:00 </POSTDATE>
On Sep 28, 3:08 pm, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:
&gt; &gt; Quite.  Take a look at the SEI site, for example.
&gt; http://www.sei.cmu.edu/productlines/frame_report/testing.htm
&gt; ----8&lt;-----------------------------------------
&gt; TDD is one practice of the agile development community. Its goal is &quot;clean
&gt; code that works&quot; [Beck 2002a]. In this practice, developers define
&gt; requirements for the piece they are assigned to construct by maintaining
&gt; close communication with the developers who are constructing related units
&gt; and writing test cases that serve as the specification for the unit. The
&gt; developer then writes and revises code until the unit passes all the tests.
&gt; The rhythm of TDD is very short cycles of these steps:
&gt; Define a new test.
&gt; Execute all tests.
&gt; Write code to fix tests that fail.
&gt; Execute all tests.
&gt; TDD has the advantage that the test code is always in synch with the product
&gt; code, because the test code defines the product code. The disadvantage of
&gt; TDD is that there is not a good method for determining whether the set of
&gt; test cases is complete, since the completeness of a test set is usually
&gt; determined by comparing it to the specification.
&gt; TDD is applicable to product line organizations provided it is applied to
&gt; units that are first defined in the context of the product line
&gt; architecture. TDD does not provide tools and techniques for balancing the
&gt; diverse quality attributes usually present in a product line. TDD can be
&gt; successful if applied to units that a small group of developers, often a
&gt; two-person or pair programming team, can produce in a timely manner. The
&gt; range of variability for the unit should also be sufficiently narrow to
&gt; allow for timely completion. The success of TDD depends on the availability
&gt; of tools, such as JUnit, to assist with development and automate testing.
&gt; ----8&lt;-----------------------------------------
">

Exactly.  Very, very limited in its range of applicability.

<QUOTE PREVIOUSPOST="
- Hide quoted text - - Show quoted text -
">

<QUOTE PREVIOUSPOST="
&gt; ftp://ftp.sei.cmu.edu/pub/documents/articles/pdf/xp-from-a-cmm-perspe ...
&gt; XP satisfaction of key process areas, given the appropriate environment
&gt; Level Satisfaction Key process area
&gt; 2 ++ Requirements management
&gt; 2 ++ Software project planning
&gt; 2 ++ Software project tracking and oversight
&gt; 2 -  Software subcontract management
&gt; 2 +  Software quality assurance
&gt; 2 +  Software configuration management
&gt; 3 +  Organization process focus
&gt; 3 +  Organization process definition
&gt; 3 -  Training program
&gt; 3 -  Integrated software management
&gt; 3 ++ Software product engineering
&gt; 3 ++ Intergroup coordination
&gt; 3 ++ Peer reviews
&gt; 4 -  Quantitative process management
&gt; 4 -  Software quality management
&gt; 5 +  Defect prevention
&gt; 5 -  Technology change management
&gt; 5 -  Process change management
&gt; + Partially addressed in XP
&gt; ++ Largely addressed in XP (perhaps by inference)
&gt; - Not addressed in XP
">

You'll note some very important issues that it doesn't address.
Like overall quality.  I'd also disagree about it's addressing
software project planning or software project tracking and
oversight---planning definitly involves specifications: how much
time/money to do what, and tracking and oversight pre-suppose a
plan: how can you know if you're on schedule if you don't have a
schedule.

<QUOTE PREVIOUSPOST="
&gt; ----8&lt;-----------------------------------------
&gt; Note that this survey only compares XP's documentation and
&gt; verbiage to CMMi's verbiage. It is not a study of real
&gt; projects in action. So under &quot;Training program&quot;, the -
&gt; represents the author, Dr. Mark Paulk, declines to speculate
&gt; that pair programming could be used as an ideal training
&gt; program.
">

Pair programming is a very effective *training* tool; it has
been more or less one of standard training tools for some 20 or
30 years now (although it didn't have such a catchy name in the
past).

<QUOTE PREVIOUSPOST="
&gt; Next, all Agile projects, in practice, automate their entire
&gt; build chain.  Maybe the CMMi has higher goals for its
&gt; &quot;Integrated software management&quot; KPA.
">

All projects I've worked on for the past twenty or thirty years
have automated the build chain.  That's only a small part of the
problem.  The issues of tracking progress and measuring quality
are less often addressed.

<QUOTE PREVIOUSPOST="
&gt; And note that &quot;Defect prevention&quot; gets only one +. The actual
&gt; response from folks who switched to XP (and did all its
&gt; practices, not just the convenient ones) is their code grows
&gt; very robust and difficult to break over time.
">

Do you have anything but annecdotal evidence?  What tools do you
use to measure?

On a well managed project, I would estimate that one error per
100,000 lines of code would be about the minimum acceptable
level of quality.  Up until that point, at least, reducing the
number of errors reduces cost.  (I've heard of projects with
less than one error per million lines of code.  These are the
ones, of course, where the developers don't even have access to
a compiler.  It can be done.  It is done for very critical
systems, where human life is at stake.  But I don't think it's
really cost effective otherwise.)

<QUOTE PREVIOUSPOST="
&gt; Agile development provides aspects of design and teamwork
&gt; which the SEI is not yet capable of interpreting.
">

So called &quot;agile development&quot; in fact takes us back to what was
current practive 20 or 30 years ago.  SEI is roughly a
generation beyond that.

<QUOTE PREVIOUSPOST="
&gt; So, in conclusion, I don't think it's the Agile community who
&gt; is being immature here.
">

Just a serious step backwards.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> Ian Collins &lt;ian-n...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-28T19:00:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:

&gt;&gt; So, in conclusion, I don't think it's the Agile community who
&gt;&gt; is being immature here.

&gt; Just a serious step backwards.
">

Those with the courage to have taken the step would disagree.

--
Ian Collins.
</POST>
<POST>
<POSTER> Ian Collins &lt;ian-n...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-28T19:11:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:

&gt; You'll note some very important issues that it doesn't address.
&gt; Like overall quality.  I'd also disagree about it's addressing
&gt; software project planning or software project tracking and
&gt; oversight---planning definitly involves specifications: how much
&gt; time/money to do what, and tracking and oversight pre-suppose a
&gt; plan: how can you know if you're on schedule if you don't have a
&gt; schedule.
">

You have a pile of completed stories with a measured velocity and a pile
of estimated stories.  These give you real data on progress and time
remaining, rather than speculation.

From this data, the customer (or management) knows exactly where the
development is and has real data to use to manipulate the priorities of
stories to meet specific deadlines.  The data is extremely useful in the
feature/cost juggling act.

I have found that project managers tend to start out hostile to this
approach, but come to love it once they realise they have solid
reporting and estimations to work with.

<QUOTE PREVIOUSPOST="
&gt;&gt; Note that this survey only compares XP's documentation and
&gt;&gt; verbiage to CMMi's verbiage. It is not a study of real
&gt;&gt; projects in action. So under &quot;Training program&quot;, the -
&gt;&gt; represents the author, Dr. Mark Paulk, declines to speculate
&gt;&gt; that pair programming could be used as an ideal training
&gt;&gt; program.

&gt; Pair programming is a very effective *training* tool; it has
&gt; been more or less one of standard training tools for some 20 or
&gt; 30 years now (although it didn't have such a catchy name in the
&gt; past).
">

It is also a very effective development tool, but you really have to try
it for real to understand this.

--
Ian Collins.
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-28T20:11:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ian Collins wrote:
&gt;&gt; Just a serious step backwards.

&gt; Those with the courage to have taken the step would disagree.
">

I'm dropping out; this is all just slants and innuendos. Not even a word
that James was obviously surprised to learn SEI is seriously investigating
XP, and was not sharing his drooling invective for them.

In conclusion, I quote Pierre Pachet, Professor of Physiology at Toulouse,
1872:

&quot;Louis Pasteur's theory of germs is ridiculous fiction.&quot;

No actual discourse, no alternate theory of disease exctept evil spirits,
and no washing hands before we operate on our patients.

--
Phlip
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-29T04:49:00 </POSTDATE>
On Sep 29, 1:00 am, Ian Collins &lt;ian-n ... @hotmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:
&gt; &gt;&gt; So, in conclusion, I don't think it's the Agile community who
&gt; &gt;&gt; is being immature here.
&gt; &gt; Just a serious step backwards.
&gt; Those with the courage to have taken the step would disagree.
">

It corresponds roughly to what was standard practice 20 or 30
years ago.  Fancy names have been added, but the fundamentals
haven't changed.  Some companies have moved beyond.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-29T04:58:00 </POSTDATE>
On Sep 29, 1:11 am, Ian Collins &lt;ian-n ... @hotmail.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; James Kanze wrote:
&gt; &gt; You'll note some very important issues that it doesn't address.
&gt; &gt; Like overall quality.  I'd also disagree about it's addressing
&gt; &gt; software project planning or software project tracking and
&gt; &gt; oversight---planning definitly involves specifications: how much
&gt; &gt; time/money to do what, and tracking and oversight pre-suppose a
&gt; &gt; plan: how can you know if you're on schedule if you don't have a
&gt; &gt; schedule.
&gt; You have a pile of completed stories with a measured velocity
&gt; and a pile of estimated stories.  These give you real data on
&gt; progress and time remaining, rather than speculation.
">

After a certain time.  The problem is that the customer wants to
know the price and delivery date before starting the project.

<QUOTE PREVIOUSPOST="
&gt; From this data, the customer (or management) knows exactly where the
&gt; development is and has real data to use to manipulate the priorities of
&gt; stories to meet specific deadlines.  The data is extremely useful in the
&gt; feature/cost juggling act.
&gt; I have found that project managers tend to start out hostile to this
&gt; approach, but come to love it once they realise they have solid
&gt; reporting and estimations to work with.
">

I'm not sure about project managers, but the customer or final
user generally doesn't want to be that involved.  He has a
specific need or want.  He asks how much it will cost, and when
can we deliver.  We make our estimates; he decides whether it
will fit into his budget or not, signs the order, and then moves
on---specifying or designing software is the main occupation of
his organization.

It does happen that he changes his mind about some things during
development.  The probability of this depends, obviously, on the
length of the project.  When that happens, he recontacts us, and
we negotiate the changes.  (If the project is running well, and
we have a good, modular design, we can often make small changes
for nothing.)

It also happens that during development, we discover that some
features which seem useful would be trivial to add, or that a
slight modification might make the software easier to use, and
less expensive to develop.  In that case, we contact the
customer, and make a proposition.

We don't involve him on a day to day basis, or even weekly.

<QUOTE PREVIOUSPOST="
&gt; &gt;&gt; Note that this survey only compares XP's documentation and
&gt; &gt;&gt; verbiage to CMMi's verbiage. It is not a study of real
&gt; &gt;&gt; projects in action. So under &quot;Training program&quot;, the -
&gt; &gt;&gt; represents the author, Dr. Mark Paulk, declines to speculate
&gt; &gt;&gt; that pair programming could be used as an ideal training
&gt; &gt;&gt; program.
&gt; &gt; Pair programming is a very effective *training* tool; it has
&gt; &gt; been more or less one of standard training tools for some 20 or
&gt; &gt; 30 years now (although it didn't have such a catchy name in the
&gt; &gt; past).
&gt; It is also a very effective development tool, but you really
&gt; have to try it for real to understand this.
">

My experience with it is that it isn't cost effective.  It does
improve quality and development speed somewhat, but it costs
more than it brings.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> James Kanze &lt;james.ka...@gmail.com&gt; </POSTER>
<POSTDATE> 2007-09-29T05:01:00 </POSTDATE>
On Sep 29, 2:11 am, &quot;Phlip&quot; &lt;phlip ... @yahoo.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Ian Collins wrote:
&gt; &gt;&gt; Just a serious step backwards.
&gt; &gt; Those with the courage to have taken the step would disagree.
&gt; I'm dropping out; this is all just slants and innuendos.
">

All I've seen is established facts.  Adopting the process which
was standard 20 or 30 years ago is a step backward, even if you
wrap it in a fancy new name.

<QUOTE PREVIOUSPOST="
&gt; Not even a word
&gt; that James was obviously surprised to learn SEI is seriously investigating
&gt; XP, and was not sharing his drooling invective for them.
">

I wasn't at all surprised, since I'd read articles about it from
them five or more years ago.  The SEI, in fact, investigates
pretty much everything that is proposed.

Their style is, of course, different from mine:-).  Even when we
say basically the same thing.

--
James Kanze (GABI Software)             email:james.ka ... @gmail.com
Conseils en informatique orientée objet/
Beratung in objektorientierter Datenverarbeitung
9 place Sémard, 78210 St.-Cyr-l'École, France, +33 (0)1 30 23 00 34
</POST>
<POST>
<POSTER> Ian Collins &lt;ian-n...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-09-29T05:09:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James Kanze wrote:
&gt; On Sep 29, 1:00 am, Ian Collins &lt;ian-n ... @hotmail.com&gt; wrote:
&gt;&gt; James Kanze wrote:

&gt;&gt;&gt;&gt; So, in conclusion, I don't think it's the Agile community who
&gt;&gt;&gt;&gt; is being immature here.

&gt;&gt;&gt; Just a serious step backwards.

&gt;&gt; Those with the courage to have taken the step would disagree.

&gt; It corresponds roughly to what was standard practice 20 or 30
&gt; years ago.  Fancy names have been added, but the fundamentals
&gt; haven't changed.  Some companies have moved beyond.
">

30 year ago when I started writing software, we submitted our jobs on
cards and waited days for the results.  When I started professional
development 5 years later, our build still took hours.

I don't see much similarity between how I work now and how I worked then.

I must admit, TDD with cards would be interesting, submit the test cards
Monday, get the fail back Wednesday, submit the code cards to pass the
test Thursday, get the pass back Monday.  One test a week. Things would
have improved with our PDP11, we could probably manage to or three a day
on that.

--
Ian Collins.
</POST>
<POST>
<POSTER> &quot;Phlip&quot; &lt;phlip...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-09-29T08:09:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ian Collins wrote:
&gt; I must admit, TDD with cards would be interesting, submit the test cards
&gt; Monday, get the fail back Wednesday, submit the code cards to pass the
&gt; test Thursday, get the pass back Monday.  One test a week. Things would
&gt; have improved with our PDP11, we could probably manage to or three a day
&gt; on that.
">

IIRC, that's how the Gemini software program worked.

--
Phlip
</POST>
</TEXT>
</BODY>
</DOC>
