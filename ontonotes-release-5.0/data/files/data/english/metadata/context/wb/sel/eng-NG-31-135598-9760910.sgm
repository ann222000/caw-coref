<DOC>
<DOCID> eng-NG-31-135598-9760910 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2008-01-08T08:09:00 </DATETIME>
<BODY>
<HEADLINE>
Java as a first language &quot;considered harmful&quot;
</HEADLINE>
<TEXT>
<POST>
<POSTER> Didier Verna &lt;did...@lrde.epita.fr&gt; </POSTER>
<POSTDATE> 2008-01-08T08:09:00 </POSTDATE>
Guys,

this article might be of interest to you:

http://www.stsc.hill.af.mil/CrossTalk/2008/01/0801DewarSchonberg.html

--
Resistance is futile. You will be jazzimilated.

Didier Verna, did ... @lrde.epita.fr, http://www.lrde.epita.fr/~didier

EPITA / LRDE, 14-16 rue Voltaire   Tel.+33 (0)1 44 08 01 85
94276 Le Kremlin-Bicêtre, France   Fax.+33 (0)1 53 14 59 22  did ... @xemacs.org
</POST>
<POST>
<POSTER> Jon Harrop &lt;use...@jdh30.plus.com&gt; </POSTER>
<POSTDATE> 2008-01-08T10:01:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Didier Verna wrote:
&gt;        Guys,

&gt; this article might be of interest to you:

&gt; http://www.stsc.hill.af.mil/CrossTalk/2008/01/0801DewarSchonberg.html
">

I'm more worried about the future of programming implementations myself.
Who's going to create the industrial-strength FPL implementations that
we're currently missing?

--
Dr Jon D Harrop, Flying Frog Consultancy Ltd.
http://www.ffconsultancy.com/products/?u
</POST>
<POST>
<POSTER> Ray Dillinger &lt;b...@sonic.net&gt; </POSTER>
<POSTDATE> 2008-01-09T04:04:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Jon Harrop wrote:
&gt; I'm more worried about the future of programming implementations myself.
&gt; Who's going to create the industrial-strength FPL implementations that
&gt; we're currently missing?
">

What, in your opinion, are the characteristics of these implementations?

My list includes:

* Creation of native-format executables requiring no linkables
or libraries unlikely to be found on end-user systems
(user expectation, deliverability)

* Profiling and profile-based optimization tools.

* Code analysis tools integrated with source control
and identity-based package management and keyword
control.  ie, when setting up source control, the lead
engineer should be able to specify that the &quot;graphics&quot;
group should not be able to modify code in the &quot;math&quot;
library and the newbie programmers should not be able
to check in code that uses anything off a list of
&quot;dangerous or ill-concieved&quot; constructs (meta-macros,
data not subject to GC, finalizers that can bring
things back to life, call/cc, goto, routines with
nonstandard argument evaluation disciplines, etc) until
they've had the relevant training, and certain constructs
are absolutely forbidden in the &quot;air-traffic control&quot;
project (which is subject to a legal requirement of
proofs of certain properties such as termination, hard
realtime constraints, and finite memory usage).
The source control system should include calls to
code analysis tools to help enforce these policies.

* Constructs (possibly including &quot;dangerous or
ill-concieved&quot; ones, see above) that can serve as the
direct translation target for all or nearly-all extant
computer languages. IOW, it should be simple to
translate a particular set of routines (perhaps
originally written in smalltalk) preserving their
call-by-need semantics, and still have them
interoperate nicely with the rest of the system,
even if mixing call-by-need with call-by-value
semantics is considered confusing or dangerous.

* If we have analysis tools that recognize dangerous or
ill-concieved constructs, and means of transforming or
eliminating these constructs are known, then we should
have code transformation tools that do exactly that,
automatically, insofar as possible.  For example,
eliminating call/cc by using continuation-passing style.

* System settings that choose between whole-program
optimization, fully separate file-based static
compilation, and an interactive runtime.

Bear
</POST>
<POST>
<POSTER> Paul Rubin &lt;http://phr...@NOSPAM.invalid&gt; </POSTER>
<POSTDATE> 2008-01-09T04:32:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Dillinger &lt;b ... @sonic.net&gt; writes:
&gt;    are absolutely forbidden in the &quot;air-traffic control&quot;
&gt;    project (which is subject to a legal requirement of
&gt;    proofs of certain properties such as termination, hard
&gt;    realtime constraints, and finite memory usage).
">

I've seen some papers mentioning termination proofs but the methods
described didn't seem of much use for air traffic control.  For
example, David Turner's &quot;Total Functional Programming&quot; describes a
language where all programs terminate, but the running time can be
anything describable in Peano arithmetic, i.e. it could be a tower of
exponentials of arbitrary height.  So proofs of termination without
practical complexity bounds seem not so useful.
</POST>
<POST>
<POSTER> Abdulaziz Ghuloum &lt;aghul...@cee.ess.indiana.edu&gt; </POSTER>
<POSTDATE> 2008-01-09T06:38:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Dillinger wrote:
&gt; Jon Harrop wrote:

&gt;&gt; I'm more worried about the future of programming implementations myself.
&gt;&gt; Who's going to create the industrial-strength FPL implementations that
&gt;&gt; we're currently missing?

&gt; What, in your opinion, are the characteristics of these implementations?

&gt; My list includes:
">

You had a list, so let me ask this:
Who's going to create the tools that you said you are missing if it's
not you?  You're not waiting for kids/programmers of the future to do it
for you, right?
</POST>
<POST>
<POSTER> Jon Harrop &lt;use...@jdh30.plus.com&gt; </POSTER>
<POSTDATE> 2008-01-09T07:12:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Dillinger wrote:
&gt; Jon Harrop wrote:
&gt;&gt; I'm more worried about the future of programming implementations myself.
&gt;&gt; Who's going to create the industrial-strength FPL implementations that
&gt;&gt; we're currently missing?

&gt; What, in your opinion, are the characteristics of these implementations?

&gt; My list includes:

&gt; * Creation of native-format executables requiring no linkables
&gt;    or libraries unlikely to be found on end-user systems
&gt;    (user expectation, deliverability)
">

I would like a cross-platform open source common language run-time with good
support for functional programming (e.g. tail calls and fast allocation)
and commerce (so I can sell shared run-time DLLs to its users).

That is essentially the .NET CLR but open source and cross platform.

<QUOTE PREVIOUSPOST="
&gt; * Profiling and profile-based optimization tools.
">

Yes.

<QUOTE PREVIOUSPOST="
&gt; * Code analysis tools integrated with source control
&gt;    and identity-based package management and keyword
&gt;    control.  ie, when setting up source control, the lead
&gt;    engineer should be able to specify that the &quot;graphics&quot;
&gt;    group should not be able to modify code in the &quot;math&quot;
&gt;    library and the newbie programmers should not be able
&gt;    to check in code that uses anything off a list of
&gt;    &quot;dangerous or ill-concieved&quot; constructs (meta-macros,
&gt;    data not subject to GC, finalizers that can bring
&gt;    things back to life, call/cc, goto, routines with
&gt;    nonstandard argument evaluation disciplines, etc) until
&gt;    they've had the relevant training, and certain constructs
&gt;    are absolutely forbidden in the &quot;air-traffic control&quot;
&gt;    project (which is subject to a legal requirement of
&gt;    proofs of certain properties such as termination, hard
&gt;    realtime constraints, and finite memory usage).
&gt;    The source control system should include calls to
&gt;    code analysis tools to help enforce these policies.
">

Interesting.

<QUOTE PREVIOUSPOST="
&gt; * System settings that choose between whole-program
&gt;    optimization, fully separate file-based static
&gt;    compilation, and an interactive runtime.
">

Yes.

I would add:

. High-performance numerics, like OCaml and C++.
. High-performance symbolics, like MLton.
. Tested bindings to core libraries like OpenGL 2.
. Documentation.

--
Dr Jon D Harrop, Flying Frog Consultancy Ltd.
http://www.ffconsultancy.com/products/?u
</POST>
<POST>
<POSTER> Ray Dillinger &lt;b...@sonic.net&gt; </POSTER>
<POSTDATE> 2008-01-09T14:44:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Abdulaziz Ghuloum wrote:
&gt; Ray Dillinger wrote:
&gt;&gt; My list includes:

&gt; You had a list, so let me ask this:
&gt; Who's going to create the tools that you said you are missing if it's
&gt; not you?  You're not waiting for kids/programmers of the future to do it
&gt; for you, right?
">

Heh.  got me in one.

Yes, I have a lisp implementation.  No, it's not finished yet.
Yes, I'm planning and doing the stuff on my list with it and was
hoping to see what other people's lists include.

The ideas involved have been evolving for a long time, starting with
some theoretical frustration with the absence of callable objects
that are both first-class (like functions in Lispy languages) and
first-order (like macros in Lispy languages).

It took me several years and a bunch of iterations to get to this,
and I repeated a lot of work (fexprs, etc) that smarter people had
already found dead-ends in first.  But eventually I came up with
theory both sound and semi-practical about how to keep environments
and their arguments straight through multiple layers of calls
and code transformations.

And the immediate result was that all traces of &quot;macros&quot; vanished.

Once I'd finally figured out a way to support first-class &quot;macros&quot;,
they weren't macros anymore: they were functions with argument
evaluation under control of the called function, like fexprs, but
an environment (both static and dynamic) packaged with each
argument expression. The calling function's responsibility is
just to make sure that the called function has environment info
for each and every argument and for the call-site. And the same
calling discipline could handle functions, whether call-by-need,
call-by-value, call-by-name (as in denotational semantics)
or whatever else.

And that in turn gave me the idea that what I was working on here
was sort of fundamental in some way; a language wherein every calling
discipline you've ever heard of can coexist, plus you can effectively
&quot;roll your own&quot; calling discipline on an individual basis for things
you'd normally use macrology for in Lisp -- means it can be a very
easy, straightforward translation target for code written in every
computer language extant.  A nice compiler intermediate language,
for those who don't want to use it directly.

My main interests are semantics and theory; and these cool semantics
have come with a substantial performance and memory cost, since I
wind up basically keeping the whole source in memory at runtime.
Also, I'm much more interested in &quot;correct&quot; numerics than &quot;fast&quot;
numerics: I like MatLab's format for keeping exact representations
wherever possible, Scheme's distinction between exact and inexact
values, COBOL's requirement for exact base-ten mathematics, and
interval arithmetic for documenting possible error ranges.  I'm not
so much into the idea of blazing speed as I'm into the idea of
mathematical results that you can prove are exact, or at least
correct within a known error limit.

I think that finding ways to compile it so it runs as efficiently
as other dialects (at least when you don't do things impossible in
those dialects) will occupy years to come.

Bear
</POST>
<POST>
<POSTER> Xah Lee &lt;x...@xahlee.org&gt; </POSTER>
<POSTDATE> 2008-01-09T15:20:00 </POSTDATE>
Adding to Ray Dillinger and Joh Harrop's post about a ideal functional
lang system, my wishes are:

★ The language will be absolutely high-level, meaning in particular:

☆ The language will not have cons.

☆ The language's documentation, will not need to have mentioning any
of the following words: pointer, reference, memory allocation, stacks,
hash.

☆ The language's variables must not have types. Its values can have
types. And, these types must be mathematics, not computer engineering
inventions. Specifically: The language will not have any concepts or
terms of: float, double, int, long, etc. (it can, however, have
concepts like exact number, n-bit approximate number aka machine-
precision number (See below on optimization declaration).)

☆ The numbers in the language can have the following types: integer,
rational, real, complex number. (and possibly extention from these,
such as algebraic/roots number, etc.)

☆ The language's computational model should be simple, high-level,
mathematics based as possible, as opposed to being a model of some
abstract machine, implementation, or computational model (e.g. OOP,
Lisp Machine). In particular: the language will not have concepts of
“reference” or “object” (as in Java), or “object” (as in lisp). (Here,
few of today's language that qualify includes: Mathematica, PHP,
Javascript)

☆ The language will not use extraneous computer-science-geeking
moronic jargons. (which in part exist as a side-effect of human
animal's power struggle in academia) More specifically, the language
will not have anything named “lambda”. (name it “Function” or simply
“Subroutine”) It should not have anything named or discused as
“Currying”. (when appropriate, explain with precision such as:
applying a function to a function, or decompose multi-valued function,
etc as appropriate. If necessary, invent new terms that is
communicative: e.g. Function Arity Reduction) The lang should not have
a name or doc about tail-fucking-recursion. (if necessary, use
something like this: “the compiler does automatic optimization of
recursions when ...” ) ... etc.

☆ The language will not have concept of binary bits, bit operator,
bytes, etc. (see the optimization section below)

★ The language can have many implementation/compiling necessateted/
related concepts. (which are examplified in the above occuring in
other langs which we forbid here) However, these concepts, must be
designed as special declaration constructs, whose purpose to the
programer is _clearly_ understood as instructions/hints for the
compiler that will create fast code, yet has no purpose what-so-ever
as far as his program is concerned. So, for example, let's say the
typical concept of “int” in most langs. In our ideal lang, it would be
like this:

myNumber= 123
DeclareForOptimization(myNumber, (number, (8-bits,no-decimals)))

For another example, suppose you need a hash table. But in our high
level concept there's no such stupid computer-engineering concept, nor
such a stupid term “hash”. However, because we are great
mathematicians, we know that compilers cannot in theory 100% determine
user's use of lists for the purpose of optimization. e.g. hash table.
So, the lang will have a usage-declaration for the purpose of
compiling/optimization, something like this:

myKeydList= ((k1 v1) (k2 v2) ...)
DeclareForOptimization(myKeydList, (list, ((fastLookup true),
(beginNumOfSlots 4000),(extendBy, 500), ...)))

----------------------

The above criterions are basically all satisfied by Mathematica,
except the last item about a complete, systematic, declarative
statements for the purpose of optimization/compiling.

In general, the above criterions are more and more satisfied by modern
high-level languages, in particular PHP and Javascript. (except in
places where they can't help, such as the widely-established int,
long, double fuck created (or propergated) by the unix/C fuckheads.)

Languages in the past 2 decades are already more and more observing
the above criterions. Roughly in order of history and high-level-ness:
C, shell, Perl, Java, Python, Ruby, Javascript, PHP.

2 major exceptions to this progression are lisp and Mathematica. Lisp
is extremely high-level, however, due to it's 40-years-old age,
inevitably today it has many socially un-fixable baggages. (such as
the cons business) Mathematica, which was born about the same time
Perl was, is simply well designed, by a certified genius, who happens
to be a rich kid too to boot, and happens to be a not a tech-geeking
moron and in fact is a gifted entrepreneur and started a exceedingly
successful company, who's social contributions to the world has made
revolutionary impact. (besides his scientific contributions, e.g. in
physics.)

Some lispers conceive that langs are just more and more converging to
lisp. No, they are no converging to no fucking lisp. They are simply
getting higher-level, which lisp is the grand-daddy of high-level,
intelligent, computing and design.

Note: I had some ambiguity about compiled langs like Haskel (or F#
etc) They are extremely high-level, but meanwhile has the compilation/
types thingy... I dont have much experience with functional compiled
languages (dont have much experience with compiled lang, period.) so i
don't have much idea how they fit into the high-level measured for
real-world ease-of-use practicality.
)

----------------------

in the above writing, i'm in a bit frenzy to fuck the tech geeking
morons that are bountiful here. But in hindsight, i think my criterion
for a ideal functional lang, two of the rather unusual ideas of high-
level-ness, can be summarized thus:

★ Do not use any computer science or computer engineering terms who's
names does not convey its meaning. (e.g. float, int, lambda, tail-
recursion, currying)

★ Separation of compiler/optimization needed constructs or concepts
clearly out of the language. (no more int, float, double, hash,
vector, array types. Perhaps no more even types for variables.)

----------------------

many functional programers are acquainted with the idea, that language
influence thought. (tech geeking morons will invariably invoke the
“Sapir-Whorf” lingo fuck.) However, these average people (arbeit
elite), do not have the independent thinking to realize that,
terminology, or, the naming of things, has a major impact on the thing
and the social aspects of the thing. More concretely, how is a thing/
concept named, has major impact on education, communication,
popularity/spread, and actual use of the thing/concept. As a actual
example, many high-level functional languages are invented by
academicians, who, in general, do not have much social-science related
knowledge or understand of its practicality. And, being high-powered
computer engineers, are basically conditioned in their thought
patterns around dense, stupid, computer jargons. And thus in their
languages, these jargons permeate throughout the language. The
practical effect is that, these languages, are perpetually swirling
around these academic communities with a viral tranmission so powerful
that sucks in anyone who tried to touch/study/use the language into
mumble jumble. (most examplary: Scheme, Haskell) This basically seals
the fate of these languages.

(of course, the one notable exception, is Mathematica. It is still to
be seen, if Microsoft's f#'s team will avoid stupid jargons.)

----------------------

Further readings:

★ Math Terminology and Naming of Things
http://xahlee.org/cmaci/notation/math_namings.html

★ What are OOP's Jargons and Complexities
http://xahlee.org/Periodic_dosage_dir/t2/oop.html

★ Jargons of Info Tech industry
http://xahlee.org/UnixResource_dir/writ/jargons.html

★ Politics and the English Language
http://xahlee.org/p/george_orwell_english.html

★ Lisp's List Problem
http://xahlee.org/emacs/lisp_list_problem.html

Xah
x ... @xahlee.org
∑ http://xahlee.org/
</POST>
<POST>
<POSTER> &quot;David B. Benson&quot; &lt;dben...@eecs.wsu.edu&gt; </POSTER>
<POSTDATE> 2008-01-10T19:40:00 </POSTDATE>
On Jan 9, 12:20 pm, Xah Lee &lt;x ... @xahlee.org&gt; wrote:
(four letter word, repeatedly, I suppose to make up for his inability
to comprehend, let alone understand.)

Yawn.
</POST>
<POST>
<POSTER> Ray Dillinger &lt;b...@sonic.net&gt; </POSTER>
<POSTDATE> 2008-01-10T21:01:00 </POSTDATE>
What we hate isn't relevant. If I thought that what I hated were
relevant, I'd have left out most loop syntax.  If I thought that what
you (or any random person on the Internet) hated was relevant, I'd
quit using the internet and go pet the cat some more (the cat likes
that).

Good languages are designed by making it easy to do things right and
do them in ways that leave their meanings well-defined, not by avoiding
things that someone hates.

Bear
</POST>
<POST>
<POSTER> Ray Blaak &lt;rAYbl...@STRIPCAPStelus.net&gt; </POSTER>
<POSTDATE> 2008-01-11T02:21:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; The ideas involved have been evolving for a long time, starting with
&gt; some theoretical frustration with the absence of callable objects
&gt; that are both first-class (like functions in Lispy languages) and
&gt; first-order (like macros in Lispy languages).

&gt; It took me several years and a bunch of iterations to get to this,
&gt; and I repeated a lot of work (fexprs, etc) that smarter people had
&gt; already found dead-ends in first.  But eventually I came up with
&gt; theory both sound and semi-practical about how to keep environments
&gt; and their arguments straight through multiple layers of calls
&gt; and code transformations.
[...]
&gt; And that in turn gave me the idea that what I was working on here
&gt; was sort of fundamental in some way; a language wherein every calling
&gt; discipline you've ever heard of can coexist, plus you can effectively
&gt; &quot;roll your own&quot; calling discipline on an individual basis for things
&gt; you'd normally use macrology for in Lisp -- means it can be a very
&gt; easy, straightforward translation target for code written in every
&gt; computer language extant.  A nice compiler intermediate language,
&gt; for those who don't want to use it directly.
">

Have you written this up somewhere, or do you plan to? This is the kind of
thing that would make a good paper.

[Followups not obeyed -- I read this in c.l.s and want to continue to do
so. What you're talking about transcends scheme, lisp, and impacts any
functional language. This discussion should be interesting to all the
newsgroups mentioned.]

--
Cheers,                                        The Rhythm is around me,
The Rhythm has control.
Ray Blaak                                      The Rhythm is inside me,
rAYbl ... @STRIPCAPStelus.net                    The Rhythm has my soul.
</POST>
<POST>
<POSTER> maximinus &lt;maximi...@gmail.com&gt; </POSTER>
<POSTDATE> 2008-01-11T04:49:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt; ★ Do not use any computer science or computer engineering terms who's
&gt; names does not convey its meaning. (e.g. float, int, lambda, tail-
&gt; recursion, currying)
">

You know what? All of those terms convey meaning to me, and to most
people here I expect. I could say 'a number composed of a mantissa
with integer range 0 - 2^x, an exponent of integer range 0 - 2^y, and
flag S to indicate the sign of the value, the whole taking up (x+y)+1
bits of computer memory', or I could just say 'float'. Should I also
drop math jargon whilst doing math? Cos' the term 'C-infinite
functions' really tells me whats going on there...

I'm sorry. I'll stop feeding the trolls right now.

On Jan 11, 7:21 am, Ray Blaak &lt;rAYbl ... @STRIPCAPStelus.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &gt; The ideas involved have been evolving for a long time, starting with
&gt; &gt; some theoretical frustration with the absence of callable objects
&gt; &gt; that are both first-class (like functions in Lispy languages) and
&gt; &gt; first-order (like macros in Lispy languages).

&gt; &gt; It took me several years and a bunch of iterations to get to this,
&gt; &gt; and I repeated a lot of work (fexprs, etc) that smarter people had
&gt; &gt; already found dead-ends in first.  But eventually I came up with
&gt; &gt; theory both sound and semi-practical about how to keep environments
&gt; &gt; and their arguments straight through multiple layers of calls
&gt; &gt; and code transformations.
&gt; [...]
&gt; &gt; And that in turn gave me the idea that what I was working on here
&gt; &gt; was sort of fundamental in some way; a language wherein every calling
&gt; &gt; discipline you've ever heard of can coexist, plus you can effectively
&gt; &gt; &quot;roll your own&quot; calling discipline on an individual basis for things
&gt; &gt; you'd normally use macrology for in Lisp -- means it can be a very
&gt; &gt; easy, straightforward translation target for code written in every
&gt; &gt; computer language extant.  A nice compiler intermediate language,
&gt; &gt; for those who don't want to use it directly.

&gt; Have you written this up somewhere, or do you plan to? This is the kind of
&gt; thing that would make a good paper.

&gt; [Followups not obeyed -- I read this in c.l.s and want to continue to do
&gt;  so. What you're talking about transcends scheme, lisp, and impacts any
&gt;  functional language. This discussion should be interesting to all the
&gt;  newsgroups mentioned.]

&gt; --
&gt; Cheers,                                        The Rhythm is around me,
&gt;                                                The Rhythm has control.
&gt; Ray Blaak                                      The Rhythm is inside me,
&gt; rAYbl ... @STRIPCAPStelus.net                    The Rhythm has my soul.
">
</POST>
<POST>
<POSTER> Ulf Wiger &lt;ulf.wi...@e-r-i-c-s-s-o-n.com&gt; </POSTER>
<POSTDATE> 2008-01-11T06:38:00 </POSTDATE>
Jon Harrop skrev:

<QUOTE PREVIOUSPOST="
&gt; Didier Verna wrote:
&gt;&gt;        Guys,

&gt;&gt; this article might be of interest to you:

&gt;&gt; http://www.stsc.hill.af.mil/CrossTalk/2008/01/0801DewarSchonberg.html

&gt; I'm more worried about the future of programming implementations
">

&gt; myself. Who's going to create the industrial-strength FPL
&gt; implementations that we're currently missing?

While this is in many respects a very valid point, I think
it's important to stress that there /are/ FPL implementations
that are industrial-strength within a certain niche. Even
so, these implementations face roughly the same resistance
in their niche, and many people decide to hold out until
their own favorite language reaches reasonable industrial-
strength.

We experienced this with Erlang, which obviously meets
the requirements on industrial-strength within telecoms
(obviously, since competitive telecom products based on
Erlang have been shipped to customers for 10 years now,
and have also exhibited excellent life-cycle economy.)

Java implementations really haven't been serious
contenders, from a technical point of view, until
recently - and then mainly when there is an expressed
requirement to support J2EE for customer-specific
adaptation. Even so, we've seen several projects rather
choose Java (and face the consequences) than look at
Erlang, even in the face of studies showing that it
would be at least 5x more expensive to do so, and would
result in worse performance and less flexibility(*).

So there is a big measure of opportunism in the
argumentation against FPLs: if there happen to be
reasonable mainstream alternatives that can be described
as &quot;industrial-strength&quot;, then this becomes the main
argument; if not, then the &quot;mainstream&quot; argument is used,
to roughly the same effect.

I think there seems to be a trend now, that FP concepts
make their way into &quot;conventional&quot; programming languages
(Python, C#, ...). This may serve to create a broader
interest in higher-order functions, strong type systems,
etc., which in turn, may feed an interest in actually
asking for industrial-strength tools that have strong FP
support.

But as long as there is no broad consensus that FP is
even useful, no one will care if your FP implementation
is industrial-strength or not.

BR,
Ulf W

(*) In this particular case, surprisingly many readers
seemed to interpret the report as &quot;it's not impossible
to do this in Java&quot;, and that was seen as the good news.
That doing it in Erlang would be much better was not
interesting, since it wasn't seen as a viable alternative
for other reasons (proof or clarification not needed).
</POST>
<POST>
<POSTER> Joachim Durchholz &lt;j...@durchholz.org&gt; </POSTER>
<POSTDATE> 2008-01-12T14:52:00 </POSTDATE>
Ray Dillinger schrieb:

<QUOTE PREVIOUSPOST="
&gt; My list includes:

&gt; * Code analysis tools integrated with source control
">

What's source control?
Revision control? In that case: why integrated?

<QUOTE PREVIOUSPOST="
&gt;   and identity-based package management
">

Borderline case: bugfix updates.

&gt;   and keyword control.

What's that?

&gt;   ie, when setting up source control, the lead

<QUOTE PREVIOUSPOST="
&gt;   engineer should be able to specify that the &quot;graphics&quot;
&gt;   group should not be able to modify code in the &quot;math&quot;
&gt;   library and the newbie programmers should not be able
&gt;   to check in code that uses anything off a list of
&gt;   &quot;dangerous or ill-concieved&quot; constructs (meta-macros,
&gt;   data not subject to GC, finalizers that can bring
&gt;   things back to life, call/cc, goto, routines with
&gt;   nonstandard argument evaluation disciplines, etc) until
&gt;   they've had the relevant training,
">

This kind of stuff is better controlled via policy.
Software-enforced policy is usually far too rigid to be really useful.

&gt;   and certain constructs

<QUOTE PREVIOUSPOST="
&gt;   are absolutely forbidden in the &quot;air-traffic control&quot;
&gt;   project (which is subject to a legal requirement of
&gt;   proofs of certain properties such as termination, hard
&gt;   realtime constraints, and finite memory usage).
">

It's better to enable the compiler to check this kind of property directly.
Forbidding constructs is suppressing symptoms instead of applying a cure
IMNSHO.

<QUOTE PREVIOUSPOST="
&gt; * Constructs (possibly including &quot;dangerous or
&gt;   ill-concieved&quot; ones, see above) that can serve as the
&gt;   direct translation target for all or nearly-all extant
&gt;   computer languages. IOW, it should be simple to
&gt;   translate a particular set of routines (perhaps
&gt;   originally written in smalltalk) preserving their
&gt;   call-by-need semantics, and still have them
&gt;   interoperate nicely with the rest of the system,
&gt;   even if mixing call-by-need with call-by-value
&gt;   semantics is considered confusing or dangerous.
">

What's dangerous is actually the combination of nonstrict evaluation
strategies and impurity.
I'd also expect that mixing evaluation strategies makes the actual flow
of control even more nonobvious than with lazy evaluation alone.

<QUOTE PREVIOUSPOST="
&gt; * If we have analysis tools that recognize dangerous or
&gt;   ill-concieved constructs, and means of transforming or
&gt;   eliminating these constructs are known, then we should
&gt;   have code transformation tools that do exactly that,
&gt;   automatically, insofar as possible.  For example,
&gt;   eliminating call/cc by using continuation-passing style.
">

If call/cc is used in a way that can be replaced with CPS, then you
don't need to transform it because it is already safe (assuming for the
moment that CPS is safe).

Regards,
Jo
</POST>
<POST>
<POSTER> Ray Blaak &lt;rAYbl...@STRIPCAPStelus.net&gt; </POSTER>
<POSTDATE> 2008-01-12T23:01:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Joachim Durchholz &lt;j ... @durchholz.org&gt; writes:
&gt; Ray Dillinger schrieb:
&gt; &gt;   ie, when setting up source control, the lead engineer should be able to
&gt; &gt;   specify that the &quot;graphics&quot; group should not be able to modify code in
&gt; &gt;   the &quot;math&quot; library and the newbie programmers should not be able to
&gt; &gt;   check in code that uses anything off a list of &quot;dangerous or
&gt; &gt;   ill-concieved&quot; constructs (meta-macros, data not subject to GC,
&gt; &gt;   finalizers that can bring things back to life, call/cc, goto, routines
&gt; &gt;   with nonstandard argument evaluation disciplines, etc) until they've had
&gt; &gt;   the relevant training,

&gt; This kind of stuff is better controlled via policy.
&gt; Software-enforced policy is usually far too rigid to be really useful.
">

I agree, strongly so, in fact. Just defining what is dangerous is a moving
target, let alone implementing any tools to enforce it. &quot;Common sense&quot; with
decent code reviews is the cheapest way to acheive sanity at a reasonable
cost. How would a tool measure one's trained ability?

How would you write such a tool set?

--
Cheers,                                        The Rhythm is around me,
The Rhythm has control.
Ray Blaak                                      The Rhythm is inside me,
rAYbl ... @STRIPCAPStelus.net                    The Rhythm has my soul.
</POST>
<POST>
<POSTER> Ray Dillinger &lt;b...@sonic.net&gt; </POSTER>
<POSTDATE> 2008-01-13T14:44:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Joachim Durchholz wrote:
&gt; Ray Dillinger schrieb:
&gt;&gt; My list includes:

&gt;&gt; * Code analysis tools integrated with source control

&gt; What's source control?
&gt; Revision control? In that case: why integrated?
">

Yes, revision control.  Integrated to give additional tools
for code policy enforcement to organizations with policies
and coding standards.

Integrated to prevent projects that are supposed to have
specific coding goals or runtime models from using or depending
on code that does not.

Integrated to prevent people from ignoring a requirement of
code review when using constructs they may or may not understand.
And so on.

Whether it's harmful or not depends on whether it's done with
understanding of the coding process; a pointy-haired boss could
use this tool the way pointy-haired bosses use any software
engineering management tool.  There is no help for that but
the eventual bankruptcy of their companies.

But assuming for a moment or two that the people we care about
in software development are mostly smart, and set up their
software-enforced policies with some understanding of the process
and the work it creates, it should be at worst a speed bump to
someone checking it in himself (&quot;oh yeah, the stupid tool's
worried about infinite precision rationals exploding in memory,
but I don't need that level of precision so I should go back and
use float values instead...&quot;), and at best a useful tool to prompt
a code review (&quot;Hey Jack? I want to check in a meta-macro, but my
key doesn't have that level of authority in the Foonly project.
Can you look it over and vet it with your key?&quot;).

<QUOTE PREVIOUSPOST="
&gt;  &gt;   and keyword control.

&gt; What's that?
">

Sorry.  That's me using words together that don't mean anything
together except to me.  I've developed, and have been using,
this idea as a guide in trying to design a lisp dialect whose
primitives have very clean design.

I have a perception that in a language of clean design, most
specific concepts, ideas, or extensions are associated with
particular keywords.  For example, if you want to do a project
using immutable CONS cells (which allows the compiler to do all
sorts of optimizations it otherwise could not do, pretty much
including eliminating CONS cells as a distinct entity in memory
all together) then you need to scan the code for uses of RPLACA
and RPLACD.  Because nothing else mutates cons cells, these
keywords control the idea or extension of mutable cons cells.

Code that doesn't use them (including the transitive closure of
code it calls) will not violate the constraint against using immutable
CONS cells.  Thus RPLACA and RPLACD are the &quot;keywords&quot; you need to
control if your policy is that a particular project should benefit
from the optimizations available only to code that uses immutable
CONS cells.  Likewise if you want your project to benefit from
optimizations that can happen only in the absence of reified
continuations (like, say, using a hardware stack discipline) then
you need to control the keyword CALL/CC (or
call-with-current-continuation if you're using the spectacularly
verbose version of the keyword from Scheme).

<QUOTE PREVIOUSPOST="
&gt; This kind of stuff is better controlled via policy.
&gt; Software-enforced policy is usually far too rigid to be really useful.
">

Speed limits are enforced by policy.  That's no reason to shirk
the manufacture of mechanical speedometers.  A speedometer can
tell you when you're in compliance with the policy even in cases
where you otherwise might have to guess.  And hey, if I could get
a device that used GPS to determine exactly where I was, looked
up the local speed limit in a table, and notified me every time
I was exceeding the speed limit, I'd happily buy it.  If there
were a hardware switch I had to specifically flip ON in order to
exceed the speed limit, I think I'd have left if OFF for at
least the last three years.

<QUOTE PREVIOUSPOST="
&gt; It's better to enable the compiler to check this kind of property
&gt; directly. Forbidding constructs is suppressing symptoms instead of
&gt; applying a cure IMNSHO.
">

Yeah.  In my model/plan/system I'm trying to make, the serious hardcore
analysis of code is mostly done by the compiler.  It checks this kind of
property and outputs a file of analysis results that client programs can
read and check.  Other code analysis tools can append expressions to the
analysis results files too, but mostly if you can do it programmatically
I'm thinking you should be doing it in the compiler.  The client
programs, in turn, would be called at need by the revision-control system.

Bear
</POST>
<POST>
<POSTER> Ray Blaak &lt;rAYbl...@STRIPCAPStelus.net&gt; </POSTER>
<POSTDATE> 2008-01-14T02:58:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Dillinger &lt;b ... @sonic.net&gt; writes:
&gt; But assuming for a moment or two that the people we care about in software
&gt; development are mostly smart, and set up their software-enforced policies
&gt; with some understanding of the process and the work it creates, it should be
&gt; at worst a speed bump to someone checking it in himself (&quot;oh yeah, the
&gt; stupid tool's worried about infinite precision rationals exploding in
&gt; memory, but I don't need that level of precision so I should go back and use
&gt; float values instead...&quot;), and at best a useful tool to prompt a code review
&gt; (&quot;Hey Jack? I want to check in a meta-macro, but my key doesn't have that
&gt; level of authority in the Foonly project.  Can you look it over and vet it
&gt; with your key?&quot;).
">

I stand against this. First of all, if your people are mostly smart, the solves
most of the problems right there.

Second, I don't think such tools can be practically written. Fiercly
difficult, they would be.

Third, such tools cannot properly understand the reason for any violation, so
in the end it equates to getting a human involved to vet things. And if that's
the case, it is far far simpler and easier to enforce that all commits simply
get reviewed, period.

Forth, it is far more effective and efficient to track things as opposed to
prevent possible bad things. In a proper revision system you can always
recover and correct mistakes. Halting commits will be guaranteed to cause
problems and unexpected delays. If Jack gets hit by a bus, you need to still
proceed.

[Followups *not* obeyed, again. Please don't do that. This is a meta level
discussion, and I submit, interesting to all the groups]

<QUOTE PREVIOUSPOST="
&gt; For example, if you want to do a project using immutable CONS cells...then
&gt; you need to scan the code for uses of RPLACA and RPLACD.  Because nothing
&gt; else mutates cons cells, these keywords control the idea or extension of
&gt; mutable cons cells.
">

Just fix your language/compiler instead to not have them present. Then no
scanning is needed.

Would you also have to fix libraries as well? If you can't then you have no
guarantees anyway.
--
Cheers,                                        The Rhythm is around me,
The Rhythm has control.
Ray Blaak                                      The Rhythm is inside me,
rAYbl ... @STRIPCAPStelus.net                    The Rhythm has my soul.
</POST>
<POST>
<POSTER> gavino &lt;gavcom...@gmail.com&gt; </POSTER>
<POSTDATE> 2008-01-14T03:34:00 </POSTDATE>
On Jan 8, 7:01 am, Jon Harrop &lt;use ... @jdh30.plus.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Didier Verna wrote:
&gt; &gt;        Guys,

&gt; &gt; this article might be of interest to you:

&gt; &gt; http://www.stsc.hill.af.mil/CrossTalk/2008/01/0801DewarSchonberg.html

&gt; I'm more worried about the future of programming implementations myself.
&gt; Who's going to create the industrial-strength FPL implementations that
&gt; we're currently missing?

&gt; --
&gt; Dr Jon D Harrop, Flying Frog Consultancy Ltd. http://www.ffconsultancy.com/products/?u
">

I thought SBCL and haskell are here?
</POST>
<POST>
<POSTER> gavino &lt;gavcom...@gmail.com&gt; </POSTER>
<POSTDATE> 2008-01-14T03:37:00 </POSTDATE>
On Jan 9, 1:04 am, Ray Dillinger &lt;b ... @sonic.net&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; Jon Harrop wrote:
&gt; &gt; I'm more worried about the future of programming implementations myself.
&gt; &gt; Who's going to create the industrial-strength FPL implementations that
&gt; &gt; we're currently missing?

&gt; What, in your opinion, are the characteristics of these implementations?

&gt; My list includes:

&gt; * Creation of native-format executables requiring no linkables
&gt;    or libraries unlikely to be found on end-user systems
&gt;    (user expectation, deliverability)

&gt; * Profiling and profile-based optimization tools.

&gt; * Code analysis tools integrated with source control
&gt;    and identity-based package management and keyword
&gt;    control.  ie, when setting up source control, the lead
&gt;    engineer should be able to specify that the &quot;graphics&quot;
&gt;    group should not be able to modify code in the &quot;math&quot;
&gt;    library and the newbie programmers should not be able
&gt;    to check in code that uses anything off a list of
&gt;    &quot;dangerous or ill-concieved&quot; constructs (meta-macros,
&gt;    data not subject to GC, finalizers that can bring
&gt;    things back to life, call/cc, goto, routines with
&gt;    nonstandard argument evaluation disciplines, etc) until
&gt;    they've had the relevant training, and certain constructs
&gt;    are absolutely forbidden in the &quot;air-traffic control&quot;
&gt;    project (which is subject to a legal requirement of
&gt;    proofs of certain properties such as termination, hard
&gt;    realtime constraints, and finite memory usage).
&gt;    The source control system should include calls to
&gt;    code analysis tools to help enforce these policies.

&gt; * Constructs (possibly including &quot;dangerous or
&gt;    ill-concieved&quot; ones, see above) that can serve as the
&gt;    direct translation target for all or nearly-all extant
&gt;    computer languages. IOW, it should be simple to
&gt;    translate a particular set of routines (perhaps
&gt;    originally written in smalltalk) preserving their
&gt;    call-by-need semantics, and still have them
&gt;    interoperate nicely with the rest of the system,
&gt;    even if mixing call-by-need with call-by-value
&gt;    semantics is considered confusing or dangerous.

&gt; * If we have analysis tools that recognize dangerous or
&gt;    ill-concieved constructs, and means of transforming or
&gt;    eliminating these constructs are known, then we should
&gt;    have code transformation tools that do exactly that,
&gt;    automatically, insofar as possible.  For example,
&gt;    eliminating call/cc by using continuation-passing style.

&gt; * System settings that choose between whole-program
&gt;    optimization, fully separate file-based static
&gt;    compilation, and an interactive runtime.

&gt;                                 Bear
">

a kick butt appserver/webserver
</POST>
<POST>
<POSTER> Joachim Durchholz &lt;j...@durchholz.org&gt; </POSTER>
<POSTDATE> 2008-01-14T07:13:00 </POSTDATE>
Ray Dillinger schrieb:

<QUOTE PREVIOUSPOST="
&gt; Joachim Durchholz wrote:

&gt;&gt; Ray Dillinger schrieb:
&gt;&gt;&gt; My list includes:

&gt;&gt;&gt; * Code analysis tools integrated with source control
&gt;&gt; What's source control?
&gt;&gt; Revision control? In that case: why integrated?

&gt; Yes, revision control.  Integrated to give additional tools
&gt; for code policy enforcement to organizations with policies
&gt; and coding standards.

&gt; Integrated to prevent projects that are supposed to have
&gt; specific coding goals or runtime models from using or depending
&gt; on code that does not.

&gt; Integrated to prevent people from ignoring a requirement of
&gt; code review when using constructs they may or may not understand.
&gt; And so on.
">

I have agree that this is not harmful in itself.
However, it makes pointy-haired decisions easy and does almost nothing
for smart decisions.

Simply make sure that everything is on revision control, with a working
blame functionality. If anybody makes a mistake, it's easy to attribute
it to him, and he won't do it again if proper policies are in place.

One thing where revision control could become smarter is about making
diffs. Most diff programs are smart enough to ignore indentation and
other whitespace differences, but they could improve if they were able
to ignore language-specific irrelevancies (changed code formatting, for
example, or for language like Perl a change from if-then to do-unless).
However, you don't need to integrate revision control into the
environment to do that, a plugin interface for the revision control is
more salient.

There's another thing: if you do your own revision control, your
language will collide with established revision control policies. You'll
have to answer the question what to do with files that belong to the
project but are written in other languages (documentation, XML files,
configuration data etc.).

<QUOTE PREVIOUSPOST="
&gt; Sorry.  That's me using words together that don't mean anything
&gt; together except to me.  I've developed, and have been using,
&gt; this idea as a guide in trying to design a lisp dialect whose
&gt; primitives have very clean design.

&gt; I have a perception that in a language of clean design, most
&gt; specific concepts, ideas, or extensions are associated with
&gt; particular keywords.
">

Actually, I see that concepts and keywords tend to get dissociated.

&gt; For example, if you want to do a project

<QUOTE PREVIOUSPOST="
&gt; using immutable CONS cells (which allows the compiler to do all
&gt; sorts of optimizations it otherwise could not do, pretty much
&gt; including eliminating CONS cells as a distinct entity in memory
&gt; all together) then you need to scan the code for uses of RPLACA
&gt; and RPLACD.  Because nothing else mutates cons cells, these
&gt; keywords control the idea or extension of mutable cons cells.
">

RPLACA and RPLACD may be the only functions that mutate cons cells, but
to optimize, you need to know which functions mutate *any* data.

<QUOTE PREVIOUSPOST="
&gt; Code that doesn't use them (including the transitive closure of
&gt; code it calls) will not violate the constraint against using immutable
&gt; CONS cells.
">

The transitive closure is usually a problem. You usually find that most
functions somehow, somewhere call a mutating operator.
Did you consider non-mutating higher-order functions that get a mutating
function as a parameter?

<QUOTE PREVIOUSPOST="
&gt;&gt; This kind of stuff is better controlled via policy.
&gt;&gt; Software-enforced policy is usually far too rigid to be really useful.

&gt; Speed limits are enforced by policy.  That's no reason to shirk
&gt; the manufacture of mechanical speedometers.   A speedometer can
&gt; tell you when you're in compliance with the policy even in cases
&gt; where you otherwise might have to guess.
">

A speedometer can be useful, yes. However, what you're aiming at is a
device that throttles the car whenever it exceeds the speed limit,
something that isn't installed in cars for excellend reasons.

Regards,
Jo
(please don't restrict followup-tos, I'm not reading c.l.l.)
</POST>
<POST>
<POSTER> Ray Dillinger &lt;b...@sonic.net&gt; </POSTER>
<POSTDATE> 2008-01-14T12:09:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Blaak wrote:
&gt; I stand against this. First of all, if your people are mostly smart, the
&gt; solves most of the problems right there.
">

'deed it does. :-)

<QUOTE PREVIOUSPOST="
&gt; Second, I don't think such tools can be practically written. Fiercly
&gt; difficult, they would be.
">

I'm not seeing it, really.  The fundamental level of difficulty is
in doing the code analysis and proving properties, and your compiler
has to do that anyway if it's going to be any good.  The rest is
a bunch of details, with no particularly crunchy bits to stop you.
Good compilers are fiercely difficult; the rest of this, I believe,
will just take patient and competent work of no particularly great
complexity.

<QUOTE PREVIOUSPOST="
&gt; Forth, it is far more effective and efficient to track things as opposed
&gt; to prevent possible bad things. In a proper revision system you can always
&gt; recover and correct mistakes. Halting commits will be guaranteed to cause
&gt; problems and unexpected delays. If Jack gets hit by a bus, you need to
&gt; still proceed.
">

Generally, the problem is that the modern generation of programmers
has vastly varying levels and areas of core competence.  I think it
makes sense for revision control to be configurable by someone aware
of the differences, so that different programmers are &quot;trusted&quot; for
different things. And yes, where Jack gets hit by a bus you still
have to proceed.  That's when you make your own fork of the project
with different privilege assignments, check in your meta-macro, and
go - with all responsibility for the new branch on your own head.

Also, I think the logical conclusion to revision control is that
it should be possible to mark some branch as &quot;deployed&quot; relative
to a particular server so that check-in to that branch can
instantly changing a running program on a server that faces
tens of thousands of customers.  In that case tracking changes
is not enough, and you have to be pretty careful about who can
check in changes to that particular branch.

<QUOTE PREVIOUSPOST="
&gt; [Followups *not* obeyed, again. Please don't do that. This is a meta level
&gt; discussion, and I submit, interesting to all the groups]
">

Ergh.  Sorry, I've just switched to Knode and it appears to have some kind
of bug where it does not follow-up to all groups unless you specifically
notice them and tell it to by adding your own followup-to line.  I didn't
know it was doing it the first time, forgot to check the second time.  I
will be watching out for it in the future.

Bear
</POST>
<POST>
<POSTER> Ray Blaak &lt;rAYbl...@STRIPCAPStelus.net&gt; </POSTER>
<POSTDATE> 2008-01-14T13:04:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Ray Dillinger &lt;b ... @sonic.net&gt; writes:
&gt; Ray Blaak wrote:
&gt; &gt; Second, I don't think such tools can be practically written. Fiercly
&gt; &gt; difficult, they would be.

&gt; I'm not seeing it, really.  The fundamental level of difficulty is
&gt; in doing the code analysis and proving properties, and your compiler
&gt; has to do that anyway if it's going to be any good.  The rest is
&gt; a bunch of details, with no particularly crunchy bits to stop you.
&gt; Good compilers are fiercely difficult; the rest of this, I believe,
&gt; will just take patient and competent work of no particularly great
&gt; complexity.
">

I should clarify that my objection here is one of practicality. It might very
well be possible to make tools like this. I believe though that they are
simply not worth the effort. I think that you can get the same benefits for
much cheaper simply by having people pay attention, possibly with some (far
easier) tool support to help people pay attention.

--
Cheers,                                        The Rhythm is around me,
The Rhythm has control.
Ray Blaak                                      The Rhythm is inside me,
rAYbl ... @STRIPCAPStelus.net                    The Rhythm has my soul.
</POST>
</TEXT>
</BODY>
</DOC>
