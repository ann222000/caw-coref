<DOC>
<DOCID> eng-NG-31-135589-9749716 </DOCID>
<DOCTYPE SOURCE="usenet"> USENET TEXT </DOCTYPE>
<DATETIME> 2007-10-03T08:21:00 </DATETIME>
<BODY>
<HEADLINE>
APL2007 update
</HEADLINE>
<TEXT>
<POST>
<POSTER> Mike Kent &lt;mk...@acm.org&gt; </POSTER>
<POSTDATE> 2007-10-03T08:21:00 </POSTDATE>
APL 2007 in Montreal (only 2 1/2 weeks away, Oct 20-22).

Summary program  information is now available
on the APL2007 home page

http://www.sigapl.org/apl2007.html

with a link to the comprehensive program description at

http://www.sigapl.org/apl2007-program.html#a2

Registration for APL2007 is at

http://www.regmaster.com/conf/oopsla2007.html
</POST>
<POST>
<POSTER> Beliavsky &lt;beliav...@aol.com&gt; </POSTER>
<POSTDATE> 2007-10-03T08:49:00 </POSTDATE>
I wonder if any APL programmers have tried Fortran 90 or a later
version of the language. F90 introduced array operations into the
language, which is something APL is known for. Considering that
Fortran compilers are more widely available than APL interpreters,
what are the main advantages of the latter that keep some people using
it? What about APL vs. another interpreted language such as Octave of
Python+Numpy?

The repeated announcements of an APL conference in comp.lang.fortran
are what &quot;provoked&quot; me to ask this question.
</POST>
<POST>
<POSTER> Dan Nagle &lt;danna...@verizon.net&gt; </POSTER>
<POSTDATE> 2007-10-03T09:52:00 </POSTDATE>
Hello,

<QUOTE PREVIOUSPOST="
Beliavsky wrote:
&gt; The repeated announcements of an APL conference in comp.lang.fortran
&gt; are what &quot;provoked&quot; me to ask this question.
">

Reading the apl newsgroup would lead one to believe
that a major activity of apl programmers is seeking an apl font.

Otherwise, apl is an interesting language.

--

Dan Nagle
Purple Sage Computing Solutions, Inc.
</POST>
<POST>
<POSTER> &quot;Michael Metcalf&quot; &lt;michaelmetc...@compuserve.com&gt; </POSTER>
<POSTDATE> 2007-10-03T09:53:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
">

news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt;I wonder if any APL programmers have tried Fortran 90 or a later
&gt; version of the language. F90 introduced array operations into the
&gt; language, which is something APL is known for. Considering that
&gt; Fortran compilers are more widely available than APL interpreters,
&gt; what are the main advantages of the latter that keep some people using
&gt; it? What about APL vs. another interpreted language such as Octave of
&gt; Python+Numpy?
">

At the time I briefly used APL, around 1981, its fans claimed that, as an
interpreted language, it could be used very effectively to develop
algorithms. For production purposes, these would then be translated into
Fortran. So, the one did not exclude the other.

Regards,

Mike Metcalf
</POST>
<POST>
<POSTER> glaroc...@cfl.forestry.ca </POSTER>
<POSTDATE> 2007-10-03T14:19:00 </POSTDATE>
On Oct 3, 8:49 am, Beliavsky &lt;beliav ... @aol.com&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt; version of the language. F90 introduced array operations into the
&gt; language, which is something APL is known for. Considering that
&gt; Fortran compilers are more widely available than APL interpreters,
&gt; what are the main advantages of the latter that keep some people using
&gt; it? What about APL vs. another interpreted language such as Octave of
&gt; Python+Numpy?

&gt; The repeated announcements of an APL conference in comp.lang.fortran
&gt; are what &quot;provoked&quot; me to ask this question.
">

The announcement of an APL conference is APL2007, and it is organized
by SIGAPL.
This conference will focus on Array Programming Languages, not only on
the
traditional APL. So users of Fortran 90, J, Octave and Python and
others Array
Programming Languages are more than welcome to come to Montreal and
participate
in the debate. It has been the policy of SIGAPL to promote links among
users
of all Array Programming Languages for several years.

Guy Larocque
SIGAPL Chairman
</POST>
<POST>
<POSTER> neit...@marshlabs.gaertner.de (Martin Neitzel) </POSTER>
<POSTDATE> 2007-10-03T16:42:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
beliav ... @aol.com wrote:
&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt; version of the language.
">

Bob Bernecky did so, and he wrote a fine description why F90 is
still miles away from APL.  I'll dig out the exact APL QQ reference
tomorrow.  (IIRC the main point was that F90 has a rather constrained
set of array operations;  REDUCE exists is limited to a few primitive
arithmetic operations only.)

Martin Neitzel
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-03T18:00:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
">

news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt;I wonder if any APL programmers have tried Fortran 90 or a later
&gt; version of the language. F90 introduced array operations into the
&gt; language, which is something APL is known for. Considering that
&gt; Fortran compilers are more widely available than APL interpreters,
&gt; what are the main advantages of the latter that keep some people using
&gt; it? [ ... snipped]
">

Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
(APL91 - Stanford) in Fortran90. The function was rounding of values to
a given number of decimals (or rather powers of 10) in the left arg, with the
requirement that the sum of the rounded off values should exactly equal the
rounded off sum of the bare values (e.g. as required in financial reports).
Executing the Fortran90 on 10000 values gave me time not only to get
and drink coffee, but even to collect and burn the beans.
That's why I stuck to APL.

An other funny story is that in some project the &quot;programmer's team&quot; wanted
to see Fortran code, while the work was already done in APL by the responsible
engineers satisfactorily (funny? silly? but true!).
The APL-people decided to write an APL code that produced the thing in
Fortran, so, they had their Fortran code (APL Toronto '93 - it's on video tape).
But why? How would you ever write, test and debug 20.000 lines of Fortran?
S.E &amp; O.

Jan Karman
http://www.ganuenta.com
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-04T03:42:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;jk&quot; &lt;*a ... @planet.nl&gt; wrote in message
">

news:4704113d$1$25500$ba620dc5@text.nova.planet.nl ...

<QUOTE PREVIOUSPOST="
&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt; it? [ ... snipped]

&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt; a given number of decimals (or rather powers of 10) in the left arg, with the
&gt; requirement that the sum of the rounded off values should exactly equal the
&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt; and drink coffee, but even to collect and burn the beans.
&gt; That's why I stuck to APL.

&gt; An other funny story is that in some project the &quot;programmer's team&quot; wanted
&gt; to see Fortran code, while the work was already done in APL by the responsible
&gt; engineers satisfactorily (funny? silly? but true!).
&gt; The APL-people decided to write an APL code that produced the thing in
&gt; Fortran, so, they had their Fortran code (APL Toronto '93 - it's on video
&gt; tape).
&gt; But why? How would you ever write, test and debug 20.000 lines of Fortran?
&gt; S.E &amp; O.
">

As Phil Benkard put it that time in his paper 'A Dance of Rounds' litteraly:
&quot;Rounding off is a bit of lying.&quot;
How much are you lying? In the platina grey pre-computer days my actuarial
science courses provided a few simple lessons in numerical analysis on the
topics of 'shorthand multiplication and division'. The aim was to know how much
exactly you were lying in rounding off. E.g. 23.456 x 53.14, in which 53.14 had
been somewhere between 53.135xx and 53.144xx, myriads of those manually
with the comptometer or sumlock, later by electrical machines. Not surprising
that Phil's routine is still one of my favourite subjects - in fact it's a
beauty! I'll remember it &quot;when I'm 94 ...&quot;, because Phil's function did exactly
what I had learned far back in the past.

The fact that the APL routine consisted of say 30 characters (including the
header, line-numbering and both the del's, that is) and the Fortran90 code
needed two pages A4 is, in the meantime, a corny, if not a stale remark.
Reasoning backwards, it's a very instructive excercise to take a sheet of paper
and pencil and construct a table with a real example, following the process in
Phil's APL function (backwards of course, each primitive a column) in order to
see what's happening. This indepth paper &amp; pencil analysis is indispensible
when coding in Fortran(90).
By the way, web-teaching, which is poisoning our students these days, will never
cross this sort of exercises, although they are crucial in learning.

Re the Fortran90 routine: I had to cancel the process at five by &lt;Ctrl&gt;+C.
The APL-function took a few seconds (on a VAX/VMS 750 system in VAX APL).

You might wonder how K is performing in this funtion. In K, you rather talk
about an array of say 500,000 or 6,000,000 items (remember that a middle-sized
life-insurance office is a billion half penny's business).
An always attractive, intriguing, what do I say, seducing property of this sort
of APL-routines is that they often can be mirrored  into K (I'm now talking
about the body of the function). The characters will be different but the
primitive functions are the same. Here is the K-routine:

f:{x*(_ y)+(&lt;&gt;d)&lt;-0.5+/d:(y:y%x)!1}

It's identical to Phil's APL routine, and you even may now reversively mirror to
APL from it, x and y being right and left arg respectively.
The sequence &quot;&lt;&gt;&quot; is the bottleneck for the Fortran code: you'll need the
indices from the grading up and down! They are &quot;wheighing&quot; the values to be
rounded, deciding whether it's going up or down. (Look at the paper with the
analysis).
The Fortran90 SORT routine only gives you the sorted array - who needs it?

I had expected the K-routine to performing sub-second on a (few) million items,
but it's still between one and two on my PC. That figures the tax of the
function to the CPU.

A J-function would probably look a bit different - I'm not familiar enough with
J to provide the routine. Eugene McDonnell will certainly have done so in one of
his &quot;At Play with J&quot;.

Happy Fortranning!!
Jan Karman
http://www.ganuenta.com
</POST>
<POST>
<POSTER> Walter Spector &lt;w6ws_xthiso...@earthlink.net&gt; </POSTER>
<POSTDATE> 2007-10-04T08:57:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
&gt; ...
&gt; The sequence &quot;&lt;&gt;&quot; is the bottleneck for the Fortran code: you'll need the
&gt; indices from the grading up and down! They are &quot;wheighing&quot; the values to be
&gt; rounded, deciding whether it's going up or down. (Look at the paper with the
&gt; analysis).
&gt; The Fortran90 SORT routine only gives you the sorted array - who needs it?
">

I used DEC-10 APLSF a lot back in the 1970s.  Also used the CDC 6000/Cyber
UMASS version some.  Fun language!

Sadly, Fortran does not have a Standardized SORT routine.  Some Fortran
implementations do offer entry points into the qsort(3c) routine.
But these are slow due to the necessary calls to the user-defined
comparison routine.

The High Performance Fortran (HPF) Standard had GRADE_UP and GRADE_DOWN
intrinsics defined.  (I wonder where they got those names from? ;)
HPF 2.0 defined SORT_UP and SORT_DOWN intrinsics.  However HPF is mostly
ignored by vendors these days.  Even the good parts.

W.
</POST>
<POST>
<POSTER> Dan Nagle &lt;danna...@verizon.net&gt; </POSTER>
<POSTDATE> 2007-10-04T09:39:00 </POSTDATE>
Hello,

<QUOTE PREVIOUSPOST="
Walter Spector wrote:
&gt; The High Performance Fortran (HPF) Standard had GRADE_UP and GRADE_DOWN
&gt; intrinsics defined.  (I wonder where they got those names from? ;)
&gt; HPF 2.0 defined SORT_UP and SORT_DOWN intrinsics.  However HPF is mostly
&gt; ignored by vendors these days.  Even the good parts.
">

J3 tried to standardize some of the HPF array intrinsics for f08,
but, IIRC due to differences in the style of the documents,
we were unable to do so in a timely manner.  So the effort
was abandoned.

I can't recall whether the sort or grade routines
were in the set, but ISTR that the prefix and suffix routines
were.

--

Dan Nagle
Purple Sage Computing Solutions, Inc.
</POST>
<POST>
<POSTER> dpb &lt;n...@non.net&gt; </POSTER>
<POSTDATE> 2007-10-04T10:27:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt; it? [ ... snipped]

&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt; a given number of decimals (or rather powers of 10) in the left arg, with the
&gt; requirement that the sum of the rounded off values should exactly equal the
&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt; and drink coffee, but even to collect and burn the beans.
">

...

Accepting it took more source code, it would seem the actual operations
required would end up roughly the same on the same hardware and similar
levels of optimization...what would reasonably say otherwise that APL
can do the same processing in significantly fewer machine operations?

--
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-04T11:08:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt; jk wrote:
&gt;&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt;&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt;&gt; it? [ ... snipped]

&gt;&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt;&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt;&gt; a given number of decimals (or rather powers of 10) in the left arg, with the
&gt;&gt; requirement that the sum of the rounded off values should exactly equal the
&gt;&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt;&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt;&gt; and drink coffee, but even to collect and burn the beans.
&gt; ...

&gt; Accepting it took more source code, it would seem the actual operations
&gt; required would end up roughly the same on the same hardware and similar levels
&gt; of optimization...what would reasonably say otherwise that APL can do the same
&gt; processing in significantly fewer machine operations?
">

This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
in the sorting, the APL-{grade-up} {grade-down}, in other words a different
approach of sorting. In that operation APL gives you the indices for the array
to be sorted, while Fortran gives - by a runtime routine - the sorted array.
Please, see also the later postings.
Sorting is a big thing in computing. Performance may stand or fall with the
sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
algorithm for sorting - the fastest one known at present (I was told by somebody
who is supposed to know).
</POST>
<POST>
<POSTER> Dan Nagle &lt;danna...@verizon.net&gt; </POSTER>
<POSTDATE> 2007-10-04T11:31:00 </POSTDATE>
Hello,

<QUOTE PREVIOUSPOST="
jk wrote:
&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt; approach of sorting. In that operation APL gives you the indices for the array
&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
">

Well, there is no Fortran sort.  So the design
of the external sort routine is determined by the author
of the external sort routine.

Also, the amount of memory traffic (I presume that's the issue
you're implying here) will be the same for sorting the indices,
usually Fortran default integers, as for any other data type
requiring one numeric storage unit.

Perhaps there should be a standard Fortran sort, but as of the present,
there isn't.

--

Dan Nagle
Purple Sage Computing Solutions, Inc.
</POST>
<POST>
<POSTER> dpb &lt;n...@non.net&gt; </POSTER>
<POSTDATE> 2007-10-04T12:43:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
&gt; &quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt;&gt; jk wrote:
&gt;&gt;&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt;&gt;&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt;&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt;&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt;&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt;&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt;&gt;&gt; it? [ ... snipped]
&gt;&gt;&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt;&gt;&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt;&gt;&gt; a given number of decimals (or rather powers of 10) in the left arg, with the
&gt;&gt;&gt; requirement that the sum of the rounded off values should exactly equal the
&gt;&gt;&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt;&gt;&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt;&gt;&gt; and drink coffee, but even to collect and burn the beans.
&gt;&gt; ...

&gt;&gt; Accepting it took more source code, it would seem the actual operations
&gt;&gt; required would end up roughly the same on the same hardware and similar levels
&gt;&gt; of optimization...what would reasonably say otherwise that APL can do the same
&gt;&gt; processing in significantly fewer machine operations?

&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt; approach of sorting. In that operation APL gives you the indices for the array
&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
&gt; Please, see also the later postings.
&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt; algorithm for sorting - the fastest one known at present (I was told by somebody
&gt; who is supposed to know).
">

I would say that is essentially unrelated to the language itself then.
Whatever internal implementation of a SORT intrinsic isn't mandated by
the Fortran Standard.  I don't know enough of APL to know whether it
would be so that all implementations would use the same algorithm or not.

Either way, whether any particular algorithm is optimal for sorting is
dependent on the data and I don't believe there's any single fastest in
a global sense.

But, it does answer the underlying question and the &quot;Say _what_?&quot; factor
of the previous posting, thanks...

--
</POST>
<POST>
<POSTER> nos...@see.signature (Richard Maine) </POSTER>
<POSTDATE> 2007-10-04T12:46:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Dan Nagle &lt;danna ... @verizon.net&gt; wrote:
&gt; jk wrote:

&gt; &gt; I'd say that the bottleneck for Fortran lies
&gt; &gt; in the sorting,...
&gt; Well, there is no Fortran sort.
">

Which is to say that the criticism seems to have nothing to do with
Fortran, but rather with an author who either did not know Fortran or
chose not to use it. Doesn't sound like a very good basis for judgement
to me.

--
Richard Maine                    | Good judgement comes from experience;
email: last name at domain . net | experience comes from bad judgement.
domain: summertriangle           |  -- Mark Twain
</POST>
<POST>
<POSTER> dpb &lt;n...@non.net&gt; </POSTER>
<POSTDATE> 2007-10-04T12:52:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
dpb wrote:
&gt; jk wrote:
&gt;&gt; &quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt;&gt;&gt; jk wrote:
&gt;&gt;&gt;&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt;&gt;&gt;&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;&gt;&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt;&gt;&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt;&gt;&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt;&gt;&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt;&gt;&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt;&gt;&gt;&gt; it? [ ... snipped]
&gt;&gt;&gt;&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt;&gt;&gt;&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt;&gt;&gt;&gt; a given number of decimals (or rather powers of 10) in the left arg,
&gt;&gt;&gt;&gt; with the
&gt;&gt;&gt;&gt; requirement that the sum of the rounded off values should exactly
&gt;&gt;&gt;&gt; equal the
&gt;&gt;&gt;&gt; rounded off sum of the bare values (e.g. as required in financial
&gt;&gt;&gt;&gt; reports).
&gt;&gt;&gt;&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt;&gt;&gt;&gt; and drink coffee, but even to collect and burn the beans.
&gt;&gt;&gt; ...

&gt;&gt;&gt; Accepting it took more source code, it would seem the actual operations
&gt;&gt;&gt; required would end up roughly the same on the same hardware and
&gt;&gt;&gt; similar levels
&gt;&gt;&gt; of optimization...what would reasonably say otherwise that APL can do
&gt;&gt;&gt; the same
&gt;&gt;&gt; processing in significantly fewer machine operations?

&gt;&gt; This is a bit beyond my scope, but I'd say that the bottleneck for
&gt;&gt; Fortran lies
&gt;&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a
&gt;&gt; different
&gt;&gt; approach of sorting. In that operation APL gives you the indices for
&gt;&gt; the array
&gt;&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted
&gt;&gt; array.
&gt;&gt; Please, see also the later postings.
&gt;&gt; Sorting is a big thing in computing. Performance may stand or fall
&gt;&gt; with the
&gt;&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt;&gt; algorithm for sorting - the fastest one known at present (I was told
&gt;&gt; by somebody
&gt;&gt; who is supposed to know).

&gt; I would say that is essentially unrelated to the language itself then.
&gt; Whatever internal implementation of a SORT intrinsic isn't mandated by
&gt; the Fortran Standard.  ...
">

Actually, there isn't a SORT intrinsic at all, what I was thinking of is
a compiler-specific routine that is an extension of the particular
compiler I happen to use most frequently...

--
</POST>
<POST>
<POSTER> Bob Smith &lt;bsm...@sudleydeplacespam.com&gt; </POSTER>
<POSTDATE> 2007-10-04T13:25:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
">

[...]

<QUOTE PREVIOUSPOST="
&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt; algorithm for sorting - the fastest one known at present (I was told by somebody
&gt; who is supposed to know).
">

I'd be surprised if Dyalog uses Hoare's algorithm (also known as
Quicksort) as it is not a stable sort.

--
_________________________________________
Bob Smith -- bsm ... @sudleydeplacespam.com

To reply to me directly, delete &quot;despam&quot;.
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-04T13:24:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe35d5$vtm$1@aioe.org ...
&gt; jk wrote:
&gt;&gt; &quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt;&gt;&gt; jk wrote:
&gt;&gt;&gt;&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt;&gt;&gt;&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;&gt;&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt;&gt;&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt;&gt;&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt;&gt;&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt;&gt;&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt;&gt;&gt;&gt; it? [ ... snipped]
&gt;&gt;&gt;&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt;&gt;&gt;&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt;&gt;&gt;&gt; a given number of decimals (or rather powers of 10) in the left arg, with
&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt; requirement that the sum of the rounded off values should exactly equal the
&gt;&gt;&gt;&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt;&gt;&gt;&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt;&gt;&gt;&gt; and drink coffee, but even to collect and burn the beans.
&gt;&gt;&gt; ...

&gt;&gt;&gt; Accepting it took more source code, it would seem the actual operations
&gt;&gt;&gt; required would end up roughly the same on the same hardware and similar
&gt;&gt;&gt; levels
&gt;&gt;&gt; of optimization...what would reasonably say otherwise that APL can do the
&gt;&gt;&gt; same
&gt;&gt;&gt; processing in significantly fewer machine operations?

&gt;&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran
&gt;&gt; lies
&gt;&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt;&gt; approach of sorting. In that operation APL gives you the indices for the
&gt;&gt; array
&gt;&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
&gt;&gt; Please, see also the later postings.
&gt;&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt;&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt;&gt; algorithm for sorting - the fastest one known at present (I was told by
&gt;&gt; somebody
&gt;&gt; who is supposed to know).

&gt; I would say that is essentially unrelated to the language itself then.
&gt; Whatever internal implementation of a SORT intrinsic isn't mandated by the
&gt; Fortran Standard.  I don't know enough of APL to know whether it would be so
&gt; that all implementations would use the same algorithm or not.

&gt; Either way, whether any particular algorithm is optimal for sorting is
&gt; dependent on the data and I don't believe there's any single fastest in a
&gt; global sense.

&gt; But, it does answer the underlying question and the &quot;Say _what_?&quot; factor of
&gt; the previous posting, thanks...

&gt; --
">

In the Fortran routine I used the external routine ('runtime routine') that came
with the compiler;
APL has it's own {grade-up}, {grade-down} - but we write 199x ? and I haven't
seen anything since. I had other APL-funtions that tumbled on &quot;sorting&quot; (not
really, but when benchmarked). Some authors wrote entire books on sorting
(Flores, Knuth, Lorin and others), so it must be something wothwhile to think of
...
All I can say is that it seems the system was apparantly working and working on
the sorting part. (It reminds me of the paper of Dan King on Arthur Whitney's,
while demo-ing his Kdb, where a manager said: &quot;Well, I can do that too with my
system&quot; - making his machines hang, in the meantime telling that he &quot;really must
go by now&quot; ...
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-04T14:19:00 </POSTDATE>
Bob,

Pete Donnelly told me ... about 10 years ago.

<QUOTE PREVIOUSPOST="
&quot;Bob Smith&quot; &lt;bsm ... @sudleydeplacespam.com&gt; wrote in message
">

news:jm9Ni.18340$B25.11725@news01.roc.ny ...

<QUOTE PREVIOUSPOST="
&gt; jk wrote:
&gt; [...]
&gt;&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt;&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt;&gt; algorithm for sorting - the fastest one known at present (I was told by
&gt;&gt; somebody
&gt;&gt; who is supposed to know).

&gt; I'd be surprised if Dyalog uses Hoare's algorithm (also known as Quicksort) as
&gt; it is not a stable sort.

&gt; --
&gt; _________________________________________
&gt; Bob Smith -- bsm ... @sudleydeplacespam.com

&gt; To reply to me directly, delete &quot;despam&quot;.
">
</POST>
<POST>
<POSTER> glen herrmannsfeldt &lt;g...@ugcs.caltech.edu&gt; </POSTER>
<POSTDATE> 2007-10-04T15:35:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Bob Smith wrote:
">

(snip on APL and sorting)

<QUOTE PREVIOUSPOST="
&gt; I'd be surprised if Dyalog uses Hoare's algorithm (also known as
&gt; Quicksort) as it is not a stable sort.
">

You can always add the original position to the comparison function.
If you are generating the index vector, you start with an array with
the index values in it and rearrange that based on the comparisons.
In the case of a tie, use the index values to break the tie.

-- glen
</POST>
<POST>
<POSTER> se...@panix.com (Seth) </POSTER>
<POSTDATE> 2007-10-04T14:57:00 </POSTDATE>
In article &lt;47050218$0$25474$ba620 ... @text.nova.planet.nl&gt;,

<QUOTE PREVIOUSPOST="
jk &lt;*a ... @planet.nl&gt; wrote:
&gt;&quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt;&gt; Accepting it took more source code, it would seem the actual operations
&gt;&gt; required would end up roughly the same on the same hardware and similar levels
&gt;&gt; of optimization...what would reasonably say otherwise that APL can do the same
&gt;&gt; processing in significantly fewer machine operations?

&gt;This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
&gt;in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt;approach of sorting. In that operation APL gives you the indices for the array
&gt;to be sorted, while Fortran gives - by a runtime routine - the sorted array.
">

Actually, for this underlying problem, you don't need to sort.  It can
be solved in linear time.  (You're looking for the M largest out of N,
which can be done in O(N) comparisons.)

I've seen the algorithm.  I'd rather sort.

Seth
</POST>
<POST>
<POSTER> dpb &lt;n...@non.net&gt; </POSTER>
<POSTDATE> 2007-10-04T15:01:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
&gt; &quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe35d5$vtm$1@aioe.org ...
&gt;&gt; jk wrote:
&gt;&gt;&gt; &quot;dpb&quot; &lt;n ... @non.net&gt; wrote in message news:fe2tf3$739$1@aioe.org ...
&gt;&gt;&gt;&gt; jk wrote:
&gt;&gt;&gt;&gt;&gt; &quot;Beliavsky&quot; &lt;beliav ... @aol.com&gt; wrote in message
&gt;&gt;&gt;&gt;&gt; news:1191415778.387994.323090@n39g2000hsh.googlegroups.com ...
&gt;&gt;&gt;&gt;&gt;&gt; I wonder if any APL programmers have tried Fortran 90 or a later
&gt;&gt;&gt;&gt;&gt;&gt; version of the language. F90 introduced array operations into the
&gt;&gt;&gt;&gt;&gt;&gt; language, which is something APL is known for. Considering that
&gt;&gt;&gt;&gt;&gt;&gt; Fortran compilers are more widely available than APL interpreters,
&gt;&gt;&gt;&gt;&gt;&gt; what are the main advantages of the latter that keep some people using
&gt;&gt;&gt;&gt;&gt;&gt; it? [ ... snipped]
&gt;&gt;&gt;&gt;&gt; Yes, I did. As an exercise I re-coded Phil Benkard's SUMROUND-function
&gt;&gt;&gt;&gt;&gt; (APL91 - Stanford) in Fortran90. The function was rounding of values to
&gt;&gt;&gt;&gt;&gt; a given number of decimals (or rather powers of 10) in the left arg, with
&gt;&gt;&gt;&gt;&gt; the
&gt;&gt;&gt;&gt;&gt; requirement that the sum of the rounded off values should exactly equal the
&gt;&gt;&gt;&gt;&gt; rounded off sum of the bare values (e.g. as required in financial reports).
&gt;&gt;&gt;&gt;&gt; Executing the Fortran90 on 10000 values gave me time not only to get
&gt;&gt;&gt;&gt;&gt; and drink coffee, but even to collect and burn the beans.
&gt;&gt;&gt;&gt; ...

&gt;&gt;&gt;&gt; Accepting it took more source code, it would seem the actual operations
&gt;&gt;&gt;&gt; required would end up roughly the same on the same hardware and similar
&gt;&gt;&gt;&gt; levels
&gt;&gt;&gt;&gt; of optimization...what would reasonably say otherwise that APL can do the
&gt;&gt;&gt;&gt; same
&gt;&gt;&gt;&gt; processing in significantly fewer machine operations?

&gt;&gt;&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran
&gt;&gt;&gt; lies
&gt;&gt;&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt;&gt;&gt; approach of sorting. In that operation APL gives you the indices for the
&gt;&gt;&gt; array
&gt;&gt;&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
&gt;&gt;&gt; Please, see also the later postings.
&gt;&gt;&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt;&gt;&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt;&gt;&gt; algorithm for sorting - the fastest one known at present (I was told by
&gt;&gt;&gt; somebody
&gt;&gt;&gt; who is supposed to know).
&gt;&gt; I would say that is essentially unrelated to the language itself then.
&gt;&gt; Whatever internal implementation of a SORT intrinsic isn't mandated by the
&gt;&gt; Fortran Standard.  I don't know enough of APL to know whether it would be so
&gt;&gt; that all implementations would use the same algorithm or not.

&gt;&gt; Either way, whether any particular algorithm is optimal for sorting is
&gt;&gt; dependent on the data and I don't believe there's any single fastest in a
&gt;&gt; global sense.

&gt;&gt; But, it does answer the underlying question and the &quot;Say _what_?&quot; factor of
&gt;&gt; the previous posting, thanks...

&gt;&gt; --

&gt; In the Fortran routine I used the external routine ('runtime routine') that came
&gt; with the compiler;
&gt; APL has it's own {grade-up}, {grade-down} - but we write 199x ? and I haven't
&gt; seen anything since. I had other APL-funtions that tumbled on &quot;sorting&quot; (not
&gt; really, but when benchmarked). Some authors wrote entire books on sorting
&gt; (Flores, Knuth, Lorin and others), so it must be something wothwhile to think of
&gt; ...
&gt; All I can say is that it seems the system was apparantly working and working on
&gt; the sorting part. ...
">

Well, certainly sorting is a significant problem or why, as you say, are
there entire treatises about it?

But, that a particular implementation of a vendor-specific extension in
one compiler is or isn't suitable for a particular task is nothing
meaningful by which to compare languages.

--
</POST>
<POST>
<POSTER> &quot;James J. Weinkam&quot; &lt;j...@cs.sfu.ca&gt; </POSTER>
<POSTDATE> 2007-10-04T17:59:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
glen herrmannsfeldt wrote:
&gt; Bob Smith wrote:

&gt; (snip on APL and sorting)

&gt;&gt; I'd be surprised if Dyalog uses Hoare's algorithm (also known as
&gt;&gt; Quicksort) as it is not a stable sort.

&gt; You can always add the original position to the comparison function.
&gt; If you are generating the index vector, you start with an array with
&gt; the index values in it and rearrange that based on the comparisons.
&gt; In the case of a tie, use the index values to break the tie.

&gt; -- glen
">

This message has long lines that will display best with line wrap set to 100.

What you say is true, but stability is not the only issue.  Quicksort's worst case performance is
n*2 not n{times}2{log}n; admittedly the worst case isn't very likely to occur and there are defenses
against it but it's still a blemish.

Also Quicksort's legendary performance applies primarily to its application to sorting an array of
numbers which is not the typical use of sorting in serious applications.  The following is from a
post I made to the PL/I newsgroup when a similar topic was discussed there back in 2004:

<QUOTE PREVIOUSPOST="
Mark Yudkin wrote:
">

... 5-1/2 pages giving the Microsoft implementation of Quicksort in C

Wow, 5-1/2 pages just for a quick sort routine and not very carefully checked.

Here is my transliteration of the C function:

* process mar(2,100); /* qsortmv */
qsortmv: proc(a,n,w,c) reorder;
/* jjw2004/06/13 -- Conversion of Microsoft's C qsort function */
/* call qsortmv(a,n,w,c); where a points to an array of n items of width w, */
/* c(p,q) returns -1,0, or 1 based on the relationship between the two items */
dcl
(n,w,s,sz) bin fixed(31),
(a,l,h,m,lg,hg,(sl,sh)(30)) ptr,
c entry(ptr,ptr) returns(bin fixed(31)) options(byvalue);
if n&lt;2|w=0 then return;
s=1; l=a; h=a+w*(n-1);
recurse: sz=(h-l)/w+1;
if sz&lt;=8 then call short(l,h,w,c);
else do;
m=l+(sz/2)*w; call swap(m,l,w);
lg=l; hg=h+w;
do forever;
do forever; lg+=w; if lg&lt;=h then if c(lg,l)&lt;=0 then; else leave; else leave; end;
do forever; hg-=w; if hg&gt;l then if c(hg,l)&gt;=0 then; else leave; else leave; end;
if hg&lt;lg then leave;
call swap(lg,hg,w);
end;
call swap(l,hg,w);
if hg-w-l&gt;=h-hg then do;
if l+w&lt;hg then do; sl(s)=l; sh(s)=hg-w; s+=1; end;
if lg&lt;h then do; l=lg; goto recurse; end;
end;
else do;
if lg&lt;h then do; sl(s)=lg; sh(s)=h; s+=1; end;
if l+w&lt;hg then do; h=hg-w; goto recurse; end;
end;
end;
s-=1; if s&gt;0 then do; l=sl(s); h=sh(s); goto recurse; end;
short: proc(l,h,w,c) options(byvalue);
dcl (l,h,p,m) ptr, w bin fixed(31),
c entry(ptr,ptr) returns(bin fixed(31)) options(byvalue);
do while(h&gt;l); m=l;
do p=l+w repeat(p+w) while(p&lt;=h); if c(p,m)&gt;0 then m=p; end;
call swap(m,h,w); h-=w;
end;
end short;
swap: proc(a,b,w) options(byvalue);
dcl (a,b) ptr, w bin fixed(31), (c based,t) char(1);
if a~=b then do while(w&gt;0); t=a-&gt;c; a-&gt;c=b-&gt;c; b-&gt;c=t; a+=1; b+=1; w-=1; end;
end swap;
end qsortmv;

The real question is how good is this as a general internal sort?

Another question is why is nearly everyone so enamored with Quicksort?

Certainly it is an elegant algorithm.  Its average performance is also faster
than most other methods, at least in the textbook setting of sorting an array
of integers.

On the other hand it has some serious drawbacks:  Its worst case performance is
O(n**2) and it is not stable.  (A sorting method is stable if it does not
permute items that compare equal.)

The reason that it performs so well relative to other methods when sorting an
array of integers is that comparison of two integers and movement of an integer
from one array element to another both take similar amounts of time to that of other
integer operations used in subscript manipulation.  When this is coupled with the
fact that Quicksort performs very few subscript manipulations compared to other
sorting methods, the reason why it outperforms other methods when sorting an
integer array, even though the other methods make fewer comparisons and/or data
movements becomes clear.

However most real applications are sorting items which can be thought of as
consisting of two parts: a KEY upon which the comparison of items depends and the
associated DATA.  In many if not most cases, the total size of an item is
considerably larger than an integer and the algorithm to compare two KEYs is
more complex than comparing two integers or reals.

To demonstrate the effect of these considerations, I prepared two arrays and
applied several sorting routines to each of them.

The first array consists of 1000000 random bin fixed(31) integers uniformly
distributed in the interval (0,2147483647).  For this array, the comparison
criterion is a simple integer comparison.

The second array consists of 1000000 80 byte items.  Each item consists of three
bin fixed(31) integers followed by 68 bytes of text.  The three integers are
independent random variables.  The first two are in the interval [0,99] and the
third is in the interval [0,99999].  For this array, the comparison criterion
is lexicographic ordering on the three integers in left to right order.

The sorting algorithms compared are:

qsortmv: The transliteration of Microsoft's C qsort shown above using byvalue;
qsortmr: The same routine, but using call by reference;
qsortmp: Same as qsortmr but replacing the loop in swap with a call to PLIMOVE;

qsorta: My own implementation, modified to sort the array instead of an array of
pointers;

sorta: My preferred array sorter which uses an auxiliary link array to sort using
binary merge, then rearranges the array in place;

sortl: My binary merge list sorter applied to linked lists containing the same data.

Since no i/o is involved, timing of the sorts is very reproducible.  The times do not
vary by more than +- .01 sec.  All tests were run on a Thinkpad 600 with no other
applications running.

The following table summarizes the timing results and also shows the number of key
comparisons (C) and data movements (M) performed by each routine. For qsorta, the
theoretical average number of comparisons (A) is shown. For sorta, the theoretical
maximum number of comparisons (X) is shown; this value also applies to sortl.  Finally
the number of fixed elements (F) and number of cycles (Y) of the stable permutation are
shown; these values apply to sorta and sortl but not necessarily to the Quicksort
methods since they are unstable and find a different permutation in general unless
there are no equal elements.

4 byte elements     80 byte elements

qsortmv: T          3.63                 19.86
qsortmr: T          4.30                 17.96
qsortmp: T          5.60                 13.47
C          25603868             25773956
M          14723739             14714256

qsorta   T          3.57                 11.15
C          24760597             25390212
M          10060710             10039911
A                     25505599

sorta    T          7.34                 10.33
C          18715816             18715915
M          1000011              1000015
X                     18931570

sortl    T          5.16                 5.91

F          1                    0
Y          12                   15

Discussion:  Comparison of the times for sortl shows that the more complex comparison has minimal
effect.  On the other hand, increasing the item size to a realistic value has a drastic effect on
the relative performance of the various routines.  Call by value is faster than call by reference
for one word items, whereas call by reference is faster for 80 byte items. For four byte items, the
calling overhead of PLIMOVE leads to an increased running time of about 1.3 sec while for 80 byte
items there is a 4.5 sec decrease.  qsorta is uniformly better than the more involved implementation
based on the Microsoft routine even though it only treats size 2 as a special case.  Even though
sorta does far more subscript arithmetic than any of the Quicksort versions, the smaller number of
key comparisons and greatly reduced number of data moves makes it the clear winner among the array
sorters for the 80 byte data.  If an an array is not necessary, sortl is the method of choice, since
it eliminates data movement entirely.
</POST>
<POST>
<POSTER> glen herrmannsfeldt &lt;g...@ugcs.caltech.edu&gt; </POSTER>
<POSTDATE> 2007-10-04T19:31:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
James J. Weinkam wrote:
">

(snip regarding non-stable quicksort, I wrote)

<QUOTE PREVIOUSPOST="
&gt;&gt; You can always add the original position to the comparison function.
&gt;&gt; If you are generating the index vector, you start with an array with
&gt;&gt; the index values in it and rearrange that based on the comparisons.
&gt;&gt; In the case of a tie, use the index values to break the tie.
&gt; What you say is true, but stability is not the only issue.  Quicksort's
&gt; worst case performance is n*2 not n{times}2{log}n; admittedly the worst
&gt; case isn't very likely to occur and there are defenses against it but
&gt; it's still a blemish.
">

It is, especially since some of the slow cases occurs when the array
is already in order, or close to being in order.

<QUOTE PREVIOUSPOST="
&gt; Also Quicksort's legendary performance applies primarily to its
&gt; application to sorting an array of numbers which is not the typical use
&gt; of sorting in serious applications.  The following is from a post I made
&gt; to the PL/I newsgroup when a similar topic was discussed there back in
&gt; 2004:
">

(snip)

<QUOTE PREVIOUSPOST="
&gt; Another question is why is nearly everyone so enamored with Quicksort?
&gt; Certainly it is an elegant algorithm.  Its average performance
">

&gt; is also faster than most other methods, at least in the textbook
&gt; setting of sorting an array of integers.
(snip)

<QUOTE PREVIOUSPOST="
&gt; The reason that it performs so well relative to other methods
&gt; when sorting an array of integers is that comparison of two integers
">

&gt; and movement of an integer from one array element to another both
&gt; take similar amounts of time to that of other integer operations
&gt; used in subscript manipulation.  When this is coupled with the

<QUOTE PREVIOUSPOST="
&gt; fact that Quicksort performs very few subscript manipulations
">

&gt; compared to other sorting methods, the reason why it outperforms
&gt; other methods when sorting an integer array, even though the other

<QUOTE PREVIOUSPOST="
&gt; methods make fewer comparisons and/or data movements becomes clear.
">

For internal sort there is also the cache to consider.  I don't know
how much effect that has on quicksort, though.

<QUOTE PREVIOUSPOST="
&gt; However most real applications are sorting items which can be thought
&gt; of as consisting of two parts: a KEY upon which the comparison of items
&gt; depends and the associated DATA.  In many if not most cases, the total
&gt; size of an item is considerably larger than an integer and the
&gt; algorithm to compare two KEYs is more complex than comparing two
&gt; integers or reals.
">

That is true.  When I learned about sort algorithms there were three
fast sorts: Quicksort, Shellsort, and Heapsort.  Actually, I don't
know so well the details on comparisons vs. data movement for the
three.  All three are considered NlogN typical if not worst case.

For C's qsort (which, despite the name, isn't necessarily
quicksort) many people sort a list of pointers instead of the data
array itself.  Reasonably often I sort the data array, but usually
it is small enough that I am not really worried about the time.

For the case that started this thread, though, it was a list of
indexes into the array.  In that case, the data movement is the
same as for an integer array and it is easy to make a stable
sort.

(big snip)

-- glen
</POST>
<POST>
<POSTER> &quot;Dr Ivan D. Reid&quot; &lt;Ivan.R...@brunel.ac.uk&gt; </POSTER>
<POSTDATE> 2007-10-04T18:56:00 </POSTDATE>
On Thu, 04 Oct 2007 21:59:29 GMT, James J. Weinkam &lt;j ... @cs.sfu.ca&gt;
wrote in &lt;5ndNi.117826$Pd4.1744@edtnps82&gt;:

<QUOTE PREVIOUSPOST="
&gt; This message has long lines that will display best with line wrap set to 100.
">

Why?  I adjust my text windows for height, rarely for width.  80
columns has been &quot;standard&quot; width for decades now.  Assuming anything else is
arrogance of the first water!

--
Ivan Reid, School of Engineering &amp; Design, _____________  CMS Collaboration,
Brunel University.    Ivan.Reid@[brunel.ac.uk|cern.ch]    Room 40-1-B12, CERN
KotPT -- &quot;for stupidity above and beyond the call of duty&quot;.
</POST>
<POST>
<POSTER> wclod...@lanl.gov </POSTER>
<POSTDATE> 2007-10-04T19:26:00 </POSTDATE>
On Oct 4, 9:08 am, &quot;jk&quot; &lt;*a ... @planet.nl&gt; wrote:

<QUOTE PREVIOUSPOST="
&gt; &lt;snip&gt;

&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt; approach of sorting. In that operation APL gives you the indices for the array
&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
&gt; Please, see also the later postings.
&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt; algorithm for sorting - the fastest one known at present (I was told by somebody
&gt; who is supposed to know).
">

I would hope that Dyalog doesn't use Quicksort. FWIW the somebody
doesn't know.

For finite (machine) integers (and I believe also floats) there are
sorting algorithms of order 0(n). Quicksort's average sorting for
randomly distributed orders of values in the array is 0(n log(n)).
However Quicksort's worst performance (0(n^2)) occurs for arrays that
are already sorted (or almost sorted), and in practice those occur far
more often than would be predicted for random distributions. Quicksort
should not be used on arrays of machine integers or floating point
numbers.

For more complicated types (e.g. character strings, bignums, etc.)
where 0(n) sorting is not practical, the most commonly used
competitors to Quicksort are Heapsort and Mergesort. All three have
&quot;average&quot; 0(n log(n)) performance. Worst case for Heapsort and
Mergesort remains 0(n log(n)) as opposed to Quicksort's (n^2). Note
that Quicksort and Heapsort are largely sort in place algorithms, but
Mergesort can need significantly (~2-3*) more memory. Mergesort
(unlike Quicksort and Heapsort) is stable (preserves relative order of
identical items). The relative performance of these algorithms depends
on how the data is stored (array, linked list, tree), and the relative
costs of comparisons versus memory moves. Modifications of all three
algorithms can change the coefficients, often minimizing memory moves.
Most analyses (and tests) use (comparison and memory) operations
appropriate for machine integers (where the 0(n) methods are more
appropriate), on randomly ordered data. In these tests Quicksort is
usually faster than Heapsort, but slightly slower than Mergesort. If
either stability or performance is more important than minimizing
memory usage, (a sophisticated) Mergesort is to be preferred, if
minimizing memory usage is more important and if Quicksort's worst
case is expected to be uncommon Quicksort is to be preferred,
otherwise Heapsort is to be preferred.
</POST>
<POST>
<POSTER> Harold Stevens &lt;woo...@aces.localdomain&gt; </POSTER>
<POSTDATE> 2007-10-05T09:39:00 </POSTDATE>
[&quot;Followup-To:&quot; header set to comp.lang.fortran.]

In &lt;slrnfgarsm.b3s.Ivan.R ... @loki.brunel.ac.uk&gt; Dr Ivan D. Reid:

[Snip...]

<QUOTE PREVIOUSPOST="
&gt; 80 columns has been &quot;standard&quot; width for decades now
">

Indeed. FWIW...

I rarely read any post(er) with runon lines past 80 columns. Already quite
enough noise without that oblivious &quot;style&quot; intruding as well.

JMO; YMMV...

--
Regards, Weird (Harold Stevens) * IMPORTANT EMAIL INFO FOLLOWS *
Pardon any bogus email addresses (wookie) in place for spambots.
Really, it's (wyrd) at airmail, dotted with net. DO NOT SPAM IT.
Kids jumping ship? Looking to hire an old-school type? Email me.
</POST>
<POST>
<POSTER> &quot;Jeff Reid&quot; &lt;jeffar...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-10-05T21:45:00 </POSTDATE>
There is a forum thread about sorting:

http://www.physicsforums.com/showthread.php?t=181589

Pros and cons of sort algorithms:

quicksort - O(n log n) best / average time, worst case time of O(n^2) is rare.
Depends on how well the pivot points are chosen. Doesn't preserve the order of
&quot;equal&quot; elements. Sorts the data in place, but has significant recursion
overhead.

http://en.wikipedia.org/wiki/Quicksort

mergesort - O(n log n) average / worst case time. The wiki article for merge
sort is not good, it describes a recursvie &quot;top down&quot; algorithm, while a
typical merge sort is a non-recursive, looping &quot;bottom up&quot; algorithm. Merge
sorts are two phased. An initial pass to create groups of sorted data, then the
groups are merged until group size is equal to size of data to be sorted.
During the initial pass, a &quot;natural&quot; merge sort creates variable size groups,
based on the length of sequences of ascending or descending data. For
descending data, the data is reversed on the first pass, or dealt with in the
first merge pass. The base case time for a &quot;natural&quot; merge sort is O(n-1) on a
pre-sorted file, since the initial pass will create a single group, and the
merge pass will do nothing. The other basic type of merge sort uses fixed sized
groups. In my merge sort in &quot;fixed&quot; mode, I create groups of two elements,
swapping as needed. In Micorosoft's stable_sort, insertion sort is used to
create groups of 32. A merge sort requires a second memory area to store data or
pointers, so it's memory usage is double that of a quicksort, but using pointers
reduces this overhead. Also a merge sort always moves data for each pass, while
a quicksort only swaps data if needed. A merge sort will have fewer compares
than a quicksort. On truly random data, the time for both quicksort and merge
sort is close. If the data has significant pre-ordering, then the &quot;natural&quot;
merge sort is the fastest.

radix sort - If you have a lot of memory, and element size is not large, then
this can be fast. Similar to an old card sorter, read the data and place the
data into buckets, based on the least significant sub-element field (byte,
digit, ...). Merge the buckets and repeat for the next least significant sub-
element field. Repeat until most significant sub-element field is done. For
example, one of my test files is 8 digits of data (from pi), cr, lf. Split the
data up into 10 buckets based on the 8th digit, then repeath for the 7th digit,
... until the 1st digit is done. This requires 8 passes of the data, so in this
case time is O(n*8). With 256 buckets, a radix sort on 64 bit elements would
also take 8 passes.

Actual source code and results:

The source code referenced below can be compiled and tested using Microsoft's
Visual Studio Express, which is free:

The version I downloaded was one that I could burn to a cd-rom then install
from the cd-rom. This appears to have changed. Select Visual Studio C++ express
to setup.

Array of 64 bit &quot;random&quot; number sorts:

http://jeffareid.net/misc/sortv.zip

Text file sort (pointer to data) sorts:
src1.txt is a 1048576 record file, each record is 8 digits of pi, cr, lf.
src2.txt is a 1048576 record file, each record is assembly code, cr, lf.

http://jeffareid.net/misc/sortp.zip

I compared the following sort algorithms:

Microsoft qsort() from the C library - abandoned, it was relatively slow.

Microsoft sort() from Visual Studio Standard Template Library, which isn't a
library but actaully code that gets included with the programmers code at
compile time. The sort() function uses the &quot;vector&quot; template, but a &quot;vector&quot;
template is just an array with some class functions. This is a quicksort that
switches to a heap sort if the recursion gets too high. I didn't investigate
as to how it picks good pivot points (probably psuedo random).

Microsoft stable_sort() from Visual Studio Standard Template Library. This is a
merge sort, but uses an insertion sort on the first pass to create groups of 32
elements.

My own merge sort, with fixed sized groups and variable sized groups. The
variable sized groups are generated during the initial pass, looking for
sequences of ascending or descending data, and forming groups from these
sequences, reversing the data or pointers for descending groups.

Results: using 64hz ticker for timing:

sorting 2^22 (4 million) 64 bit numbers, 64 bit mode, internal compare:
Microsoft sort()        0.453seconds
My merge sort()         0.531 seconds
Microsoft stable_sort() 0.546 seconds

sorting 2^22 (4 million) 64 bit numbers, 32 bit mode, internal compare:
Microsoft sort()        0.547 seconds
My merge sort()         0.578 seconds
Microsoft stable_sort() 0.609 seconds

When sorting numbers (64 bit), using the internal compare, Microsoft sort()
(quicksort) was the fastest, while my merge sort and Microsoft stable_sort()
were about 17% slower in 64 bit mode. The numbers were &quot;random&quot;, using the
rand() function bits 4-&gt;11 for each byte of data.

sorting 2^22 (4 million) 64 bit numbers, 32 bit mode, user defined compare:
My merge sort()         1.109 seconds
Microsoft stable_sort() 1.313 seconds
Microsoft sort()        1.422 seconds

A user defined compare slows the Micorsoft routines slowed dramatically. sort()
became the slowest, probably because the pointer to function is being passed
with all the recursive calls. I haven't tried modifying the code to use a
global ptr to function instead. My merge sort was the fastest, because it uses
no recursion. The Microsoft stable_sort was also significantly slower, probably
because it calls internal functions to merge pairs of groups, passing ptr to
function, while my code is just a single loop for the merge part.

The remaining sorts work on pointers which requires a user defined compare,
and done 32 bit mode only. (64 bit mode using 64 bit pointers for the
memcmp() function ends up being slower).

sorting 2^20 (1 million) record pointers (src1):
My merge sort()         0.422 seconds
Microsoft stable_sort() 0.500 seconds
Microsoft sort()        0.687 seconds

sorting 2^20 (1 million) record pointers (src2):
My merge sort() fix     0.516 seconds
My merge sort() var     0.531 seconds
Microsoft stable_sort() 0.594 seconds
Microsoft sort()        0.781 seconds

sorting 2^20 (1 million) record pointers (sorted src1):
My merge sort() var     0.015 seconds
My merge sort() fix     0.110 seconds
Microsoft stable_sort() 0.125 seconds
Microsoft sort()        0.140 seconds
</POST>
<POST>
<POSTER> &quot;Jeff Reid&quot; &lt;jeffar...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-10-06T02:01:00 </POSTDATE>
After my long post, consider the fact that it took less than 1
second to sort 4 million (2^22) 64 bit values directly, and less
than 1 second to sort records via pointer (indexes could be
used instead of pointers to the the equivalent of APL's sort),
does it really matter which algorithm is used, as long as it's
reasonably fast?

I forgot to mention that my system has an Intel Core Two X6800
cpu (2.93ghz), running on an Intel D975XBX motherboard with 2GB
of 4-4-4-10 667mhz ddr2 ram.
</POST>
<POST>
<POSTER> &quot;jk&quot; &lt;*a...@planet.nl&gt; </POSTER>
<POSTDATE> 2007-10-06T03:29:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Jeff Reid&quot; &lt;jeffar ... @hotmail.com&gt; wrote in message
">

news:RxFNi.157537$dI1.109541@newsfe08.phx ...

<QUOTE PREVIOUSPOST="
&gt; After my long post, consider the fact that it took less than 1
&gt; second to sort 4 million (2^22) 64 bit values directly, and less
&gt; than 1 second to sort records via pointer (indexes could be
&gt; used instead of pointers to the the equivalent of APL's sort),
&gt; does it really matter which algorithm is used, as long as it's
&gt; reasonably fast?
">

[...snipped]

no, not at all, and besides you don't need to be a mechanic to
drive fluently a porsche.
thanks Jeff for your comprehensive exposé!
</POST>
<POST>
<POSTER> Gary Scott &lt;garylsc...@sbcglobal.net&gt; </POSTER>
<POSTDATE> 2007-10-06T11:05:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
jk wrote:
&gt; &quot;Jeff Reid&quot; &lt;jeffar ... @hotmail.com&gt; wrote in message
&gt; news:RxFNi.157537$dI1.109541@newsfe08.phx ...

&gt;&gt;After my long post, consider the fact that it took less than 1
&gt;&gt;second to sort 4 million (2^22) 64 bit values directly, and less
&gt;&gt;than 1 second to sort records via pointer (indexes could be
&gt;&gt;used instead of pointers to the the equivalent of APL's sort),
&gt;&gt;does it really matter which algorithm is used, as long as it's
&gt;&gt;reasonably fast?
">

Not other than issues of style, maintainability, and the usual
discouragement of pointers where possible.

<QUOTE PREVIOUSPOST="
&gt; [...snipped]

&gt; no, not at all, and besides you don't need to be a mechanic to
&gt; drive fluently a porsche.
&gt; thanks Jeff for your comprehensive exposé!
">

--

Gary Scott
mailto:garylscott@sbcglobal dot net

Fortran Library: http://www.fortranlib.com

Support the Original G95 Project: http://www.g95.org
-OR-
Support the GNU GFortran Project: http://gcc.gnu.org/fortran/index.html

If you want to do the impossible, don't hire an expert because he knows
it can't be done.

-- Henry Ford
</POST>
<POST>
<POSTER> Mike Kent &lt;mk...@acm.org&gt; </POSTER>
<POSTDATE> 2007-10-06T14:45:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Bob Smith wrote:
&gt; I'd be surprised if Dyalog uses Hoare's algorithm (also known as
&gt; Quicksort) as it is not a stable sort.
">

Non-stable sorts /are/ stable if all the items in the
to-be-sorted data are distinct -- so, a fast but non-stable
/sort/ can underly a fast and stable /grade/ (by doing a
lexicographic sort on pairs (datum . index)).

In a grade algorithm the extra work need not be /very/
onerous since

(a) you are permuting the indices so they are easily
at hand already

(b) you only have to do the comparison of original
indices in the case where the data values match

For the case of &quot;enough&quot; things to grade, the balance may
favor fancy efficient non-stable sorts over less efficient
but stable sorts.

Engineering questions that might be asked include

how many is enough?

how dreadful is the penalty for smaller cases
(is it small enough to be neglected)?

is there a crossover point a where you get
back most of the penalty in the not-so-many
cases, and if so where (not forgetting
the cost of the size check)?

what's the arg size distribution likely to
be seen in use?

IAC Dyalog use some kind of modern sort or so I was told
once by Pete or, more likely, someone from their interpreter
implementation team .
</POST>
<POST>
<POSTER> &quot;Jeff Reid&quot; &lt;jeffar...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-10-06T16:12:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt;&gt;&gt;After my long post, consider the fact that it took less than 1
&gt;&gt;&gt;second to sort 4 million (2^22) 64 bit values directly, and less
&gt;&gt;&gt;than 1 second to sort records via pointer (indexes could be
&gt;&gt;&gt;used instead of pointers to the the equivalent of APL's sort),
&gt;&gt;&gt;does it really matter which algorithm is used, as long as it's
&gt;&gt;&gt;reasonably fast?

&gt; Not other than issues of style, maintainability, and the usual discouragement of pointers where possible.
">

All of the pointers could be replaced by indexes to arrays, which
would be desired for usage in APL.
</POST>
<POST>
<POSTER> TC &lt;tclviii...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-10-07T11:09:00 </POSTDATE>
On Oct 4, 2:57 pm, se ... @panix.com (Seth) wrote:

<QUOTE PREVIOUSPOST="
&gt; Actually, for this underlying problem, you don't need to sort.  It can
&gt; be solved in linear time.  (You're looking for the M largest out of N,
&gt; which can be done in O(N) comparisons.)

&gt; I've seen the algorithm.  I'd rather sort.

&gt; Seth
">

actually, that non sorting algorithm might be pretty useful for me, as
I've got a case where I'm scanning across a 9.6 million row by 4
column matrix and sorting that might take too long [sub 200
milliseconds] . . . which if i can be clever about that tail-recursion-
reuses-the-stack feature of dyalog i might be able to achieve

could you point me to a citation for that algorithm
</POST>
<POST>
<POSTER> &quot;James Van Buskirk&quot; &lt;not_va...@comcast.net&gt; </POSTER>
<POSTDATE> 2007-10-07T11:28:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;TC&quot; &lt;tclviii ... @yahoo.com&gt; wrote in message
">

news:1191769751.425676.279460@o3g2000hsb.googlegroups.com ...

<QUOTE PREVIOUSPOST="
&gt; On Oct 4, 2:57 pm, se ... @panix.com (Seth) wrote:
&gt;&gt; Actually, for this underlying problem, you don't need to sort.  It can
&gt;&gt; be solved in linear time.  (You're looking for the M largest out of N,
&gt;&gt; which can be done in O(N) comparisons.)
&gt;&gt; I've seen the algorithm.  I'd rather sort.
&gt; actually, that non sorting algorithm might be pretty useful for me, as
&gt; I've got a case where I'm scanning across a 9.6 million row by 4
&gt; column matrix and sorting that might take too long [sub 200
&gt; milliseconds] . . . which if i can be clever about that tail-recursion-
&gt; reuses-the-stack feature of dyalog i might be able to achieve
&gt; could you point me to a citation for that algorithm
">

http://home.comcast.net/~kmbtib/Fortran_stuff/order_stat.i90
http://home.comcast.net/~kmbtib/Fortran_stuff/order_stat_test.f90

--
write(*,*) transfer(0.64682312090346863D-153,(/'X'/));end
</POST>
<POST>
<POSTER> phil chastney &lt;phil.hates.s...@amadeus.munged.eclipse.co.uk&gt; </POSTER>
<POSTDATE> 2007-10-08T03:25:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Gary Scott wrote:
&gt; jk wrote:
&gt;&gt; &quot;Jeff Reid&quot; &lt;jeffar ... @hotmail.com&gt; wrote in message
&gt;&gt; news:RxFNi.157537$dI1.109541@newsfe08.phx ...

&gt;&gt;&gt; After my long post, consider the fact that it took less than 1
&gt;&gt;&gt; second to sort 4 million (2^22) 64 bit values directly, and less
&gt;&gt;&gt; than 1 second to sort records via pointer (indexes could be
&gt;&gt;&gt; used instead of pointers to the the equivalent of APL's sort),
&gt;&gt;&gt; does it really matter which algorithm is used, as long as it's
&gt;&gt;&gt; reasonably fast?

&gt; Not other than issues of style, maintainability, and the usual
&gt; discouragement of pointers where possible.
">

isn't that a bit of a sweeping statement?

I thought the usual objection was to the use of pointers in high-level
languages, with more-or-less the reverse being true at assembler level

/phil
</POST>
<POST>
<POSTER> phil chastney &lt;phil.hates.s...@amadeus.munged.eclipse.co.uk&gt; </POSTER>
<POSTDATE> 2007-10-08T03:30:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
wclod ... @lanl.gov wrote:
&gt; On Oct 4, 9:08 am, &quot;jk&quot; &lt;*a ... @planet.nl&gt; wrote:
&gt;&gt; &lt;snip&gt;

&gt;&gt; This is a bit beyond my scope, but I'd say that the bottleneck for Fortran lies
&gt;&gt; in the sorting, the APL-{grade-up} {grade-down}, in other words a different
&gt;&gt; approach of sorting. In that operation APL gives you the indices for the array
&gt;&gt; to be sorted, while Fortran gives - by a runtime routine - the sorted array.
&gt;&gt; Please, see also the later postings.
&gt;&gt; Sorting is a big thing in computing. Performance may stand or fall with the
&gt;&gt; sorting algorithm. As far as I know, e.g. Dyalog uses Hoare's
&gt;&gt; algorithm for sorting - the fastest one known at present (I was told by somebody
&gt;&gt; who is supposed to know).

&gt; I would hope that Dyalog doesn't use Quicksort. FWIW the somebody
&gt; doesn't know.

&gt; For finite (machine) integers (and I believe also floats) there are
&gt; sorting algorithms of order 0(n). Quicksort's average sorting for
&gt; randomly distributed orders of values in the array is 0(n log(n)).
&gt; However Quicksort's worst performance (0(n^2)) occurs for arrays that
&gt; are already sorted (or almost sorted), and in practice those occur far
&gt; more often than would be predicted for random distributions. Quicksort
&gt; should not be used on arrays of machine integers or floating point
&gt; numbers.

&gt; For more complicated types (e.g. character strings, bignums, etc.)
&gt; where 0(n) sorting is not practical, the most commonly used
&gt; competitors to Quicksort are Heapsort and Mergesort. All three have
&gt; &quot;average&quot; 0(n log(n)) performance. Worst case for Heapsort and
&gt; Mergesort remains 0(n log(n)) as opposed to Quicksort's (n^2). Note
&gt; that Quicksort and Heapsort are largely sort in place algorithms, but
&gt; Mergesort can need significantly (~2-3*) more memory. Mergesort
&gt; (unlike Quicksort and Heapsort) is stable (preserves relative order of
&gt; identical items). The relative performance of these algorithms depends
&gt; on how the data is stored (array, linked list, tree), and the relative
&gt; costs of comparisons versus memory moves. Modifications of all three
&gt; algorithms can change the coefficients, often minimizing memory moves.
&gt; Most analyses (and tests) use (comparison and memory) operations
&gt; appropriate for machine integers (where the 0(n) methods are more
&gt; appropriate), on randomly ordered data. In these tests Quicksort is
&gt; usually faster than Heapsort, but slightly slower than Mergesort. If
&gt; either stability or performance is more important than minimizing
&gt; memory usage, (a sophisticated) Mergesort is to be preferred, if
&gt; minimizing memory usage is more important and if Quicksort's worst
&gt; case is expected to be uncommon Quicksort is to be preferred,
&gt; otherwise Heapsort is to be preferred.
">

FWIW, my experience is that the execution times of internal sort
routines is rarely a significant factor in the overall timing

. . . unless, of course, I had to write and debug the damn thing
before I could use it

/phil
</POST>
<POST>
<POSTER> Gary Scott &lt;garylsc...@sbcglobal.net&gt; </POSTER>
<POSTDATE> 2007-10-08T08:46:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
phil chastney wrote:
&gt; Gary Scott wrote:

&gt;&gt; jk wrote:

&gt;&gt;&gt; &quot;Jeff Reid&quot; &lt;jeffar ... @hotmail.com&gt; wrote in message
&gt;&gt;&gt; news:RxFNi.157537$dI1.109541@newsfe08.phx ...

&gt;&gt;&gt;&gt; After my long post, consider the fact that it took less than 1
&gt;&gt;&gt;&gt; second to sort 4 million (2^22) 64 bit values directly, and less
&gt;&gt;&gt;&gt; than 1 second to sort records via pointer (indexes could be
&gt;&gt;&gt;&gt; used instead of pointers to the the equivalent of APL's sort),
&gt;&gt;&gt;&gt; does it really matter which algorithm is used, as long as it's
&gt;&gt;&gt;&gt; reasonably fast?

&gt;&gt; Not other than issues of style, maintainability, and the usual
&gt;&gt; discouragement of pointers where possible.

&gt; isn't that a bit of a sweeping statement?

&gt; I thought the usual objection was to the use of pointers in high-level
&gt; languages, with more-or-less the reverse being true at assembler level
">

well, we were talking about a high-level language.

<QUOTE PREVIOUSPOST="
&gt; /phil
">

--

Gary Scott
mailto:garylscott@sbcglobal dot net

Fortran Library: http://www.fortranlib.com

Support the Original G95 Project: http://www.g95.org
-OR-
Support the GNU GFortran Project: http://gcc.gnu.org/fortran/index.html

If you want to do the impossible, don't hire an expert because he knows
it can't be done.

-- Henry Ford
</POST>
<POST>
<POSTER> &quot;Jeff Reid&quot; &lt;jeffar...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-10-08T22:40:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt;&gt;&gt;&gt; After my long post, consider the fact that it took less than 1
&gt;&gt;&gt;&gt; second to sort 4 million (2^22) 64 bit values directly, and less
&gt;&gt;&gt;&gt; than 1 second to sort records via pointer (indexes could be
&gt;&gt;&gt;&gt; used instead of pointers to the the equivalent of APL's sort),
&gt;&gt;&gt;&gt; does it really matter which algorithm is used, as long as it's
&gt;&gt;&gt;&gt; reasonably fast?

&gt;&gt; Not other than issues of style, maintainability, and the usual discouragement of pointers where possible.

&gt; isn't that a bit of a sweeping statement?
">

In the case of C and C++, pointers are used for every instance of
an object allocated via malloc or new.  If code uses an index for
an allocated object, then this consumes two cpu registers, but using
a pointer reduces this to just consumption of a single cpu register,
so there is an advantage to using pointers.
</POST>
<POST>
<POSTER> lind...@pbm.com (Greg Lindahl) </POSTER>
<POSTDATE> 2007-10-08T22:51:00 </POSTDATE>
In article &lt;JSBOi.1263$DP1.1 ... @newsfe11.phx&gt;,

<QUOTE PREVIOUSPOST="
Jeff Reid &lt;jeffar ... @hotmail.com&gt; wrote:
&gt;In the case of C and C++, pointers are used for every instance of
&gt;an object allocated via malloc or new.  If code uses an index for
&gt;an allocated object, then this consumes two cpu registers, but using
&gt;a pointer reduces this to just consumption of a single cpu register,
&gt;so there is an advantage to using pointers.
">

This is definitely not true when you're using an optimizing
compiler. It's pretty much not true when you aren't, too.

-- greg
</POST>
<POST>
<POSTER> &quot;Paul Houle&quot; &lt;asmg...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-10-09T02:04:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Greg Lindahl&quot; &lt;lind ... @pbm.com&gt; wrote in message
">

news:470aecce$1@news.meer.net ...

<QUOTE PREVIOUSPOST="
&gt; In article &lt;JSBOi.1263$DP1.1 ... @newsfe11.phx&gt;,
&gt; Jeff Reid &lt;jeffar ... @hotmail.com&gt; wrote:

&gt;&gt;In the case of C and C++, pointers are used for every instance of
&gt;&gt;an object allocated via malloc or new.  If code uses an index for
&gt;&gt;an allocated object, then this consumes two cpu registers, but using
&gt;&gt;a pointer reduces this to just consumption of a single cpu register,
&gt;&gt;so there is an advantage to using pointers.

&gt; This is definitely not true when you're using an optimizing
&gt; compiler. It's pretty much not true when you aren't, too.

&gt; -- greg
">

It is true, Greg.  Unless an object is statically allocated, efficiently
dereferencing
a &quot;pointer&quot; in the form of an index will require the use of two CPU
registers in any
modern architecture.  Think about it; write yourself some assembly sequences
to
do it.  And less available registers degrades the ability of any compiler to
generate
fast code, optimizing or not.

...Paul
</POST>
<POST>
<POSTER> lind...@pbm.com (Greg Lindahl) </POSTER>
<POSTDATE> 2007-10-09T02:46:00 </POSTDATE>
In article &lt;bSEOi.40226$RX.25 ... @newssvr11.news.prodigy.net&gt;,

<QUOTE PREVIOUSPOST="
Paul Houle &lt;asmg ... @yahoo.com&gt; wrote:
&gt;It is true, Greg.  Unless an object is statically allocated, efficiently
&gt;dereferencing
&gt;a &quot;pointer&quot; in the form of an index will require the use of two CPU
&gt;registers in any
&gt;modern architecture.
">

You might want to to examine the code actually generated by compilers.
In a loop, for example, you will be hard-pressed to find any
optimizing compiler that uses 2 registers.

-- greg
</POST>
<POST>
<POSTER> &quot;Paul Houle&quot; &lt;asmg...@yahoo.com&gt; </POSTER>
<POSTDATE> 2007-10-09T03:52:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&quot;Greg Lindahl&quot; &lt;lind ... @pbm.com&gt; wrote in message
">

news:470b23b8$1@news.meer.net ...

<QUOTE PREVIOUSPOST="
&gt; In article &lt;bSEOi.40226$RX.25 ... @newssvr11.news.prodigy.net&gt;,
&gt; Paul Houle &lt;asmg ... @yahoo.com&gt; wrote:

&gt;&gt;It is true, Greg.  Unless an object is statically allocated, efficiently
&gt;&gt;dereferencing
&gt;&gt;a &quot;pointer&quot; in the form of an index will require the use of two CPU
&gt;&gt;registers in any
&gt;&gt;modern architecture.

&gt; You might want to to examine the code actually generated by compilers.
&gt; In a loop, for example, you will be hard-pressed to find any
&gt; optimizing compiler that uses 2 registers.

&gt; -- greg
">

Oh, now we're in a loop are we?  I guess you imagine it to
be a tight one, where not many registers are needed?
I suppose your indexes are pre-shifted per the operand width?
And maybe they've had the base-array-address added to
them as well.....   (oh wait, then they'd be... pointers)

I have examined code generated by compilers, hundreds of
times, on multiple architectures, over a few decades.

The structure of a complex memory reference these days is
offset[basereg+indexreg*scale]
Where offset is a constant, and scale a constant small power
of two.  Note the use of 'basereg' and 'indexreg' where there
is both array-start-pointer and index.  It takes two registers.

When using a pointer alone, the memory reference will be
simply   [pointer]   -- one register.

Sure, you could use one register in the base/index case, by
shifting the index and adding the base pointer directly to the
index from memory, but that is abysmally slow in comparison.

It's not rocket science.  When you have two components
(array-base-pointer and index) it takes more registers --
even if one is initialized just once outside a loop --
to address a target operand than if you are given a pointer
directly to it.  The loss of register space does negatively
impact the speed of machine code to implement an
algorithm of even moderate complexity.  Especially on a
register-limited architecture like the X86 (prior to the 64-bit
extensions).

...Paul
</POST>
<POST>
<POSTER> glen herrmannsfeldt &lt;g...@ugcs.caltech.edu&gt; </POSTER>
<POSTDATE> 2007-10-09T06:58:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Greg Lindahl wrote:
&gt; In article &lt;JSBOi.1263$DP1.1 ... @newsfe11.phx&gt;,
&gt; Jeff Reid &lt;jeffar ... @hotmail.com&gt; wrote:
&gt;&gt;In the case of C and C++, pointers are used for every instance of
&gt;&gt;an object allocated via malloc or new.  If code uses an index for
&gt;&gt;an allocated object, then this consumes two cpu registers, but using
&gt;&gt;a pointer reduces this to just consumption of a single cpu register,
&gt;&gt;so there is an advantage to using pointers.
&gt; This is definitely not true when you're using an optimizing
&gt; compiler. It's pretty much not true when you aren't, too.
">

Both compilers and machines have changed over the years.
On register starved hardware, such as x86, it may still be
true.  Often, though, the compiler can figure it out.

If you are comparing C code such as:

s=a;
for(i=0;i&lt;n;i++) *s++=i;

to

for(i=0;i&lt;n;i++) a[i]=i;

note that the index is already in a register.  Many systems
can directly index with that register.  In others, the compiler
will figure it out and generate the appropriate pointer
incrementing code.  Compilers have been able to optimize
this one at least since the OS/360 Fortran H compiler.
(This is similar to one of the examples given.)

-- glen
</POST>
<POST>
<POSTER> Chip Coldwell &lt;coldw...@gmail.invalid&gt; </POSTER>
<POSTDATE> 2007-10-09T11:17:00 </POSTDATE>
&quot;James J. Weinkam&quot; &lt;j ... @cs.sfu.ca&gt; writes:

<QUOTE PREVIOUSPOST="
&gt; Here is my transliteration of the C function:
">

Here's my version of quicksort in Fortran:

! Fortran 95 implementation of the quicksort algorithm.  This
! subroutine does not directly touch the array it sorts; rather it
! relies on the two callbacks &quot;compare&quot; and &quot;exchange&quot; for that.
! Code inspired by R. Sedgewick, &quot;Algorithms in C&quot;.

subroutine quicksort(n, compare, exchange)
integer, intent(in) :: n ! the length of the implied array

! The compare function must return an integer that is
! greater than zero if element(i) &gt; element(j)
! equal to zero     if element(i) = element(j)
! less than zero    if element(i) &lt; element(j)
interface
integer function compare(i,j)
integer, intent(in) :: i, j
end function compare
end interface

! The exchange subroutine exchanges the elements at locations i and j.
interface
subroutine exchange(i,j)
integer, intent(in) :: i, j
end subroutine exchange
end interface

call subfile(1,n)
return

contains

recursive subroutine subfile(l,r)
integer, intent(in) :: l, r
integer :: i

if (r .le. l) return
i = partition(l,r)
call subfile(l,i-1)
call subfile(i+1,r)
end subroutine subfile

integer function partition(l,r)
integer, intent(in) :: l, r
integer :: i, j

i = l
j = r - 1
do
do while (compare(i,r) .lt. 0)
i = i+1
end do
do while (compare(j,r) .gt. 0)
if (j .eq. l) exit
j = j-1
end do
if (i .ge. j) exit
call exchange(i,j)
end do
call exchange(i,r)
partition = i
end function partition
end subroutine quicksort

--
Charles M. &quot;Chip&quot; Coldwell
Senior Software Engineer
Red Hat, Inc.
10 Technology Park Drive
Westford, MA  01886
978-392-2426
</POST>
<POST>
<POSTER> lind...@pbm.com (Greg Lindahl) </POSTER>
<POSTDATE> 2007-10-09T11:46:00 </POSTDATE>
In article &lt;RqGOi.5328$4V6.4 ... @newssvr14.news.prodigy.net&gt;,

<QUOTE PREVIOUSPOST="
Paul Houle &lt;asmg ... @yahoo.com&gt; wrote:
&gt;I have examined code generated by compilers, hundreds of
&gt;times, on multiple architectures, over a few decades.
">

Great. Then why are you making sweeping generalizations which are
incorrect?

From reading your latest note, it seems that when you say &quot;pointer&quot;
you may be intending to mean an address register, not a high-level
language pointer. That would explain why you're making no sense. I've
never seen anyone us &quot;pointer&quot; like that before.

-- greg
</POST>
<POST>
<POSTER> lind...@pbm.com (Greg Lindahl) </POSTER>
<POSTDATE> 2007-10-09T11:46:00 </POSTDATE>
In article &lt;RqGOi.5328$4V6.4 ... @newssvr14.news.prodigy.net&gt;,

<QUOTE PREVIOUSPOST="
Paul Houle &lt;asmg ... @yahoo.com&gt; wrote:
&gt;I have examined code generated by compilers, hundreds of
&gt;times, on multiple architectures, over a few decades.
">

Great. Then why are you making sweeping generalizations which are
incorrect?

From reading your latest note, it seems that when you say &quot;pointer&quot;
you may be intending to mean an address register, not a high-level
language pointer. That would explain why you're making no sense. I've
never seen anyone us &quot;pointer&quot; like that before.

-- greg
</POST>
<POST>
<POSTER> glen herrmannsfeldt &lt;g...@ugcs.caltech.edu&gt; </POSTER>
<POSTDATE> 2007-10-09T16:12:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
Chip Coldwell wrote:
">

(snip)

<QUOTE PREVIOUSPOST="
&gt; Here's my version of quicksort in Fortran:
">

(snip)

<QUOTE PREVIOUSPOST="
&gt;   recursive subroutine subfile(l,r)
&gt;     integer, intent(in) :: l, r
&gt;     integer :: i
&gt;     if (r .le. l) return
&gt;     i = partition(l,r)
&gt;     call subfile(l,i-1)
&gt;     call subfile(i+1,r)
&gt;   end subroutine subfile
">

Despite the obvious convenience it is usual to write quicksort
not using explicit recursion.  That is, it is done with an internal
array to save the values that need to saved.

It is also usual to stack the larger half remaining and process the
smaller half.  Doing that reduces the stack requirement (either
explicit recursion or built in array) to log2(N).

I don't see any of the methods to reduce N**2 behavior.  Without
that, and without the stack preservation above, it is easy
to achieve N levels of recursion.  Especially without checking
for only one element remaining before the recursive call.
(Some use a random number to select the pivot point, others
take the middle valued of the first, center, and last point.)

As I understand it, it is faster to not sort the smaller partitions
with quicksort, but to leave them unsorted and run an
insertion sort after the quicksort.  I don't know how popular
that one is, though.

Otherwise, it is nice to see the ability to sort an arbitrary
data structure by having the caller supply the compare and
exchange routines.  It will be nice when generic pointers are
implemented so one can pass the data to the compare and exchange
routines more conveniently.

-- glen
</POST>
<POST>
<POSTER> robert.corb...@sun.com </POSTER>
<POSTDATE> 2007-10-09T16:20:00 </POSTDATE>
On Oct 4, 4:26 pm, wclod ... @lanl.gov wrote:

<QUOTE PREVIOUSPOST="
&gt; I would hope that Dyalog doesn't use Quicksort. FWIW the somebody
&gt; doesn't know.

&gt; For finite (machine) integers (and I believe also floats) there are
&gt; sorting algorithms of order 0(n). Quicksort's average sorting for
&gt; randomly distributed orders of values in the array is 0(n log(n)).
&gt; However Quicksort's worst performance (0(n^2)) occurs for arrays that
&gt; are already sorted (or almost sorted), and in practice those occur far
&gt; more often than would be predicted for random distributions. Quicksort
&gt; should not be used on arrays of machine integers or floating point
&gt; numbers.

&gt; For more complicated types (e.g. character strings, bignums, etc.)
&gt; where 0(n) sorting is not practical, the most commonly used
&gt; competitors to Quicksort are Heapsort and Mergesort. All three have
&gt; &quot;average&quot; 0(n log(n)) performance. Worst case for Heapsort and
&gt; Mergesort remains 0(n log(n)) as opposed to Quicksort's (n^2). Note
&gt; that Quicksort and Heapsort are largely sort in place algorithms, but
&gt; Mergesort can need significantly (~2-3*) more memory. Mergesort
&gt; (unlike Quicksort and Heapsort) is stable (preserves relative order of
&gt; identical items). The relative performance of these algorithms depends
&gt; on how the data is stored (array, linked list, tree), and the relative
&gt; costs of comparisons versus memory moves. Modifications of all three
&gt; algorithms can change the coefficients, often minimizing memory moves.
&gt; Most analyses (and tests) use (comparison and memory) operations
&gt; appropriate for machine integers (where the 0(n) methods are more
&gt; appropriate), on randomly ordered data. In these tests Quicksort is
&gt; usually faster than Heapsort, but slightly slower than Mergesort. If
&gt; either stability or performance is more important than minimizing
&gt; memory usage, (a sophisticated) Mergesort is to be preferred, if
&gt; minimizing memory usage is more important and if Quicksort's worst
&gt; case is expected to be uncommon Quicksort is to be preferred,
&gt; otherwise Heapsort is to be preferred.
">

There are many implementations of quicksort with different
performance characteristics.  The problem of quicksort going
quadratic on almost ordered or almost reverse ordered arrays
is well-known and is easily eliminated.  There are
implementations of quicksort for which the worst-case
execution time is O(n log(n)).  The book *Introduction to
Algorithms* by Cormen, Leiserson, and Rivest gives this as
an exercise:

10.3-3
Show how quicksort can be made to run in O(n lg n) time
in the worst case.

Bob Corbett
</POST>
<POST>
<POSTER> &quot;Jeff Reid&quot; &lt;jeffar...@hotmail.com&gt; </POSTER>
<POSTDATE> 2007-10-09T17:05:00 </POSTDATE>
<QUOTE PREVIOUSPOST="
&gt;&gt;In the case of C and C++, pointers are used for every instance of
&gt;&gt;an object allocated via malloc or new.  If code uses an index for
&gt;&gt;an allocated object, then this consumes two cpu registers, but using
&gt;&gt;a pointer reduces this to just consumption of a single cpu register,
&gt;&gt;so there is an advantage to using pointers.

&gt; This is definitely not true when you're using an optimizing
&gt; compiler. It's pretty much not true when you aren't, too.
">

So I'll modify my statement, to read &quot;If code uses an index for an
allocated object, then this may consume two cpu registers, but using
a pointer will always use just a single cpu register.&quot;

A better example would be the actual case I used in my sort program.
In this case there are three main pointers used to point to arrays
of pointers to records, and two more used to point to arrays of group
counts. If the compiler runs out of registers during optimization,
then the original pointer(s) for the allocated memory can be left on
the stack, leaving the 5 working pointers in registers. If these
5 working pointers were replaced by 5 working indexes, there's no
guarantee that the compiler will optmized all of the index references
back into pointers and eliminate referencing the original pointer(s)
for the allocated memory.

<QUOTE PREVIOUSPOST="
&gt; Not other than issues of style, maintainability, and the usual
&gt; discouragement of pointers where possible.
">

Regardless of the compiler generated code, why the objection to using
pointers in the first place?
</POST>
</TEXT>
</BODY>
</DOC>
